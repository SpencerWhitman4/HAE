import os
import sys
import numpy as np
import logging
from pathlib import Path
from typing import Dict, Tuple, Optional, Any, List
from dataclasses import dataclass
import json
import time
from collections import deque
from gymnasium import spaces

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ç¯å¢ƒåŸºç±»
from src.envs.BaseRLAviary import BaseRLAviary
from src.utils.enums import DroneModel, Physics, ActionType, ObservationType

# å¯¼å…¥é‡æ„åçš„æ ¸å¿ƒæ¨¡å—
from src.modules import StateManager, StructuredState
from src.perceptions import (
    create_perception_manager, 
    create_observation_input
)
from src.utils.MapManager import MapManager

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class HAUAVConfig:
    """HAUAVç¯å¢ƒé…ç½®"""
    MAX_EPISODE_STEPS: int = 1000
    EXPLORATION_THRESHOLD: float = 0.95
    COLLISION_THRESHOLD: float = 0.2
    SAFETY_THRESHOLD: float = 0.5
    
    OBSERVATION_DIM: int = 86
    ACTION_DIM: int = 4
    
    REWARD_WEIGHTS: Dict[str, float] = None
    
    MAP_FILE: str = "/home/lxy/Code/gym-pybullet-drones-RL/src/maps/room_map.json"
    MAP_NAME: str = "room_basic"  # ä½¿ç”¨å®¤å†…æˆ¿é—´åŸºç¡€åœ°å›¾
    
    ENABLE_TRAJECTORY_RECORDING: bool = True
    TRAJECTORY_LOG_DIR: str = "./logs/trajectories"
    
    STATE_MANAGER_CONFIG: Dict[str, Any] = None
    PERCEPTION_CONFIG: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.REWARD_WEIGHTS is None:
            self.REWARD_WEIGHTS = {
                'exploration': 0.4,
                'safety': 0.3,
                'execution': 0.2,
                'completion': 0.1
            }
        
        if self.STATE_MANAGER_CONFIG is None:
            self.STATE_MANAGER_CONFIG = {
                'history_length': 20,
                'high_level_update_frequency': 5,
                'future_horizon': 5
            }
        
        if self.PERCEPTION_CONFIG is None:
            self.PERCEPTION_CONFIG = {
                'map_resolution': 0.1,
                'map_grid_size': (100, 100),
                'num_sectors': 36,
                'max_detection_range': 10.0
            }


class HAUAVAviary(BaseRLAviary):
    """åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ— äººæœºç¯å¢ƒ"""
    
    def __init__(
        self,
        drone_model: DroneModel = DroneModel.CF2X,
        num_drones: int = 1,
        neighbourhood_radius: float = np.inf,
        initial_xyzs=None,
        initial_rpys=None,
        physics: Physics = Physics.PYB,
        pyb_freq: int = 240,
        ctrl_freq: int = 30,
        gui: bool = False,
        record: bool = False,
        obstacles: bool = True,
        user_debug_gui: bool = True,
        vision_attributes: bool = True,
        config: Optional[HAUAVConfig] = None,
        map_file: Optional[str] = None,
        enable_logging: bool = False,
        trajectory_manager=None,
        # === TensorBoardé›†æˆå‚æ•° ===
        enable_tensorboard: bool = False,
        tensorboard_mode: str = "train",  # "train" or "eval"
        session_dir: Optional[Path] = None,  # å¤–éƒ¨ä¼ å…¥çš„ä¼šè¯ç›®å½•
        **kwargs
    ):
        """åˆå§‹åŒ–HAUAVç¯å¢ƒ"""
        
        # é…ç½®ç®¡ç†
        self.config = config if config is not None else HAUAVConfig()
        if map_file is not None:
            self.config.MAP_FILE = map_file
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.enable_logging = enable_logging
        self.logger = self._setup_logger()
        
        # === TensorBoardåˆå§‹åŒ– ===
        self.enable_tensorboard = enable_tensorboard
        self.tensorboard_mode = tensorboard_mode
        self.tensorboard_logger = None
        self.global_step = 0  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨
        
        if self.enable_tensorboard and session_dir:
            self._initialize_tensorboard(session_dir)
        elif self.enable_tensorboard and not session_dir:
            self.logger.warning("å¯ç”¨äº†TensorBoardä½†æœªæä¾›session_dirï¼ŒTensorBoardè®°å½•å°†è¢«ç¦ç”¨")
            self.enable_tensorboard = False
        
        # ğŸ”§ ä¿®å¤ï¼šåœ¨è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–å‰å…ˆè®¾ç½®æ­£ç¡®çš„INIT_XYZS
        # é¢„è®¾ç½®åœ°å›¾ä½ç½®
        self._pre_initialize_map_position()
        map_initial_xyzs = getattr(self, 'INIT_XYZS', None)
        
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šinitial_xyzsï¼Œä½¿ç”¨åœ°å›¾é…ç½®çš„ä½ç½®
        if initial_xyzs is None and map_initial_xyzs is not None:
            initial_xyzs = map_initial_xyzs
            self.logger.info(f"ä½¿ç”¨åœ°å›¾é…ç½®çš„èµ·å§‹ä½ç½®: {initial_xyzs}")
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(
            drone_model=drone_model,
            num_drones=num_drones,
            neighbourhood_radius=neighbourhood_radius,
            initial_xyzs=initial_xyzs,  # ä¼ é€’åœ°å›¾ä½ç½®
            initial_rpys=initial_rpys,
            physics=physics,
            pyb_freq=pyb_freq,
            ctrl_freq=ctrl_freq,
            gui=gui,
            record=record,
            act=ActionType.VEL,
            obs=ObservationType.KIN,
            **kwargs
        )
        
        # ç¯å¢ƒçŠ¶æ€
        self.episode_step = 0
        self.episode_count = 0
        self.total_steps = 0
        
        # å†å²æ•°æ®ç¼“å­˜
        self.action_history = deque(maxlen=6)
        self.subgoal_history = deque(maxlen=5)
        self.reward_history = deque(maxlen=100)
        self.exploration_rate_history = deque(maxlen=100)
        self.episode_history = []
        
        # å¢é‡æ§åˆ¶æ ‡å¿—
        self.use_increment_control = True
        
        # åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—
        self._initialize_core_modules()
        
        # è½¨è¿¹è®°å½•å™¨ï¼ˆç”±å¤–éƒ¨ä¼ é€’ï¼‰
        self.trajectory_logger = trajectory_manager
        if self.trajectory_logger:
            self.logger.info("å¤–éƒ¨è½¨è¿¹è®°å½•å™¨å·²è®¾ç½®")
        else:
            self.logger.info("æœªè®¾ç½®è½¨è¿¹è®°å½•å™¨")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_stats = {
            'exploration_rate': 0.0,
            'collision_count': 0,
            'safety_violations': 0,
            'completion_status': False,
            'total_reward': 0.0,
            'step_count': 0
        }
        
        # ç¼“å­˜å˜é‡
        self.latest_perception_results = None
        self.current_structured_state = None
        self.current_observation = None
        
        self.logger.debug("HAUAVç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(f'HAUAV_{id(self)}')
        logger.setLevel(logging.INFO if self.enable_logging else logging.WARNING)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _initialize_tensorboard(self, session_dir: Path):
        """
        åˆå§‹åŒ–TensorBoardè®°å½•å™¨
        
        Args:
            session_dir: å¤–éƒ¨ä¼ å…¥çš„ä¼šè¯ç›®å½•ï¼ˆç”±LogDirectoryBuilderåˆ›å»ºï¼‰
        """
        from src.utils.tensorboard_logger import TensorboardLogger
        
        # ä½¿ç”¨ä¼šè¯ç›®å½•ä¸‹çš„Tensorboard/train æˆ– Tensorboard/eval
        tensorboard_dir = session_dir / "Tensorboard" / self.tensorboard_mode
        
        # åˆ›å»ºTensorBoardè®°å½•å™¨
        self.tensorboard_logger = TensorboardLogger(tensorboard_dir, self.tensorboard_mode)
        
        self.logger.debug(f"TensorBoardè®°å½•å™¨åˆå§‹åŒ–å®Œæˆ: {tensorboard_dir}")
    
    def _initialize_core_modules(self):
        """åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—"""
        self.state_manager = StateManager(**self.config.STATE_MANAGER_CONFIG)
        self.perception_manager = create_perception_manager(self.config.PERCEPTION_CONFIG)
        
        # MapManageråˆå§‹åŒ– - å¦‚æœæ²¡æœ‰æŒ‡å®šåœ°å›¾æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼
        map_file = self.config.MAP_FILE if self.config.MAP_FILE else "assets/default_map.json"
        self.map_manager = MapManager(map_json_path=map_file)
        
        # åˆå§‹åŒ–ç‚¹äº‘è¿‡æ»¤å™¨
        self._initialize_pointcloud_filter()
        
        self.logger.debug("æ ¸å¿ƒæ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
    
    def _pre_initialize_map_position(self):
        """é¢„åˆå§‹åŒ–åœ°å›¾ä½ç½® - åœ¨çˆ¶ç±»åˆå§‹åŒ–å‰è®¾ç½®æ­£ç¡®çš„INIT_XYZS"""
        # ä»é…ç½®ä¸­è·å–åœ°å›¾æ–‡ä»¶è·¯å¾„å’Œåœ°å›¾åç§°
        map_file = self.config.MAP_FILE
        map_name = self.config.MAP_NAME
        
        if map_file:
            try:
                # ç›´æ¥è§£æåœ°å›¾æ–‡ä»¶è·å–èµ·å§‹ä½ç½®
                import json
                with open(map_file, 'r', encoding='utf-8') as f:
                    map_data = json.load(f)
                
                # æŸ¥æ‰¾æŒ‡å®šåœ°å›¾çš„èµ·å§‹ä½ç½®
                if map_name in map_data:
                    map_info = map_data[map_name]
                    start_position = map_info.get('start_pos', [0, 0, 1])  # ä½¿ç”¨æ­£ç¡®çš„é”®å
                    self.INIT_XYZS = np.array(start_position).reshape(1, 3)
                    self.logger.info(f"ä»åœ°å›¾æ–‡ä»¶åŠ è½½èµ·å§‹ä½ç½®: {start_position}")
                else:
                    self.logger.warning(f"åœ°å›¾ {map_name} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤èµ·å§‹ä½ç½®")
                    self.INIT_XYZS = np.array([[0, 0, 1]])
                
            except Exception as e:
                self.logger.warning(f"é¢„åŠ è½½åœ°å›¾å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤èµ·å§‹ä½ç½®")
                self.INIT_XYZS = np.array([[0, 0, 1]])
        else:
            self.logger.warning("æœªæŒ‡å®šåœ°å›¾æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤èµ·å§‹ä½ç½®")
            self.INIT_XYZS = np.array([[0, 0, 1]])

    def _initialize_pointcloud_filter(self):
        """åˆå§‹åŒ–ç‚¹äº‘è¿‡æ»¤å™¨"""
        from src.utils.PointCloudFilter import PointCloudFilter, PointCloudFilterConfig
        
        # åˆ›å»ºç‚¹äº‘è¿‡æ»¤é…ç½®
        filter_config = PointCloudFilterConfig(
            max_distance=self.config.PERCEPTION_CONFIG.get('max_detection_range', 10.0),
            filter_ground=False,  # ä¸´æ—¶ç¦ç”¨åœ°é¢è¿‡æ»¤
            noise_threshold=2.0,
            min_points=10
        )
        
        self.pointcloud_filter = PointCloudFilter(config=filter_config)
        self.logger.debug("ç‚¹äº‘è¿‡æ»¤å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def _load_map(self):
        """åŠ è½½åœ°å›¾"""
        pybullet_client = self.getPyBulletClient()
        self.map_manager.set_pybullet_client(pybullet_client)
        
        # ä»é…ç½®ä¸­è·å–åœ°å›¾æ–‡ä»¶è·¯å¾„å’Œåœ°å›¾åç§°
        map_file = self.config.MAP_FILE
        map_name = self.config.MAP_NAME
        
        # æ·»åŠ åœ°å›¾åŠ è½½çŠ¶æ€è·Ÿè¸ªï¼Œé¿å…é‡å¤æ‰“å°
        if not hasattr(self, '_map_loaded') or not self._map_loaded:
            self._map_loaded = True
            map_load_verbose = True
        else:
            map_load_verbose = False
        
        if map_file:
            try:
                self.map_manager.load_map(map_name)
                start_position = np.array(self.map_manager.get_start_position())
                
                self.INIT_XYZS = start_position.reshape(1, 3)
                
                if map_load_verbose:  # åªåœ¨ç¬¬ä¸€æ¬¡åŠ è½½æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    print(f"âœ… åœ°å›¾åŠ è½½å®Œæˆ: {map_name} (æ¥è‡ªæ–‡ä»¶: {map_file})")
                    print(f"   èµ·å§‹ä½ç½®: {start_position}")
                    
                    # æ‰“å°åœ°å›¾ä¿¡æ¯
                    map_info = self.map_manager.get_map_info()
                    if map_info:
                        print(f"   åœ°å›¾æè¿°: {map_info.get('description', 'æ— æè¿°')}")
                        bounds = map_info.get('bounds', {})
                        if bounds:
                            print(f"   åœ°å›¾è¾¹ç•Œ: X[{bounds.get('x_min', 0):.1f}, {bounds.get('x_max', 0):.1f}]"
                                  f" Y[{bounds.get('y_min', 0):.1f}, {bounds.get('y_max', 0):.1f}]"
                                  f" Z[{bounds.get('z_min', 0):.1f}, {bounds.get('z_max', 0):.1f}]")
                        
                        # æ˜¾ç¤ºéšœç¢ç‰©å’Œå¢™ä½“æ•°é‡
                        obstacles = map_info.get('obstacles', [])
                        cave_walls = map_info.get('cave_walls', [])
                        print(f"   éšœç¢ç‰©æ•°é‡: {len(obstacles)}, å¢™ä½“æ•°é‡: {len(cave_walls)}")
                else:
                    # åç»­åŠ è½½åªæ˜¾ç¤ºç®€å•ä¿¡æ¯
                    pass  # é™é»˜é‡ç½®
                    
            except Exception as e:
                print(f"âŒ åœ°å›¾åŠ è½½å¤±è´¥: {e}")
                print(f"   åœ°å›¾åç§°: {map_name}")
                print(f"   åœ°å›¾æ–‡ä»¶: {map_file}")
                print("   å°†ä½¿ç”¨é»˜è®¤èµ·å§‹ä½ç½®")
                # ä½¿ç”¨é»˜è®¤èµ·å§‹ä½ç½®
                if not hasattr(self, 'INIT_XYZS') or self.INIT_XYZS is None:
                    self.INIT_XYZS = np.array([[0, 0, 1]])
        else:
            print("âš ï¸  æœªæŒ‡å®šåœ°å›¾æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤èµ·å§‹ä½ç½®")
            if not hasattr(self, 'INIT_XYZS') or self.INIT_XYZS is None:
                self.INIT_XYZS = np.array([[0, 0, 1]])
    
    
    # ============ BaseRLAviaryæ ‡å‡†æ¥å£ ============

    def set_increment_control_mode(self, enable: bool = True, drone_id: int = 0):
        """å¯ç”¨/ç¦ç”¨å¢é‡æ§åˆ¶æ¨¡å¼"""
        if hasattr(self, 'ctrl') and self.ctrl:
            if hasattr(self.ctrl[drone_id], 'increment_mode'):
                self.ctrl[drone_id].increment_mode = enable
                self.logger.info(f"æ— äººæœº {drone_id} å¢é‡æ§åˆ¶æ¨¡å¼: {'å¯ç”¨' if enable else 'ç¦ç”¨'}")
            else:
                self.logger.warning(f"æ§åˆ¶å™¨ä¸æ”¯æŒå¢é‡æ§åˆ¶æ¨¡å¼")
        else:
            self.logger.error("æ§åˆ¶å™¨æœªåˆå§‹åŒ–")
    
    def apply_increment_action(self, increment: np.ndarray, drone_id: int = 0):
        """åº”ç”¨å¢é‡åŠ¨ä½œåˆ°æŒ‡å®šæ— äººæœº"""
        if hasattr(self, 'ctrl') and self.ctrl:
            if hasattr(self.ctrl[drone_id], 'set_increment_action'):
                self.ctrl[drone_id].set_increment_action(increment)
                self.logger.debug(f"æ— äººæœº {drone_id} å¢é‡åŠ¨ä½œ: {increment}")
            else:
                self.logger.warning(f"æ§åˆ¶å™¨ä¸æ”¯æŒå¢é‡åŠ¨ä½œè®¾ç½®")
        else:
            self.logger.error("æ§åˆ¶å™¨æœªåˆå§‹åŒ–")

    def get_control_relevant_state(self, drone_id: int = 0) -> Dict:
        """è·å–ç”¨äºæ§åˆ¶çš„ç›¸å…³çŠ¶æ€ä¿¡æ¯"""
        if hasattr(self, 'current_structured_state') and self.current_structured_state:
            state = self.current_structured_state
            control_state = {
                'position': state.position.copy(),
                'velocity': state.velocity.copy(),
                'attitude_quaternion': state.attitude_quaternion.copy(),
                'angular_velocity': state.angular_velocity.copy(),
                'target_velocity': getattr(self.ctrl[drone_id], 'target_velocity', np.zeros(4)) if hasattr(self, 'ctrl') else np.zeros(4)
            }
            return control_state
        return {}
    
    def _is_increment_action(self, action):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¢é‡åŠ¨ä½œæ ¼å¼"""
        return (isinstance(action, np.ndarray) and 
                ((action.ndim == 1 and action.shape[0] == 4) or 
                 (action.ndim == 2 and action.shape[1] == 4)))

    def _extract_increment_action(self, action):
        """æå–å¢é‡åŠ¨ä½œ"""
        return action.reshape(1, -1) if action.ndim == 1 else action

    def _get_increment_control_info(self):
        """è·å–å¢é‡æ§åˆ¶ä¿¡æ¯"""
        info = {}
        for k in range(self.NUM_DRONES):
            if hasattr(self.ctrl[k], 'get_target_action'):
                target_action = self.ctrl[k].get_target_action()
                if target_action is not None:
                    info[f'drone_{k}_target'] = target_action.tolist()
        return info
    
    def _actionSpace(self):
        """åŠ¨ä½œç©ºé—´å®šä¹‰"""
        return spaces.Box(
            low=-2.0,
            high=2.0,
            shape=(self.config.ACTION_DIM,),
            dtype=np.float32
        )
    
    def _observationSpace(self):
        """è§‚æµ‹ç©ºé—´å®šä¹‰"""
        return spaces.Box(
            low=-np.inf,
            high=np.inf, 
            shape=(self.config.OBSERVATION_DIM,),
            dtype=np.float32
        )
    
    def _computeObs(self):
        """è®¡ç®—86ç»´è§‚æµ‹å‘é‡ - ç»Ÿä¸€æ„å»ºæ‰€æœ‰æ„ŸçŸ¥æ•°æ®"""
        # è·å–åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®
        sensor_data = self._get_raw_sensor_data()
        
        # ä½¿ç”¨PerceptionManageræ„å»º86ç»´è§‚æµ‹å¹¶è·å–å®Œæ•´æ„ŸçŸ¥ç»“æœ
        obs_86d, perception_results = self._build_observation_with_perception_manager(sensor_data)
        
        # ä¿å­˜å®Œæ•´æ„ŸçŸ¥ç»“æœä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
        self.latest_perception_results = perception_results
        
        # æ›´æ–°StateManagerçŠ¶æ€
        structured_state = self.state_manager.parse_and_update(obs_86d)
        self.current_structured_state = structured_state
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯ - ç›´æ¥ä»PerceptionResultsè·å–æœ€æ–°çš„æ¢ç´¢ç‡
        if self.latest_perception_results and self.latest_perception_results.local_map_results:
            # ç›´æ¥ä»LocalMapResultsè·å–æ¢ç´¢ç‡ï¼Œç¡®ä¿æ•°æ®åŒæ­¥
            current_exploration_rate = getattr(self.latest_perception_results.local_map_results, 'exploration_rate', 0.0)
            self.episode_stats['exploration_rate'] = float(current_exploration_rate)
        elif structured_state.parsed_map_state:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä»StateManagerè·å–
            self.episode_stats['exploration_rate'] = structured_state.parsed_map_state.get('exploration_rate', 0.0)
        else:
            # é»˜è®¤å€¼
            self.episode_stats['exploration_rate'] = 0.0
        
        # ä¿å­˜å½“å‰è§‚æµ‹
        self.current_observation = obs_86d.astype(np.float32)
        
        return self.current_observation.reshape(1, -1)  # BaseRLAviaryæœŸæœ›(NUM_DRONES, OBS_DIM)
    
    
    def _computeTerminated(self):
        """è®¡ç®—ç»ˆæ­¢æ¡ä»¶ - ä½¿ç”¨å·²æ„å»ºçš„æ„ŸçŸ¥ç»“æœ"""
        terminated = False
        termination_reason = "CONTINUE"
        
        # 1. å®‰å…¨ç»ˆæ­¢æ£€æŸ¥ï¼ˆä½¿ç”¨PerceptionResultsï¼‰
        safety_results = self.latest_perception_results.safety_results
        
        if safety_results.should_terminate:
            terminated = True
            termination_reason = safety_results.termination_reason
        elif safety_results.collision_detected:
            terminated = True
            termination_reason = "COLLISION_DETECTED"
            self.episode_stats['collision_count'] += 1
        elif safety_results.out_of_bounds:
            terminated = True
            termination_reason = "OUT_OF_BOUNDS"
        
        # 2. æ¢ç´¢å®Œæˆæ£€æŸ¥
        if not terminated:
            exploration_rate = self.episode_stats.get('exploration_rate', 0.0)
            if exploration_rate >= self.config.EXPLORATION_THRESHOLD:
                terminated = True
                termination_reason = "EXPLORATION_COMPLETE"
                self.episode_stats['completion_status'] = True
        
        # è®°å½•ç»ˆæ­¢ä¿¡æ¯
        if terminated:
            self.logger.debug(f"Episodeç»ˆæ­¢: {termination_reason}, æ­¥æ•°: {self.episode_step}, "
                           f"æ¢ç´¢ç‡: {self.episode_stats.get('exploration_rate', 0.0):.3f}")
        
        return np.array([terminated])
    
    def _computeTruncated(self):
        """è®¡ç®—æˆªæ–­æ¡ä»¶"""
        truncated = self.episode_step >= self.config.MAX_EPISODE_STEPS
        return np.array([truncated])

    def _computeInfo(self):
        """è®¡ç®—ä¿¡æ¯å­—å…¸ - ä½¿ç”¨å·²æ„å»ºçš„æ‰€æœ‰æ•°æ®"""
        info = {
            'episode_step': self.episode_step,
            'episode_count': self.episode_count,
            'exploration_rate': self.episode_stats.get('exploration_rate', 0.0),
            'collision_count': self.episode_stats.get('collision_count', 0),
            'safety_violations': self.episode_stats.get('safety_violations', 0),
            'total_reward': self.episode_stats.get('total_reward', 0.0),
            'completion_status': self.episode_stats.get('completion_status', False),
        }
        
        # StateManagerçŠ¶æ€ä¿¡æ¯
        info['state_manager_ready'] = self.state_manager.is_ready_for_high_level()
        info['should_update_high_level'] = self.state_manager.should_update_high_level()
        info['state_history_length'] = len(self.state_manager.state_history)
        
        # PerceptionResultsçŠ¶æ€ä¿¡æ¯
        safety_results = self.latest_perception_results.safety_results
        info['min_obstacle_distance'] = safety_results.min_obstacle_distance
        info['is_safe'] = safety_results.is_safe
        
        # åŠ¨ä½œå†å²ä¿¡æ¯
        info['action_history_length'] = len(self.action_history)
        info['reward_history_length'] = len(self.reward_history)
        
        return info
    
    # ============ ç¯å¢ƒç”Ÿå‘½å‘¨æœŸæ–¹æ³• ============
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ - å®Œæ•´çš„é‡ç½®æµç¨‹"""
        # è°ƒç”¨çˆ¶ç±»reset
        obs, info = super().reset(seed=seed, options=options)
        
        # é‡ç½®ç¯å¢ƒçŠ¶æ€è®¡æ•°å™¨
        self.episode_step = 0
        self.episode_count += 1
        
        # é‡ç½®æ‰€æœ‰å†å²ç¼“å­˜
        self.action_history.clear()
        self.subgoal_history.clear()
        self.reward_history.clear()
        self.exploration_rate_history.clear()
        
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.episode_stats = {
            'exploration_rate': 0.0,
            'collision_count': 0,
            'safety_violations': 0,
            'completion_status': False,
            'total_reward': 0.0,
            'step_count': 0
        }
        
        # é‡ç½®æ ¸å¿ƒæ¨¡å—çŠ¶æ€
        self.state_manager.reset()
        self.perception_manager.reset()
        
        # æ¸…ç©ºç¼“å­˜çš„æ„ŸçŸ¥æ•°æ®
        self.latest_perception_results = None
        self.current_structured_state = None
        self.current_observation = None
        
        # å…ˆåŠ è½½åœ°å›¾ï¼Œå†è®¡ç®—è§‚æµ‹
        self._load_map()
        
        # é‡æ–°è®¡ç®—åˆå§‹è§‚æµ‹ï¼ˆè¿™ä¼šè§¦å‘å®Œæ•´çš„æ„ŸçŸ¥æµç¨‹ï¼‰
        obs = self._computeObs()

        # æ›´æ–°infoä¿¡æ¯
        info.update(self._computeInfo())
        
        # ===== é‡è¦ï¼šå¯åŠ¨æ–°episodeçš„è½¨è¿¹è®°å½• =====
        if self.trajectory_logger:
            self.trajectory_logger.start_new_episode(self.episode_count)
            self.logger.debug(f"è½¨è¿¹è®°å½•å·²å¯åŠ¨ - Episode {self.episode_count}")
        
        self.logger.debug(f"ç¯å¢ƒé‡ç½®å®Œæˆ - Episode {self.episode_count}, è§‚æµ‹ç»´åº¦: {obs.shape}")
        
        return obs[0], info  # è¿”å›å•æ™ºèƒ½ä½“è§‚æµ‹
    

    def step(self, action):
        """é‡å†™stepæ–¹æ³•ï¼Œé›†æˆå¢é‡æ§åˆ¶å’ŒTensorBoardè®°å½•"""

        # ============ æ­¥éª¤1: åŠ¨ä½œé¢„å¤„ç†å’ŒçŠ¶æ€æ›´æ–° ============
        self.episode_step += 1
        self.total_steps += 1
        self.global_step += 1
        
        # 1. å¤„ç†å¢é‡åŠ¨ä½œ
        if self.use_increment_control and self._is_increment_action(action):
            increment_action = self._extract_increment_action(action)
            # è®¾ç½®å¢é‡åˆ°æ§åˆ¶å™¨
            for k in range(self.NUM_DRONES):
                drone_increment = increment_action[k] if increment_action.ndim > 1 else increment_action
                if hasattr(self.ctrl[k], 'set_increment_action'):
                    self.ctrl[k].set_increment_action(drone_increment)
        
        # å¤„ç†åŠ¨ä½œæ ¼å¼ - æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼
        if isinstance(action, dict):
            # å¤šæ™ºèƒ½ä½“æ ¼å¼ï¼š{0: action_array}
            action_array = action[0] if 0 in action else list(action.values())[0]
        else:
            # å•æ™ºèƒ½ä½“æ ¼å¼
            action_array = action
        
        # æ ‡å‡†åŒ–åŠ¨ä½œæ ¼å¼
        if isinstance(action_array, (list, tuple)):
            action_array = np.array(action_array, dtype=np.float32)
        elif not isinstance(action_array, np.ndarray):
            action_array = np.array([action_array], dtype=np.float32)
        
        # ç¡®ä¿åŠ¨ä½œç»´åº¦æ­£ç¡®
        if action_array.ndim == 1:
            action_array = action_array.reshape(1, -1)
        
        # è®°å½•åŠ¨ä½œå†å² - æ”¯æŒè¿ç»­æ€§åˆ†æ
        action_4d = action_array.flatten()[:4]  # ç¡®ä¿4ç»´
        if len(action_4d) < 4:
            action_4d = np.pad(action_4d, (0, 4 - len(action_4d)))
        
        # ä¿å­˜å†å²åŠ¨ä½œä¾›è¿ç»­æ€§å¥–åŠ±è®¡ç®—
        if hasattr(self, 'last_action') and self.last_action is not None:
            self.prev_action = self.last_action.copy()
        else:
            self.prev_action = None
        
        self.last_action = action_4d.copy()
        self.action_history.append(action_4d)
        
        # ============ æ­¥éª¤2: æ‰§è¡Œç‰©ç†ä»¿çœŸ ============
        # å…ˆé¢„å¤„ç†åŠ¨ä½œä»¥è·å–RPMè¾“å‡ºï¼ˆåœ¨BaseRLAviary.stepä¹‹å‰ï¼‰
        rpm_output = self._preprocessAction(action_array)
        self.last_rpm = rpm_output  # ä¿å­˜RPMè¾“å‡ºä¾›è½¨è¿¹è®°å½•ä½¿ç”¨
        
        # è°ƒç”¨BaseRLAviary.stepæ‰§è¡Œç‰©ç†ä»¿çœŸ
        # è¿™ä¼šè‡ªåŠ¨è°ƒç”¨ _computeObs, _computeReward, _computeTerminated, _computeTruncated, _computeInfo
        base_obs, base_reward, base_terminated, base_truncated, base_info = super().step(action_array)
        
        # ============ æ­¥éª¤3: è·å–ç»Ÿä¸€æ„å»ºçš„æ„ŸçŸ¥æ•°æ® ============
        # åœ¨_computeObsä¸­å·²ç»æ„å»ºå¹¶ä¿å­˜äº†ä»¥ä¸‹æ•°æ®:
        # - self.latest_perception_results (å®Œæ•´æ„ŸçŸ¥ç»“æœ)
        # - self.current_structured_state (StateManagerè§£æçŠ¶æ€)
        # - self.current_observation (86ç»´è§‚æµ‹)
        
        # éªŒè¯æ„ŸçŸ¥æ•°æ®å®Œæ•´æ€§
        
        # ============ æ­¥éª¤4: ä½¿ç”¨ç»Ÿä¸€æ„ŸçŸ¥æ•°æ®çš„è¿”å›å€¼ ============
        # BaseRLAviary.stepå·²ç»è°ƒç”¨äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨è¿”å›å€¼
        observation_86d = base_obs[0]  # å•æ™ºèƒ½ä½“è§‚æµ‹
        hierarchical_reward = base_reward[0]  # åˆ†å±‚å¥–åŠ±
        terminated = base_terminated[0]  # ç»ˆæ­¢çŠ¶æ€
        truncated = base_truncated[0]  # æˆªæ–­çŠ¶æ€
        
        # ============ æ­¥éª¤5: æ‰©å±•ä¿¡æ¯å­—å…¸ ============
        # åŸºç¡€infoå·²ç»åŒ…å«äº†_computeInfoçš„å†…å®¹ï¼Œç°åœ¨æ·»åŠ HAUAVAviaryç‰¹æœ‰ä¿¡æ¯
        info = base_info.copy()
        
        # æ·»åŠ æ„ŸçŸ¥æ¨¡å—è¯¦ç»†ä¿¡æ¯
        if self.latest_perception_results:
            safety_results = self.latest_perception_results.safety_results
            local_map_results = self.latest_perception_results.local_map_results
            
            info.update({
                'perception_safety': {
                    'min_obstacle_distance': safety_results.min_obstacle_distance,
                    'collision_detected': safety_results.collision_detected,
                    'out_of_bounds': safety_results.out_of_bounds,
                    'should_terminate': safety_results.should_terminate,
                    'is_safe': safety_results.is_safe
                },
                'perception_map': {
                    'exploration_rate': getattr(local_map_results, 'exploration_rate', 0.0),
                    'occupied_cells': getattr(local_map_results, 'occupied_cells', 0),
                    'free_cells': getattr(local_map_results, 'free_cells', 0),
                    'unknown_cells': getattr(local_map_results, 'unknown_cells', 0)
                }
            })
        
        # æ·»åŠ å¢é‡æ§åˆ¶ä¿¡æ¯
        info['incremental_control'] = self._get_increment_control_info()
        
        # æ·»åŠ æ§åˆ¶ç›¸å…³çŠ¶æ€
        if hasattr(self, 'state_manager'):
            direct_state = self.state_manager.extract_control_relevant_state(observation_86d)
            info['direct_state'] = direct_state
        
        # ============ æ­¥éª¤6: TensorBoardæ•°æ®è®°å½• ============
        if self.tensorboard_logger:
            self._log_step_data_to_tensorboard(action_4d, hierarchical_reward, info)
        
        # ============ é‡è¦ï¼šè½¨è¿¹è®°å½• ============
        if self.trajectory_logger:
            trajectory_data = self.get_trajectory_step_data()
            self.trajectory_logger.log_step(trajectory_data)
        
        # ============ æ­¥éª¤7: Episodeç»“æŸå¤„ç† ============
        if terminated or truncated:
            if self.tensorboard_logger:
                self._log_episode_data_to_tensorboard()
            
            # ===== é‡è¦ï¼šå®Œæˆè½¨è¿¹è®°å½• =====
            if self.trajectory_logger:
                termination_reason = self._determine_termination_reason(terminated, info)
                final_exploration_rate = self.episode_stats.get('exploration_rate', 0.0)
                total_reward = self.episode_stats.get('total_reward', 0.0)
                
                self.trajectory_logger.finalize_episode(
                    termination_reason=termination_reason,
                    final_exploration_rate=final_exploration_rate,
                    total_reward=total_reward
                )
                self.logger.debug(f"è½¨è¿¹è®°å½•å·²å®Œæˆ - Episode {self.episode_count}")
            
            # è®°å½•Episodeç»Ÿè®¡
            termination_reason = self._determine_termination_reason(terminated, info)
            self.logger.info(f"Episode {self.episode_count} ç»“æŸ: {termination_reason}")
        
        return observation_86d, hierarchical_reward, terminated, truncated, info

    
    
    # ============ æ„ŸçŸ¥å¤„ç†æ–¹æ³• ============
    
    def _get_raw_sensor_data(self) -> Dict[str, Any]:
        """è·å–åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®"""
        # è·å–åŸå§‹ç‚¹äº‘æ•°æ®
        raw_point_cloud = self._getDronePointCloud(0)
        
        # é¢„å¤„ç†ç‚¹äº‘æ•°æ®
        point_cloud = self._preprocess_pointcloud(raw_point_cloud)
        
        # è·å–æ— äººæœºçŠ¶æ€
        drone_state = self._getDroneStateVector(0)
        drone_position = self.pos[0]
        drone_velocity = self.vel[0]
        drone_orientation = self.rpy[0]
        
        return {
            'point_cloud': point_cloud,
            'drone_position': drone_position,
            'drone_velocity': drone_velocity,
            'drone_orientation': drone_orientation,
            'drone_state': drone_state,
            'timestamp': time.time()
        }
    def _preprocess_pointcloud(self, point_cloud: np.ndarray) -> np.ndarray:
        """
        é¢„å¤„ç†ç‚¹äº‘æ•°æ® - ä½¿ç”¨ç‹¬ç«‹çš„PointCloudFilter
        
        Args:
            point_cloud: åŸå§‹ç‚¹äº‘ [N, 3]
            
        Returns:
            np.ndarray: é¢„å¤„ç†åçš„ç‚¹äº‘
        """
        # return self.pointcloud_filter.filter(point_cloud)
        return point_cloud

    def _build_observation_with_perception_manager(self, sensor_data: Dict) -> Tuple[np.ndarray, Any]:
        """ä½¿ç”¨PerceptionManageræ„å»º86ç»´è§‚æµ‹å¹¶è¿”å›å®Œæ•´æ„ŸçŸ¥ç»“æœ"""
        # åˆ›å»ºObservationInput
        obs_input = create_observation_input(
            point_cloud=sensor_data['point_cloud'],
            drone_position=sensor_data['drone_position'],
            drone_orientation=sensor_data['drone_orientation'],
            drone_velocity=sensor_data['drone_velocity'],
            action_history=list(self.action_history),
            timestamp=sensor_data['timestamp']
        )
        
        # è·å–å½“å‰å­ç›®æ ‡åºåˆ—
        if self.state_manager.is_ready_for_high_level():
            current_subgoals = self.state_manager.current_subgoal_sequence
        else:
            current_subgoals = np.zeros((5, 2), dtype=np.float32)
        
        # è·å–åŠ¨ä½œå†å²
        if len(self.action_history) > 0:
            action_history = np.array(list(self.action_history), dtype=np.float32)
            if action_history.shape[0] < 6:
                # å¡«å……åˆ°6æ­¥
                padding = np.zeros((6 - action_history.shape[0], 4), dtype=np.float32)
                action_history = np.vstack([padding, action_history])
        else:
            action_history = np.zeros((6, 4), dtype=np.float32)
        
        # é‡è¦ï¼šå…ˆå¤„ç†è§‚æµ‹è·å–å®Œæ•´çš„PerceptionResults
        perception_results = self.perception_manager.process_observation(
            obs_input=obs_input,
            map_manager=self.map_manager
        )
        
        # ä½¿ç”¨PerceptionManageræ„å»º86ç»´è§‚æµ‹
        obs_86d = self.perception_manager.build_86d_observation(
            obs_input=obs_input,
            current_subgoals=current_subgoals,
            action_history=action_history,
            map_manager=self.map_manager
        )
        
        return obs_86d, perception_results
    
    # ============ å¥–åŠ±è®¡ç®—æ–¹æ³• ============
    
    def _compute_exploration_reward(self, structured_state: StructuredState) -> float:
        """è®¡ç®—æ¢ç´¢å¥–åŠ±"""
        if not structured_state.parsed_map_state:
            return 0.0
            
        exploration_rate = structured_state.parsed_map_state.get('exploration_rate', 0.0)
        information_gain = structured_state.parsed_map_state.get('information_gain_potential', 0.0)
        
        # åŸºç¡€æ¢ç´¢å¥–åŠ±
        base_reward = exploration_rate * 0.1
        
        # ä¿¡æ¯å¢ç›Šå¥–åŠ±
        gain_reward = information_gain * 0.05
        
        # æ¢ç´¢å®Œæˆå¤§å¥–åŠ±
        completion_bonus = 1.0 if exploration_rate >= self.config.EXPLORATION_THRESHOLD else 0.0
        
        return base_reward + gain_reward + completion_bonus
    
    

    def _compute_completion_reward(self, structured_state: StructuredState) -> float:
        """è®¡ç®—å®Œæˆå¥–åŠ±"""
        if not structured_state.parsed_map_state:
            return 0.0
            
        exploration_rate = structured_state.parsed_map_state.get('exploration_rate', 0.0)
        
        if exploration_rate >= self.config.EXPLORATION_THRESHOLD:
            return 2.0  # å®Œæˆå¤§å¥–åŠ±
        else:
            # æ¸è¿›å¥–åŠ±
            return exploration_rate * 0.1
        
        
    def _compute_action_consistency(self) -> float:
        """è®¡ç®—åŠ¨ä½œè¿ç»­æ€§ - ç”¨äºæ‰§è¡Œå¥–åŠ±"""
        if self.prev_action is None or self.last_action is None:
            return 0.0
        
        # è®¡ç®—åŠ¨ä½œå·®å¼‚
        action_diff = np.linalg.norm(self.last_action - self.prev_action)
        
        # è½¬æ¢ä¸ºä¸€è‡´æ€§åˆ†æ•° (0-1)ï¼Œå·®å¼‚è¶Šå°ä¸€è‡´æ€§è¶Šé«˜
        consistency = np.exp(-action_diff)  # æŒ‡æ•°è¡°å‡
        
        return float(consistency)

    def _get_last_reward_breakdown(self) -> Dict[str, float]:
        """è·å–æœ€åä¸€æ¬¡å¥–åŠ±è®¡ç®—çš„è¯¦ç»†åˆ†è§£"""
        if not hasattr(self, 'last_reward_breakdown'):
            return {}
        
        return getattr(self, 'last_reward_breakdown', {})

    def _is_success(self) -> bool:
        """åˆ¤æ–­æˆåŠŸæ¡ä»¶ - å¢å¼ºç‰ˆ"""
        # åŸºäºæ¢ç´¢ç‡çš„æˆåŠŸåˆ¤æ–­
        exploration_rate = self.episode_stats.get('exploration_rate', 0.0)
        exploration_success = exploration_rate >= self.config.EXPLORATION_THRESHOLD
        
        # åŸºäºå®‰å…¨æ€§çš„æˆåŠŸåˆ¤æ–­
        safety_success = True
        if self.latest_perception_results and self.latest_perception_results.safety_results:
            safety_results = self.latest_perception_results.safety_results
            safety_success = (not safety_results.collision_detected and 
                            not safety_results.out_of_bounds and
                            safety_results.is_safe)
        
        return exploration_success and safety_success

    # ============ å¢å¼ºå¥–åŠ±è®¡ç®—æ–¹æ³• ============

    def _compute_safety_reward(self, safety_results) -> float:
        """åŸºäºPerceptionResults.safety_resultsè®¡ç®—å®‰å…¨å¥–åŠ±"""
        # åŸºäºPerceptionResultsçš„å®‰å…¨çŠ¶æ€
        if safety_results.collision_detected:
            self.episode_stats['collision_count'] += 1
            return -2.0  # ä¸¥é‡ç¢°æ’æƒ©ç½š
        
        if safety_results.should_terminate:
            return -1.0  # ç»ˆæ­¢çŠ¶æ€æƒ©ç½š
        
        # åŸºäºæœ€å°éšœç¢ç‰©è·ç¦»çš„å¥–åŠ±
        min_distance = safety_results.min_obstacle_distance
        
        if min_distance < self.config.COLLISION_THRESHOLD:
            # å±é™©æ¥è¿‘
            self.episode_stats['safety_violations'] += 1
            return -0.5
        elif min_distance < self.config.SAFETY_THRESHOLD:
            # å®‰å…¨è·ç¦»ä¸è¶³
            return -0.1
        elif min_distance > self.config.SAFETY_THRESHOLD * 2:
            # è‰¯å¥½çš„å®‰å…¨è·ç¦»
            return 0.2
        else:
            # åŸºæœ¬å®‰å…¨çŠ¶æ€
            return 0.1

    def _computeReward(self):
        """è®¡ç®—åˆ†å±‚å¥–åŠ± - å¢å¼ºç‰ˆï¼Œè®°å½•å¥–åŠ±åˆ†è§£"""
        structured_state = self.current_structured_state
        perception_results = self.latest_perception_results
        
        # 1. æ¢ç´¢å¥–åŠ±
        exploration_reward = self._compute_exploration_reward(structured_state)
        
        # 2. å®‰å…¨å¥–åŠ±ï¼ˆä½¿ç”¨PerceptionResultsï¼‰
        safety_reward = self._compute_safety_reward(perception_results.safety_results)
        
        # 3. æ‰§è¡Œå¥–åŠ± (åŒ…å«åŠ¨ä½œè¿ç»­æ€§)
        execution_reward = self._compute_execution_reward(structured_state)
        
        # 4. å®Œæˆå¥–åŠ±
        completion_reward = self._compute_completion_reward(structured_state)
        
        # åŠ æƒæ€»å¥–åŠ±
        total_reward = (
            self.config.REWARD_WEIGHTS['exploration'] * exploration_reward +
            self.config.REWARD_WEIGHTS['safety'] * safety_reward +
            self.config.REWARD_WEIGHTS['execution'] * execution_reward +
            self.config.REWARD_WEIGHTS['completion'] * completion_reward
        )
        
        # ä¿å­˜å¥–åŠ±åˆ†è§£ç”¨äºè½¨è¿¹è®°å½•
        self.last_reward_breakdown = {
            'exploration': exploration_reward,
            'safety': safety_reward,
            'execution': execution_reward,
            'completion': completion_reward,
            'total': total_reward
        }
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.episode_stats['total_reward'] += total_reward
        self.reward_history.append(total_reward)
        
        # è¯¦ç»†å¥–åŠ±è®°å½•
        if self.episode_step % 50 == 0:
            self.logger.debug(f"æ­¥éª¤ {self.episode_step} å¥–åŠ±åˆ†è§£: "
                            f"æ¢ç´¢={exploration_reward:.3f}, å®‰å…¨={safety_reward:.3f}, "
                            f"æ‰§è¡Œ={execution_reward:.3f}, å®Œæˆ={completion_reward:.3f}, "
                            f"æ€»è®¡={total_reward:.3f}")
        
        return np.array([total_reward])

    def _compute_execution_reward(self, structured_state: StructuredState) -> float:
        """è®¡ç®—æ‰§è¡Œå¥–åŠ±"""
        base_reward = 0.0
        
        # åŸºç¡€æ‰§è¡Œå¥–åŠ±
        if structured_state.motion_context:
            motion_consistency = structured_state.motion_context.get('motion_consistency', 0.0)
            speed_stability = structured_state.motion_context.get('speed_stability', 0.0)
            base_reward = 0.05 * (motion_consistency + speed_stability)
        
        # åŠ¨ä½œè¿ç»­æ€§å¥–åŠ±
        action_consistency = self._compute_action_consistency()
        consistency_reward = 0.02 * action_consistency
        
        # é€Ÿåº¦é€‚åº”æ€§å¥–åŠ±
        velocity_reward = self._compute_velocity_reward()
        
        return base_reward + consistency_reward + velocity_reward
    
    def _compute_velocity_reward(self) -> float:
        """è®¡ç®—é€Ÿåº¦å¥–åŠ±"""
        current_velocity = self.vel[0]
        speed = np.linalg.norm(current_velocity)
        
        # é€‚å½“é€Ÿåº¦åŒºé—´å¥–åŠ±
        if 1 <= speed <= 4:
            return 0.1
        elif speed < 0.2:
            return 0.02
        else:
            return -0.05  # è¿‡å¿«æƒ©ç½š
    
    def get_trajectory_step_data(self, drone_idx: int = 0) -> Dict[str, Any]:
        """è·å–è½¨è¿¹è®°å½•æ‰€éœ€çš„5å…ƒç»„æ•°æ®"""
        # 1. å½“å‰ä½ç½® (current_position)
        current_position = self.pos[drone_idx].tolist()  # [x, y, z]
        
        # 2. å½“å‰é€Ÿåº¦ (current_velocity)  
        current_velocity = self.vel[drone_idx].tolist()  # [vx, vy, vz]
        
        # 3. ç›®æ ‡é€Ÿåº¦ (target_velocity) - ä»æ§åˆ¶å™¨è·å–
        if hasattr(self, 'ctrl') and self.ctrl and hasattr(self.ctrl[drone_idx], 'target_velocity'):
            target_velocity = self.ctrl[drone_idx].target_velocity.tolist()  # [vx, vy, vz, yaw_rate]
        else:
            target_velocity = [0.0, 0.0, 0.0, 0.0]  # é»˜è®¤å€¼
            
        # 4. æ¨¡å‹è¾“å‡ºåŠ¨ä½œ (model_action)
        model_action = getattr(self, 'last_action', np.zeros(4)).tolist()  # [vx, vy, vz, yaw_rate]
        
        # 5. ç”µæœºè¾“å‡º (rpm_action) - ä»æœ€è¿‘çš„è®¡ç®—ä¸­è·å–
        if hasattr(self, 'last_rpm') and self.last_rpm is not None:
            rpm_action = self.last_rpm[drone_idx].tolist()  # [rpm1, rpm2, rpm3, rpm4]
        else:
            rpm_action = [0.0, 0.0, 0.0, 0.0]  # é»˜è®¤å€¼
            
        # æ„å»ºç²¾ç®€çš„è½¨è¿¹æ•°æ® - ä»…ä¿å­˜5å…ƒç»„å’ŒåŸºæœ¬ä¿¡æ¯
        trajectory_data = {
            # åŸºæœ¬ä¿¡æ¯
            'step': self.episode_step,
            'exploration_rate': self.episode_stats.get('exploration_rate', 0.0),
            
            # æ ¸å¿ƒ5å…ƒç»„æ•°æ®
            'current_position': current_position,
            'current_velocity': current_velocity, 
            'target_velocity': target_velocity,
            'model_action': model_action,
            'rpm_action': rpm_action
        }
        
        return trajectory_data
    
    def getPyBulletClient(self):
        """è·å–PyBulletå®¢æˆ·ç«¯ - å…¼å®¹BaseRLAviary"""
        return getattr(self, '_p', None) or getattr(self, 'CLIENT', None)
    
    def _getDronePointCloud(self, drone_idx: int) -> np.ndarray:
        """è·å–æ— äººæœºç‚¹äº‘æ•°æ® - å…¼å®¹æ–¹æ³•"""
        # ä½¿ç”¨BaseRLAviaryçš„æ¿€å…‰é›·è¾¾æ–¹æ³•
        return self._getDroneRays(drone_idx)
    
    # ============ è¯Šæ–­å’Œè°ƒè¯•æ–¹æ³• ============
    
    def get_system_diagnostics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿè¯Šæ–­ä¿¡æ¯"""
        return {
            'episode_step': self.episode_step,
            'episode_count': self.episode_count,
            'episode_stats': self.episode_stats.copy(),
            'action_history_length': len(self.action_history),
            'reward_history_length': len(self.reward_history),
            'state_manager_status': {
                'history_length': len(self.state_manager.state_history),
                'is_ready_for_high_level': self.state_manager.is_ready_for_high_level(),
                'should_update_high_level': self.state_manager.should_update_high_level(),
                'step_counter': self.state_manager.step_counter
            }
        }
    
    def validate_observation_structure(self, obs: np.ndarray) -> Dict[str, bool]:
        """éªŒè¯è§‚æµ‹ç»“æ„"""
        return {
            'correct_dimension': obs.shape[-1] == self.config.OBSERVATION_DIM,
            'no_nan_values': not np.any(np.isnan(obs)),
            'no_inf_values': not np.any(np.isinf(obs)),
            'finite_values': np.all(np.isfinite(obs))
        }

    # ============ TensorBoardè®°å½•æ–¹æ³• ============
    
    def _log_step_data_to_tensorboard(self, action: np.ndarray, reward: float, info: Dict):
        """è®°å½•æ­¥éª¤çº§æ•°æ®åˆ°TensorBoard"""
        
        # 1. å¥–åŠ±ç»„ä»¶è®°å½•
        reward_breakdown = getattr(self, 'last_reward_breakdown', {})
        self.tensorboard_logger.log_reward_components(self.global_step, reward_breakdown)
        
        # 2. ä½ç½®å’ŒåŠ¨ä½œæ•°æ®
        position_data = (self.pos[0][0], self.pos[0][1], self.pos[0][2], self.rpy[0][2])  # x,y,z,yaw
        action_data = (action[0], action[1], action[2], action[3])  # vx,vy,vz,yaw_rate
        
        self.tensorboard_logger.log_position_data(self.global_step, position_data)
        self.tensorboard_logger.log_action_data(self.global_step, action_data)
        
        # 3. å®‰å…¨æ•°æ®
        safety_info = info.get('perception_safety', {})
        safety_data = {
            'min_distance': safety_info.get('min_obstacle_distance', 10.0),
            'is_safe': safety_info.get('is_safe', True),
            'collision': safety_info.get('collision_detected', False)
        }
        self.tensorboard_logger.log_safety_data(self.global_step, safety_data)
        
        # 4. æ„ŸçŸ¥æ•°æ®
        perception_info = {
            'local_map_coverage': info.get('perception_map', {}).get('exploration_rate', 0.0),
            'information_gain': reward_breakdown.get('exploration', 0.0),
            'point_cloud_size': getattr(self.latest_perception_results, 'point_cloud_size', 0) if self.latest_perception_results else 0
        }
        self.tensorboard_logger.log_perception_data(self.global_step, perception_info)
        
        # 5. çŠ¶æ€ç®¡ç†æ•°æ®
        state_metrics = {
            'history_length': len(self.state_manager.state_history) if hasattr(self, 'state_manager') else 0,
            'high_level_ready': float(self.state_manager.is_ready_for_high_level()) if hasattr(self, 'state_manager') else 0.0,
            'should_update_high_level': float(self.state_manager.should_update_high_level()) if hasattr(self, 'state_manager') else 0.0,
            'step_counter': getattr(self.state_manager, 'step_counter', 0) if hasattr(self, 'state_manager') else 0
        }
        self.tensorboard_logger.log_training_metrics(self.global_step, state_metrics)
        
        # 6. åŠ¨ä½œè¿ç»­æ€§å’Œé€Ÿåº¦æ•°æ®
        execution_metrics = {
            'action_consistency': self._compute_action_consistency(),
            'velocity_magnitude': np.linalg.norm(self.vel[0]),
            'angular_velocity_magnitude': np.linalg.norm(self.rpy[0])
        }
        self.tensorboard_logger.log_training_metrics(self.global_step, execution_metrics)

    def _log_episode_data_to_tensorboard(self):
        """è®°å½•å›åˆçº§æ•°æ®åˆ°TensorBoard"""
        episode_data = {
            'total_reward': self.episode_stats.get('total_reward', 0.0),
            'length': self.episode_step,
            'exploration_rate': self.episode_stats.get('exploration_rate', 0.0),
            'success': self._is_success(),
            'collision_count': self.episode_stats.get('collision_count', 0),
            'safety_violations': self.episode_stats.get('safety_violations', 0),
            'completion_status': self.episode_stats.get('completion_status', False)
        }
        
        self.tensorboard_logger.log_episode_data(self.episode_count, episode_data)

    def _determine_termination_reason(self, terminated: bool, info: Dict) -> str:
        """ç¡®å®šç»ˆæ­¢åŸå› """
        if not terminated:
            return "max_steps"
        
        safety_info = info.get('perception_safety', {})
        if safety_info.get('collision_detected', False):
            return "collision"
        elif safety_info.get('out_of_bounds', False):
            return "out_of_bounds"
        elif info.get('perception_map', {}).get('exploration_rate', 0.0) >= self.config.EXPLORATION_THRESHOLD:
            return "exploration_complete"
        else:
            return "safety_violation"

    def _compute_action_consistency(self) -> float:
        """è®¡ç®—åŠ¨ä½œè¿ç»­æ€§æŒ‡æ ‡"""
        if hasattr(self, 'prev_action') and self.prev_action is not None and hasattr(self, 'last_action'):
            diff = np.linalg.norm(self.last_action - self.prev_action)
            return 1.0 / (1.0 + diff)
        return 1.0

    def start_tensorboard_logging(self):
        """å¯åŠ¨TensorBoardè®°å½•"""
        if self.tensorboard_logger:
            self.tensorboard_logger.start()
            self.logger.info("TensorBoardè®°å½•å·²å¯åŠ¨")

    def close_tensorboard_logging(self):
        """å…³é—­TensorBoardè®°å½•"""
        if self.tensorboard_logger:
            self.tensorboard_logger.close()
            self.logger.info("TensorBoardè®°å½•å·²å…³é—­")

    def close(self):
        """å…³é—­ç¯å¢ƒï¼ŒåŒ…æ‹¬TensorBoard"""
        if self.tensorboard_logger:
            self.close_tensorboard_logging()
        super().close()
