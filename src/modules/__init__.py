"""
modules/__init__.py - åŸºäºç°æœ‰ç»„ä»¶çš„å®Œæ•´é›†æˆ

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š
1. å®Œå…¨å¤ç”¨ç°æœ‰çš„HierarchicalPolicyã€StateManagerã€HierarchicalRolloutBuffer
2. é€šè¿‡é…ç½®å’Œç®¡ç†å™¨ç±»æä¾›ç»Ÿä¸€æ¥å£
3. ä¸é‡æ–°åˆ›å»ºç½‘ç»œï¼Œåªåšç»„ä»¶é—´çš„æ•°æ®æµæ•´åˆ
4. åŸºäºStateManagerçš„86ç»´è§‚æµ‹æ•°æ®æµæ¶æ„

é‡æ„æ—¥æœŸ: 2025å¹´8æœˆ15æ—¥
ä½œè€…: HA-UAVå›¢é˜Ÿ
"""

import logging
import numpy as np
import torch
import torch.nn.functional as F
import gymnasium as gym
from typing import Dict, List, Tuple, Any, Optional, Union, TYPE_CHECKING
from dataclasses import dataclass, asdict
from pathlib import Path
import json

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å¯¼å…¥ç°æœ‰æ ¸å¿ƒç»„ä»¶
try:
    # StateManagerå’ŒStructuredState
    from .StateManager import StateManager, StructuredState
    
    # HierarchicalPolicyç›¸å…³ç»„ä»¶
    from .HierarchicalPolicy import (
        HierarchicalPolicyNetwork,
        HierarchicalFeaturesExtractor, 
        HierarchicalPolicy
    )
    
    # åˆ†å±‚ç¼“å†²åŒº
    from .HierarchicalRolloutBuffer import (
        HierarchicalRolloutBuffer,
        HierarchicalRolloutBufferSamples,
        create_hierarchical_rollout_buffer
    )
    
    # HA_Modulesç¥ç»ç½‘ç»œç»„ä»¶
    from .HA_Modules import HierarchicalRLSystem
    
    logger.debug("æ‰€æœ‰ç°æœ‰ç»„ä»¶å¯¼å…¥æˆåŠŸ")
    
    # Bç»„æ¶ˆèå®éªŒç»„ä»¶
    from .ablation.ablation_config import (
        AblationConfig,
        AblationConfigManager,
        get_ablation_config,
        list_ablation_experiments,
        create_b1_config,
        create_b2_config,
        create_b3_config,
        create_baseline_config
    )
    
    from .ablation.ablation_adapter import (
        AblationStateManagerAdapter,
        AblationPolicyWrapper,
        AblationBufferAdapter,
        AblationComponentsManager,
        create_ablation_system
    )
    
    from .ablation.direct_control_policy import (
        DirectControlPolicy,
        DirectControlPolicyNetwork,
        create_direct_control_policy
    )
    
    from .ablation.flat_policy import (
        FlatPolicy,
        FlatPolicyNetwork,
        create_flat_policy
    )
    
    from .ablation.single_step_hierarchical_policy import (
        SingleStepHierarchicalPolicy,
        SingleStepHighLevelNetwork,
        SingleStepLowLevelNetwork,
        create_single_step_hierarchical_policy
    )
    
    logger.debug("Bç»„æ¶ˆèå®éªŒç»„ä»¶å¯¼å…¥æˆåŠŸ")
    
except ImportError as e:
    logger.error(f"ç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    raise ImportError(f"æ— æ³•å¯¼å…¥å¿…è¦çš„æ¨¡å—ç»„ä»¶: {e}")


@dataclass
class ModelConfiguration:
    """ç»Ÿä¸€æ¨¡å‹é…ç½®ç±» - åŸºäºç°æœ‰ç»„ä»¶æ¶æ„"""
    
    # StateManageré…ç½®
    state_manager_config: Dict[str, Any] = None
    
    # HierarchicalPolicyé…ç½®  
    policy_config: Dict[str, Any] = None
    
    # HierarchicalRolloutBufferé…ç½®
    buffer_config: Dict[str, Any] = None
    
    # è®­ç»ƒé…ç½®
    training_config: Dict[str, Any] = None
    
    # ç¯å¢ƒé…ç½®
    env_config: Dict[str, Any] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤é…ç½®"""
        if self.state_manager_config is None:
            self.state_manager_config = {
                'history_length': 20,                    # K=20æ­¥çŠ¶æ€å†å²
                'high_level_update_frequency': 5,        # Ï„=5æ­¥é«˜å±‚æ›´æ–°é¢‘ç‡  
                'future_horizon': 5                      # T=5æ­¥æœªæ¥å­ç›®æ ‡æ•°é‡
            }
        
        if self.policy_config is None:
            self.policy_config = {
                'features_dim': 512,
                'net_arch': dict(pi=[256, 256], vf=[256, 256]),
                'activation_fn': 'ReLU',
                'learning_rate': 3e-4
            }
        
        if self.buffer_config is None:
            self.buffer_config = {
                'buffer_size': 2048,
                'gae_lambda': 0.95,
                'gamma': 0.99,
                'hierarchical_config': {
                    'high_level_update_freq': 5,
                    'history_length': 5
                }
            }
        
        if self.training_config is None:
            self.training_config = {
                'algorithm': 'PPO',
                'batch_size': 64,
                'n_epochs': 10,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5
            }
        
        if self.env_config is None:
            self.env_config = {
                'observation_dim': 86,
                'action_dim': 4,
                'max_episode_steps': 1000
            }
    
    def save(self, filepath: str):
        """ä¿å­˜é…ç½®"""
        config_dict = asdict(self)
        with open(filepath, 'w') as f:
            json.dump(config_dict, f, indent=2)
        logger.info(f"é…ç½®å·²ä¿å­˜åˆ°: {filepath}")
    
    @classmethod  
    def load(cls, filepath: str) -> 'ModelConfiguration':
        """åŠ è½½é…ç½®"""
        with open(filepath, 'r') as f:
            config_dict = json.load(f)
        return cls(**config_dict)


class HAComponentsManager:
    """HAç»„ä»¶ç®¡ç†å™¨ - åŸºäºç°æœ‰ç»„ä»¶çš„é›†æˆç®¡ç†"""
    
    def __init__(self, config: Optional[ModelConfiguration] = None):
        """åˆå§‹åŒ–ç»„ä»¶ç®¡ç†å™¨
        
        Args:
            config: æ¨¡å‹é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or ModelConfiguration()
        
        # ç»„ä»¶å®ä¾‹
        self.state_manager: Optional[StateManager] = None
        self.policy: Optional[HierarchicalPolicy] = None  
        self.buffer: Optional[HierarchicalRolloutBuffer] = None
        self.ha_modules: Optional[HierarchicalRLSystem] = None
        
        # ç¯å¢ƒä¿¡æ¯
        self.observation_space: Optional[gym.Space] = None
        self.action_space: Optional[gym.Space] = None
        
        # è¿è¡ŒçŠ¶æ€
        self.is_initialized = False
        self.training_mode = True
        
        logger.debug("HAComponentsManageråˆå§‹åŒ–å®Œæˆ")
    
    def initialize_components(self, env: gym.Env) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        
        Args:
            env: Gymç¯å¢ƒå®ä¾‹
            
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            # å¼€å§‹åˆå§‹åŒ–HAç»„ä»¶
            
            # 1. è·å–ç¯å¢ƒä¿¡æ¯
            self.observation_space = env.observation_space
            self.action_space = env.action_space
            
            # éªŒè¯ç¯å¢ƒç»´åº¦
            assert self.observation_space.shape == (86,), f"è§‚æµ‹ç»´åº¦å¿…é¡»æ˜¯86ï¼Œå®é™…ä¸º{self.observation_space.shape}"
            assert self.action_space.shape == (4,), f"åŠ¨ä½œç»´åº¦å¿…é¡»æ˜¯4ï¼Œå®é™…ä¸º{self.action_space.shape}"
            
            # 2. åˆå§‹åŒ–StateManager
            self.state_manager = StateManager(**self.config.state_manager_config)
            logger.debug("StateManageråˆå§‹åŒ–å®Œæˆ")
            
            # 3. åˆå§‹åŒ–HierarchicalPolicy
            # ç¡®ä¿learning_rateæ˜¯æµ®ç‚¹æ•°
            learning_rate = float(self.config.policy_config['learning_rate'])
            
            self.policy = HierarchicalPolicy(
                observation_space=self.observation_space,
                action_space=self.action_space,
                lr_schedule=lambda _: learning_rate,
                features_extractor_class=HierarchicalFeaturesExtractor,
                features_extractor_kwargs={'features_dim': self.config.policy_config['features_dim']},
                net_arch=self.config.policy_config['net_arch']
            )
            logger.debug("HierarchicalPolicyåˆå§‹åŒ–å®Œæˆ")
            
            # 4. åˆå§‹åŒ–HierarchicalRolloutBuffer
            self.buffer = create_hierarchical_rollout_buffer(
                buffer_size=self.config.buffer_config['buffer_size'],
                observation_space=self.observation_space,
                action_space=self.action_space,
                gae_lambda=self.config.buffer_config['gae_lambda'],
                gamma=self.config.buffer_config['gamma'],
                hierarchical_config=self.config.buffer_config['hierarchical_config']
            )
            logger.debug("HierarchicalRolloutBufferåˆå§‹åŒ–å®Œæˆ")
            
            # 5. åˆå§‹åŒ–HA_Modulesï¼ˆç”¨äºé«˜çº§ç¥ç»ç½‘ç»œç»„ä»¶ï¼‰
            ha_config = {
                'lidar_dim': 36,
                'action_dim': 4,
                'grid_size': 32,
                'yaw_history_len': 20,
                'state_dim': 256,
                'subgoal_dim': 10
            }
            self.ha_modules = HierarchicalRLSystem(config=ha_config)
            logger.debug("HierarchicalRLSystemåˆå§‹åŒ–å®Œæˆ")
            
            # 6. å»ºç«‹ç»„ä»¶é—´è¿æ¥
            self._establish_component_connections()
            
            self.is_initialized = True
            logger.debug("æ‰€æœ‰HAç»„ä»¶åˆå§‹åŒ–æˆåŠŸï¼")
            return True
            
        except Exception as e:
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            self.is_initialized = False
            return False
    
    def _establish_component_connections(self):
        """å»ºç«‹ç»„ä»¶é—´çš„æ•°æ®æµè¿æ¥"""
        try:
            # å°†StateManageræ³¨å…¥åˆ°Policyçš„ç‰¹å¾æå–å™¨ä¸­
            if hasattr(self.policy, 'features_extractor'):
                if hasattr(self.policy.features_extractor, 'policy_network'):
                    # å…±äº«åŒä¸€ä¸ªStateManagerå®ä¾‹
                    self.policy.features_extractor.policy_network.state_manager = self.state_manager
                    logger.info("StateManagerå·²æ³¨å…¥åˆ°HierarchicalPolicy")
            
            # å°†StateManageræ³¨å…¥åˆ°Bufferä¸­ï¼ˆå·²ç»åœ¨Bufferåˆå§‹åŒ–æ—¶åˆ›å»ºäº†è‡ªå·±çš„å®ä¾‹ï¼‰
            # è¿™é‡Œæˆ‘ä»¬ä¿æŒBufferçš„ç‹¬ç«‹StateManagerï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            logger.debug("ç»„ä»¶è¿æ¥å»ºç«‹å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"ç»„ä»¶è¿æ¥å»ºç«‹éƒ¨åˆ†å¤±è´¥: {e}")
    
    def predict(self, observation: np.ndarray) -> np.ndarray:
        """ç»Ÿä¸€é¢„æµ‹æ¥å£
        
        Args:
            observation: [86] è§‚æµ‹å‘é‡
            
        Returns:
            np.ndarray: [4] åŠ¨ä½œå‘é‡
        """
        if not self.is_initialized:
            raise RuntimeError("ç»„ä»¶æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize_components()")
        
        # ä½¿ç”¨HierarchicalPolicyè¿›è¡Œé¢„æµ‹
        action, _ = self.policy.predict(observation, deterministic=not self.training_mode)
        return action
    
    def collect_rollout(self, env: gym.Env, n_steps: int = None, trajectory_callback=None) -> Dict[str, Any]:
        """æ”¶é›†ç»éªŒæ•°æ® - å‚è€ƒHAUAVTrainerçš„ç»éªŒæ”¶é›†æ¨¡å¼ï¼Œæ”¯æŒè½¨è¿¹è®°å½•
        
        Args:
            env: ç¯å¢ƒå®ä¾‹
            n_steps: å›ºå®šæ”¶é›†æ­¥æ•°
            trajectory_callback: è½¨è¿¹è®°å½•å›è°ƒå‡½æ•°ï¼Œç­¾åä¸º (obs, action, reward, next_obs, done, info)
            
        Returns:
            dict: æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.is_initialized:
            raise RuntimeError("ç»„ä»¶æœªåˆå§‹åŒ–")
        
        if n_steps is None:
            n_steps = self.config.buffer_config['buffer_size']
        
        # æ”¶é›†ç»Ÿè®¡
        stats = {
            'total_steps': 0,
            'episodes': 0,
            'total_reward': 0.0,
            'episode_rewards': []
        }
        
        # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šç¡®ä¿ç¯å¢ƒçŠ¶æ€åˆå§‹åŒ–
        if not hasattr(self, '_current_obs') or self._current_obs is None:
            logger.info("åˆå§‹åŒ–ç¯å¢ƒçŠ¶æ€")
            obs, _ = env.reset()
            self._current_obs = obs
            self._episode_reward = 0.0
            # è½¨è¿¹è®°å½•ï¼šæ–°episodeå¼€å§‹
            if trajectory_callback and hasattr(trajectory_callback, '_start_trajectory_episode'):
                trajectory_callback._start_trajectory_episode()
        
        obs = self._current_obs
        episode_reward = self._episode_reward
        
        # ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šæŒ‰å›ºå®šæ­¥æ•°æ”¶é›†ï¼Œä¸ç®¡episodeæ˜¯å¦ç»“æŸ
        logger.info(f"å¼€å§‹æ”¶é›†ç»éªŒ: ç›®æ ‡{n_steps}æ­¥")
        
        for step in range(n_steps):
            # 1. åŠ¨ä½œé¢„æµ‹
            action = self.predict(obs)
            
            # 2. ç¯å¢ƒäº¤äº’
            next_obs, reward, done, truncated, info = env.step(action)
            episode_reward += reward
            
            # 3. è½¨è¿¹è®°å½•ï¼šè®°å½•å•æ­¥æ•°æ®
            if trajectory_callback:
                try:
                    trajectory_callback._log_trajectory_step(obs, action, reward, next_obs, done or truncated, info)
                except Exception as e:
                    logger.warning(f"è½¨è¿¹è®°å½•å¤±è´¥: {e}")
            
            # 4. å­˜å‚¨ç»éªŒï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            self.buffer.add(
                obs=obs.reshape(1, -1),
                action=action.reshape(1, -1),
                reward=np.array([reward]),
                episode_start=np.array([step == 0 and stats['total_steps'] == 0]),
                value=torch.zeros(1),  # åç»­è¡¥å……
                log_prob=torch.zeros(1),  # åç»­è¡¥å……
                should_update_high_level=False
            )
            
            stats['total_steps'] += 1
            obs = next_obs
            
            # 5. å¤„ç†episodeç»“æŸï¼ˆä½†ä¸ä¸­æ–­æ”¶é›†ï¼‰
            if done or truncated:
                stats['episodes'] += 1
                stats['episode_rewards'].append(episode_reward)
                stats['total_reward'] += episode_reward
                
                logger.info(f"Episode {stats['episodes']} å®Œæˆï¼Œå¥–åŠ±: {episode_reward:.2f}")
                
                # è½¨è¿¹è®°å½•ï¼šå®Œæˆepisode
                if trajectory_callback and hasattr(trajectory_callback, '_finalize_trajectory_episode'):
                    trajectory_callback._finalize_trajectory_episode(episode_reward, stats['total_steps'], info)
                
                # é‡ç½®ç¯å¢ƒç»§ç»­æ”¶é›†
                obs, _ = env.reset()
                episode_reward = 0.0
                
                # è½¨è¿¹è®°å½•ï¼šæ–°episodeå¼€å§‹
                if trajectory_callback and hasattr(trajectory_callback, '_start_trajectory_episode'):
                    trajectory_callback._start_trajectory_episode()
            
            # æ¯100æ­¥æŠ¥å‘Šè¿›åº¦
            if (step + 1) % 100 == 0:
                logger.info(f"æ”¶é›†è¿›åº¦: {step + 1}/{n_steps} ({(step + 1)/n_steps:.1%})")
        
        # ä¿å­˜å½“å‰çŠ¶æ€
        self._current_obs = obs
        self._episode_reward = episode_reward
        
        # è®¡ç®—GAEï¼ˆç®€åŒ–å¤„ç†ï¼‰
        with torch.no_grad():
            last_values = torch.zeros(1)
            self.buffer.compute_returns_and_advantage(last_values, np.array([False]))
        
        # ç»Ÿè®¡ä¿¡æ¯
        if stats['episodes'] > 0:
            stats['mean_reward'] = stats['total_reward'] / stats['episodes']
        else:
            # æœªå®Œæˆepisodeï¼Œä½¿ç”¨å½“å‰ç´¯ç§¯å¥–åŠ±
            stats['mean_reward'] = episode_reward
            stats['total_reward'] = episode_reward
        
        logger.info(f"ç»éªŒæ”¶é›†å®Œæˆ: {stats['total_steps']}æ­¥, {stats['episodes']}ä¸ªå®Œæ•´episode, "
                   f"å¹³å‡å¥–åŠ±: {stats['mean_reward']:.3f}")
        
        return stats
    
    def get_training_data(self) -> HierarchicalRolloutBufferSamples:
        """è·å–è®­ç»ƒæ•°æ®
        
        Returns:
            HierarchicalRolloutBufferSamples: åˆ†å±‚è®­ç»ƒæ•°æ®æ ·æœ¬
        """
        if not self.is_initialized or not self.buffer.full:
            raise RuntimeError("ç»„ä»¶æœªåˆå§‹åŒ–æˆ–bufferæœªæ»¡")
        
        # è¿”å›ç¬¬ä¸€ä¸ªbatchçš„æ•°æ®
        for batch in self.buffer.get(batch_size=self.config.training_config['batch_size']):
            return batch
    
    def update_policy(self, rollout_data: HierarchicalRolloutBufferSamples) -> Dict[str, float]:
        """å®Œæ•´çš„åˆ†å±‚PPOç­–ç•¥æ›´æ–°
        
        Args:
            rollout_data: åˆ†å±‚è®­ç»ƒæ•°æ®
            
        Returns:
            dict: è¯¦ç»†è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if not self.is_initialized:
            raise RuntimeError("ç»„ä»¶æœªåˆå§‹åŒ–")
        
        # è®­ç»ƒé…ç½®
        clip_range = self.config.training_config.get('clip_range', 0.2)
        vf_coef = self.config.training_config.get('vf_coef', 0.5)
        ent_coef = self.config.training_config.get('ent_coef', 0.01)
        max_grad_norm = self.config.training_config.get('max_grad_norm', 0.5)
        
        # ç´¯ç§¯ç»Ÿè®¡ä¿¡æ¯
        policy_losses = []
        value_losses = []
        entropy_losses = []
        high_level_losses = []
        low_level_losses = []
        approx_kl_divs = []
        clip_fractions = []
        
        # === 1. æ ‡å‡†PPOæ›´æ–°ï¼ˆç»¼åˆç­–ç•¥ï¼‰ ===
        with torch.no_grad():
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„å€¼
            values, log_probs, entropy = self.policy.evaluate_actions(
                rollout_data.observations, 
                rollout_data.actions
            )
            
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            ratio = torch.exp(log_probs - rollout_data.old_log_prob)
            
            # KLæ•£åº¦ä¼°è®¡
            approx_kl = ((rollout_data.old_log_prob - log_probs).mean()).item()
            approx_kl_divs.append(approx_kl)
        
        # PPOè£å‰ªç›®æ ‡
        surr1 = ratio * rollout_data.advantages
        surr2 = torch.clamp(ratio, 1 - clip_range, 1 + clip_range) * rollout_data.advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # ä»·å€¼å‡½æ•°æŸå¤±
        if self.config.training_config.get('clip_range_vf') is not None:
            # è£å‰ªä»·å€¼å‡½æ•°
            clip_range_vf = self.config.training_config['clip_range_vf']
            values_clipped = rollout_data.old_values + torch.clamp(
                values - rollout_data.old_values, -clip_range_vf, clip_range_vf
            )
            value_loss_1 = F.mse_loss(values, rollout_data.returns)
            value_loss_2 = F.mse_loss(values_clipped, rollout_data.returns)
            value_loss = torch.max(value_loss_1, value_loss_2).mean()
        else:
            value_loss = F.mse_loss(values.squeeze(), rollout_data.returns)
        
        # ç†µæŸå¤±
        entropy_loss = -entropy.mean()
        
        # æ€»æŸå¤±
        total_loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss
        
        # === 2. åˆ†å±‚ç‰¹å®šæ›´æ–° ===
        high_level_loss = torch.tensor(0.0, device=rollout_data.observations.device)
        low_level_loss = torch.tensor(0.0, device=rollout_data.observations.device)
        
        # é«˜å±‚ç­–ç•¥æ›´æ–°ï¼ˆåŸºäºæ›´æ–°æ©ç ï¼‰
        if rollout_data.high_level_update_mask.sum() > 0:
            high_level_indices = rollout_data.high_level_update_mask.bool()
            
            if high_level_indices.any():
                # é«˜å±‚è§‚æµ‹å’ŒåŠ¨ä½œ
                hl_obs = rollout_data.high_level_observations[high_level_indices]  # [N_hl, K, 28]
                hl_actions = rollout_data.high_level_actions[high_level_indices]   # [N_hl, 10]
                hl_advantages = rollout_data.high_level_advantages[high_level_indices]  # [N_hl]
                hl_returns = rollout_data.high_level_returns[high_level_indices]    # [N_hl]
                hl_old_values = rollout_data.high_level_values[high_level_indices]  # [N_hl]
                
                # é€šè¿‡ç­–ç•¥ç½‘ç»œè·å–é«˜å±‚ç‰¹å¾
                if hasattr(self.policy, 'features_extractor') and hasattr(self.policy.features_extractor, 'policy_network'):
                    policy_net = self.policy.features_extractor.policy_network
                    
                    # æ¨¡æ‹Ÿé«˜å±‚ç½‘ç»œå‰å‘ä¼ æ’­ï¼ˆéœ€è¦å æ®æ …æ ¼å’Œyawå†å²ï¼‰
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦ä»hl_obsé‡æ„åŸå§‹è¾“å…¥
                    batch_size = hl_obs.size(0)
                    
                    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥ï¼ˆå®é™…åº”è¯¥ä»StateManagerè·å–ï¼‰
                    occupancy_grids = torch.randn(batch_size, 5, 8, 8, device=hl_obs.device)
                    yaw_histories = torch.randn(batch_size, 5, device=hl_obs.device)
                    
                    try:
                        # é«˜å±‚å‰å‘ä¼ æ’­
                        hl_subgoals, hl_values_pred = policy_net.forward_high_level(occupancy_grids)
                        
                        # é«˜å±‚ä»·å€¼æŸå¤±
                        hl_value_loss = F.mse_loss(hl_values_pred.squeeze(), hl_returns)
                        
                        # é«˜å±‚ç­–ç•¥æŸå¤±ï¼ˆåŸºäºå­ç›®æ ‡è´¨é‡ï¼‰
                        # è¿™é‡Œç®€åŒ–ä¸ºL2æŸå¤±ï¼Œå®é™…åº”è¯¥æ˜¯ç­–ç•¥æ¢¯åº¦
                        target_subgoals = hl_actions.view(batch_size, 5, 2)  # é‡å¡‘ä¸º[N, T, 2]
                        hl_policy_loss = F.mse_loss(hl_subgoals, target_subgoals)
                        
                        high_level_loss = hl_policy_loss + 0.5 * hl_value_loss
                        
                    except Exception as e:
                        logger.warning(f"é«˜å±‚æ›´æ–°å¤±è´¥: {e}")
                        high_level_loss = torch.tensor(0.0, device=hl_obs.device)
        
        # ä½å±‚ç­–ç•¥æ›´æ–°ï¼ˆæ¯æ­¥éƒ½æœ‰ï¼‰
        if rollout_data.low_level_update_mask.sum() > 0:
            ll_indices = rollout_data.low_level_update_mask.bool()
            
            if ll_indices.any():
                # ä½å±‚è§‚æµ‹å’ŒåŠ¨ä½œ
                ll_obs = rollout_data.low_level_observations[ll_indices]      # [N_ll, 64]
                ll_actions = rollout_data.low_level_actions[ll_indices]       # [N_ll, 4]  
                ll_advantages = rollout_data.low_level_advantages[ll_indices] # [N_ll]
                ll_returns = rollout_data.low_level_returns[ll_indices]       # [N_ll]
                ll_old_values = rollout_data.low_level_values[ll_indices]     # [N_ll]
                
                # é€šè¿‡ç­–ç•¥ç½‘ç»œè·å–ä½å±‚ç‰¹å¾
                if hasattr(self.policy, 'features_extractor') and hasattr(self.policy.features_extractor, 'policy_network'):
                    policy_net = self.policy.features_extractor.policy_network
                    
                    try:
                        # ä½å±‚çŠ¶æ€ç¼–ç 
                        ll_features = policy_net.state_encoder(ll_obs)  # [N_ll, 128]
                        
                        # è·å–å½“å‰å­ç›®æ ‡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                        current_subgoals = torch.zeros(ll_obs.size(0), 2, device=ll_obs.device)
                        
                        # ä½å±‚å‰å‘ä¼ æ’­
                        ll_actions_pred, ll_values_pred = policy_net.forward_low_level(ll_features, current_subgoals)
                        
                        # ä½å±‚ä»·å€¼æŸå¤±
                        ll_value_loss = F.mse_loss(ll_values_pred.squeeze(), ll_returns)
                        
                        # ä½å±‚ç­–ç•¥æŸå¤±
                        ll_policy_loss = F.mse_loss(ll_actions_pred, ll_actions)
                        
                        low_level_loss = ll_policy_loss + 0.5 * ll_value_loss
                        
                    except Exception as e:
                        logger.warning(f"ä½å±‚æ›´æ–°å¤±è´¥: {e}")
                        low_level_loss = torch.tensor(0.0, device=ll_obs.device)
        
        # === 3. ç»„åˆæ€»æŸå¤± ===
        hierarchical_weight = self.config.training_config.get('hierarchical_weight', 0.1)
        combined_loss = total_loss + hierarchical_weight * (high_level_loss + low_level_loss)
        
        # === 4. åå‘ä¼ æ’­å’Œä¼˜åŒ– ===
        # è·å–ä¼˜åŒ–å™¨
        optimizer = None
        if hasattr(self.policy, 'optimizer'):
            optimizer = self.policy.optimizer
        elif hasattr(self.policy, 'policy') and hasattr(self.policy.policy, 'optimizer'):
            optimizer = self.policy.policy.optimizer
        
        if optimizer is not None:
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            
            # åå‘ä¼ æ’­
            combined_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if max_grad_norm is not None:
                if hasattr(self.policy, 'parameters'):
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.policy.parameters(), max_grad_norm
                    )
                else:
                    grad_norm = 0.0
            else:
                grad_norm = 0.0
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            optimizer.step()
        else:
            logger.warning("æœªæ‰¾åˆ°ä¼˜åŒ–å™¨ï¼Œè·³è¿‡å‚æ•°æ›´æ–°")
            grad_norm = 0.0
        
        # === 5. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ ===
        with torch.no_grad():
            # è£å‰ªæ¯”ä¾‹
            clip_fraction = ((ratio - 1.0).abs() > clip_range).float().mean().item()
            clip_fractions.append(clip_fraction)
            
            # è®°å½•æŸå¤±
            policy_losses.append(policy_loss.item())
            value_losses.append(value_loss.item())  
            entropy_losses.append(entropy_loss.item())
            high_level_losses.append(high_level_loss.item())
            low_level_losses.append(low_level_loss.item())
        
        # === 6. è¿”å›è¯¦ç»†ç»Ÿè®¡ ===
        return {
            # æ ‡å‡†PPOç»Ÿè®¡
            'policy_loss': np.mean(policy_losses),
            'value_loss': np.mean(value_losses),
            'entropy_loss': np.mean(entropy_losses),
            'total_loss': combined_loss.item(),
            'approx_kl': np.mean(approx_kl_divs),
            'clip_fraction': np.mean(clip_fractions),
            'explained_variance': self._explained_variance(rollout_data.returns.cpu().numpy(), 
                                                           values.detach().cpu().numpy() if hasattr(values, 'detach') else values),
            
            # åˆ†å±‚ç‰¹å®šç»Ÿè®¡
            'high_level_loss': np.mean(high_level_losses),
            'low_level_loss': np.mean(low_level_losses),
            'high_level_updates': rollout_data.high_level_update_mask.sum().item(),
            'low_level_updates': rollout_data.low_level_update_mask.sum().item(),
            'hierarchical_weight': hierarchical_weight,
            
            # è®­ç»ƒè¯Šæ–­
            'gradient_norm': grad_norm if isinstance(grad_norm, float) else grad_norm.item(),
            'learning_rate': self.config.policy_config['learning_rate'],
            'clip_range': clip_range,
            'vf_coef': vf_coef,
            'ent_coef': ent_coef,
        }
    
    def _explained_variance(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—è§£é‡Šæ–¹å·®
        
        Args:
            y_true: çœŸå®å€¼
            y_pred: é¢„æµ‹å€¼
            
        Returns:
            float: è§£é‡Šæ–¹å·®
        """
        var_y = np.var(y_true)
        if var_y == 0:
            return np.nan
        return 1 - np.var(y_true - y_pred) / var_y
    
    def train_step(self, env: gym.Env) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€æ­¥å®Œæ•´è®­ç»ƒ - å‚è€ƒHAUAVTraineræ¨¡å¼
        
        Args:
            env: ç¯å¢ƒå®ä¾‹
            
        Returns:
            dict: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        # ğŸ”§ é‡è¦ï¼šæ¯æ¬¡train_stepé‡ç½®buffer
        logger.info("è®­ç»ƒæ­¥éª¤å¼€å§‹ï¼Œé‡ç½®buffer")
        self.buffer.reset()
        
        # 1. æ”¶é›†å›ºå®šæ­¥æ•°çš„ç»éªŒ
        rollout_stats = self.collect_rollout(env, n_steps=self.config.buffer_config['buffer_size'])
        
        # 2. æ£€æŸ¥æ•°æ®å……è¶³æ€§
        min_batch_size = self.config.training_config.get('batch_size', 64)
        if rollout_stats['total_steps'] < min_batch_size:
            logger.warning(f"æ•°æ®ä¸è¶³({rollout_stats['total_steps']}æ­¥)ï¼Œè·³è¿‡ç­–ç•¥æ›´æ–°")
            rollout_stats['training_skipped'] = True
            rollout_stats['policy_loss'] = 0.0
            rollout_stats['value_loss'] = 0.0
            return rollout_stats
        
        # 3. ç­–ç•¥æ›´æ–°ï¼ˆå¦‚æœbufferæ»¡è¶³æ¡ä»¶ï¼‰
        if hasattr(self.buffer, 'full') and self.buffer.full:
            training_stats = self._update_policy_batch()
            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            combined_stats = {**rollout_stats, **training_stats}
        else:
            # bufferæœªæ»¡ï¼Œè¿›è¡Œè½»é‡çº§æ›´æ–°
            training_stats = self._lightweight_policy_update()
            combined_stats = {**rollout_stats, **training_stats}
        
        combined_stats['training_skipped'] = False
        combined_stats['buffer_stats'] = self.buffer.get_hierarchical_statistics()
        
        logger.info(f"è®­ç»ƒæ­¥éª¤å®Œæˆ: æ”¶é›†{rollout_stats['total_steps']}æ­¥, "
                   f"episodes: {rollout_stats['episodes']}, "
                   f"å¹³å‡å¥–åŠ±: {rollout_stats.get('mean_reward', 0.0):.3f}")
        
        return combined_stats
    
    def update_policy_from_buffer(self) -> Dict[str, Any]:
        """ä»bufferæ›´æ–°ç­–ç•¥ - ç‹¬ç«‹æ–¹æ³•ä¾›è®­ç»ƒå™¨è°ƒç”¨"""
        if not self.is_initialized:
            raise RuntimeError("ç»„ä»¶æœªåˆå§‹åŒ–")
        
        training_stats = {}
        n_epochs = self.config.training_config.get('n_epochs', 4)
        batch_size = self.config.training_config.get('batch_size', 64)
        
        # æ£€æŸ¥bufferä¸­æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®
        if not hasattr(self.buffer, 'size') or self.buffer.size() < batch_size:
            logger.warning(f"Bufferæ•°æ®ä¸è¶³ï¼Œè·³è¿‡ç­–ç•¥æ›´æ–°")
            return {'policy_loss': 0.0, 'value_loss': 0.0, 'training_skipped': True}
        
        logger.info(f"å¼€å§‹ç­–ç•¥æ›´æ–°ï¼Œepochs: {n_epochs}, batch_size: {batch_size}")
        for epoch in range(n_epochs):
            try:
                for batch_data in self.buffer.get(batch_size=batch_size):
                    epoch_stats = self.update_policy(batch_data)
                    
                    # ç´¯ç§¯ç»Ÿè®¡
                    for key, value in epoch_stats.items():
                        training_stats[key] = training_stats.get(key, 0.0) + value / n_epochs
            except Exception as e:
                logger.error(f"ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
                break
        
        training_stats['training_skipped'] = False
        logger.info(f"ç­–ç•¥æ›´æ–°å®Œæˆ: loss={training_stats.get('policy_loss', 0.0):.4f}")
        return training_stats
    
    def _update_policy_batch(self) -> Dict[str, Any]:
        """æ‰¹é‡ç­–ç•¥æ›´æ–° - å‚è€ƒHAUAVTrainer"""
        training_stats = {}
        n_epochs = self.config.training_config.get('n_epochs', 4)
        batch_size = self.config.training_config.get('batch_size', 64)
        
        logger.info(f"å¼€å§‹ç­–ç•¥æ›´æ–°: {n_epochs} epochs, batch_size: {batch_size}")
        
        for epoch in range(n_epochs):
            epoch_stats = {'policy_loss': 0.0, 'value_loss': 0.0, 'batches': 0}
            
            for batch_data in self.buffer.get(batch_size=batch_size):
                batch_update_stats = self.update_policy(batch_data)
                
                # ç´¯ç§¯epochç»Ÿè®¡
                for key, value in batch_update_stats.items():
                    if key in epoch_stats:
                        epoch_stats[key] += value
                epoch_stats['batches'] += 1
            
            # è®¡ç®—epochå¹³å‡å€¼
            if epoch_stats['batches'] > 0:
                for key in ['policy_loss', 'value_loss']:
                    if key in epoch_stats:
                        epoch_stats[key] /= epoch_stats['batches']
            
            # ç´¯ç§¯åˆ°æ€»ç»Ÿè®¡
            for key, value in epoch_stats.items():
                if key != 'batches':
                    training_stats[key] = training_stats.get(key, 0.0) + value / n_epochs
        
        return training_stats
    
    def _lightweight_policy_update(self) -> Dict[str, Any]:
        """è½»é‡çº§ç­–ç•¥æ›´æ–°"""
        # ç®€åŒ–çš„æ›´æ–°ï¼Œé¿å…å¤æ‚è®¡ç®—
        return {
            'policy_loss': 0.0,
            'value_loss': 0.0,
            'lightweight_update': True
        }
    
    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        if not self.is_initialized:
            raise RuntimeError("ç»„ä»¶æœªåˆå§‹åŒ–")
        
        # ä¿å­˜ç­–ç•¥
        self.policy.save(save_path)
        
        # ä¿å­˜é…ç½®
        config_path = save_path.replace('.zip', '_config.json')
        self.config.save(config_path)
        
        # ä¿å­˜StateManagerçŠ¶æ€
        import pickle
        sm_path = save_path.replace('.zip', '_state_manager.pkl')
        with open(sm_path, 'wb') as f:
            pickle.dump(self.state_manager, f)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_model(self, load_path: str, env: gym.Env):
        """åŠ è½½æ¨¡å‹
        
        Args:
            load_path: åŠ è½½è·¯å¾„
            env: ç¯å¢ƒå®ä¾‹
        """
        # åŠ è½½é…ç½®
        config_path = load_path.replace('.zip', '_config.json')
        if Path(config_path).exists():
            self.config = ModelConfiguration.load(config_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.initialize_components(env)
        
        # åŠ è½½ç­–ç•¥
        self.policy = HierarchicalPolicy.load(load_path, env=env)
        
        # åŠ è½½StateManagerçŠ¶æ€
        import pickle
        sm_path = load_path.replace('.zip', '_state_manager.pkl')
        if Path(sm_path).exists():
            with open(sm_path, 'rb') as f:
                self.state_manager = pickle.load(f)
        
        logger.info(f"æ¨¡å‹å·²ä» {load_path} åŠ è½½")
    
    def set_training_mode(self, training: bool):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼
        
        Args:
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        """
        self.training_mode = training
        if self.policy:
            self.policy.set_training_mode(training)
    
    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: åŒ…å«è®­ç»ƒç»Ÿè®¡çš„å­—å…¸
        """
        stats = {}
        
        try:
            # åŸºç¡€ç»Ÿè®¡
            stats['is_initialized'] = self.is_initialized
            stats['training_mode'] = self.training_mode
            
            # ç¼“å†²åŒºç»Ÿè®¡
            if self.buffer is not None:
                stats['buffer'] = {
                    'size': self.buffer.buffer_size,
                    'position': self.buffer.pos,
                    'full': self.buffer.full,
                    'n_envs': self.buffer.n_envs
                }
                
                # åˆ†å±‚ç¼“å†²åŒºç‰¹æœ‰ç»Ÿè®¡
                if hasattr(self.buffer, 'get_hierarchical_statistics'):
                    hierarchical_stats = self.buffer.get_hierarchical_statistics()
                    stats['buffer'].update(hierarchical_stats)
            
            # ç­–ç•¥ç»Ÿè®¡
            if self.policy is not None:
                stats['policy'] = {
                    'type': type(self.policy).__name__,
                    'observation_space': str(self.observation_space.shape) if self.observation_space else None,
                    'action_space': str(self.action_space.shape) if self.action_space else None
                }
            
            # StateManagerç»Ÿè®¡
            if self.state_manager is not None:
                stats['state_manager'] = {
                    'type': type(self.state_manager).__name__,
                    'history_length': getattr(self.state_manager, 'history_length', None),
                    'high_level_update_frequency': getattr(self.state_manager, 'high_level_update_frequency', None)
                }
            
            # HA_Modulesç»Ÿè®¡
            if self.ha_modules is not None:
                stats['ha_modules'] = {
                    'type': type(self.ha_modules).__name__,
                    'parameters': sum(p.numel() for p in self.ha_modules.parameters()),
                    'trainable_parameters': sum(p.numel() for p in self.ha_modules.parameters() if p.requires_grad)
                }
            
            # é…ç½®ç»Ÿè®¡
            stats['config'] = {
                'state_manager_config': self.config.state_manager_config,
                'buffer_config': {k: v for k, v in self.config.buffer_config.items() if k != 'hierarchical_config'},
                'training_config': self.config.training_config
            }
            
            logger.info("è®­ç»ƒç»Ÿè®¡ä¿¡æ¯æ”¶é›†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å¤±è´¥: {e}")
            stats['error'] = str(e)
        
        return stats
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€
        
        Returns:
            dict: ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
        """
        status = {
            'initialized': self.is_initialized,
            'training_mode': self.training_mode,
            'components': {
                'state_manager': self.state_manager is not None,
                'policy': self.policy is not None,
                'buffer': self.buffer is not None,
                'ha_modules': self.ha_modules is not None
            }
        }
        
        if self.state_manager:
            status['state_manager_info'] = {
                'step_counter': self.state_manager.step_counter,
                'history_length': len(self.state_manager.state_history),
                'is_ready_for_high_level': self.state_manager.is_ready_for_high_level()
            }
        
        if self.buffer and self.buffer.full:
            status['buffer_stats'] = self.buffer.get_hierarchical_statistics()
        
        return status


# =============== ä¾¿æ·å‡½æ•° ===============

def create_ha_system(env: gym.Env, config: Optional[ModelConfiguration] = None) -> HAComponentsManager:
    """åˆ›å»ºå®Œæ•´çš„HAç³»ç»Ÿ
    
    Args:
        env: Gymç¯å¢ƒ
        config: æ¨¡å‹é…ç½®
        
    Returns:
        HAComponentsManager: åˆå§‹åŒ–å®Œæˆçš„ç³»ç»Ÿç®¡ç†å™¨
    """
    manager = HAComponentsManager(config)
    success = manager.initialize_components(env)
    
    if not success:
        raise RuntimeError("HAç³»ç»Ÿåˆ›å»ºå¤±è´¥")
    
    return manager

def create_default_config() -> ModelConfiguration:
    """åˆ›å»ºé»˜è®¤é…ç½®
    
    Returns:
        ModelConfiguration: é»˜è®¤é…ç½®å¯¹è±¡
    """
    return ModelConfiguration()

# =============== æ¨¡å—å¯¼å‡º ===============

__all__ = [
    # ç°æœ‰æ ¸å¿ƒç»„ä»¶ï¼ˆå®Œå…¨å¤ç”¨ï¼‰
    'StateManager',
    'StructuredState', 
    'HierarchicalPolicyNetwork',
    'HierarchicalFeaturesExtractor',
    'HierarchicalPolicy',
    'HierarchicalRolloutBuffer',
    'HierarchicalRolloutBufferSamples',
    'HierarchicalRLSystem',
    
    # ç®¡ç†å’Œé…ç½®ç±»
    'ModelConfiguration',
    'HAComponentsManager',
    
    # ä¾¿æ·å‡½æ•°
    'create_ha_system',
    'create_default_config',
    'create_hierarchical_rollout_buffer',
    
    # Bç»„æ¶ˆèå®éªŒç»„ä»¶
    'AblationConfig',
    'AblationConfigManager', 
    'get_ablation_config',
    'list_ablation_experiments',
    'create_b1_config',
    'create_b2_config',
    'create_b3_config',
    'create_baseline_config',
    
    'AblationStateManagerAdapter',
    'AblationPolicyWrapper',
    'AblationBufferAdapter',
    'AblationComponentsManager',
    'create_ablation_system',
    
    'DirectControlPolicy',
    'DirectControlPolicyNetwork', 
    'create_direct_control_policy',
    
    'FlatPolicy',
    'FlatPolicyNetwork',
    'create_flat_policy',
    
    'SingleStepHierarchicalPolicy',
    'SingleStepHighLevelNetwork',
    'SingleStepLowLevelNetwork',
    'create_single_step_hierarchical_policy',
]

logger.info("modulesåŒ…åˆå§‹åŒ–å®Œæˆ - åŸºäºç°æœ‰ç»„ä»¶çš„å®Œæ•´é›†æˆ + Bç»„æ¶ˆèå®éªŒ")