#!/usr/bin/env python3

"""
åŸºçº¿è®­ç»ƒå™¨ - SB3ç®—æ³•å¯¹æ¯”è®­ç»ƒ (PPO/SAC/TD3)
"""

import logging
import time
import torch
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import sys

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from ..core.base_trainer import BaseTrainer, TrainingStage, TrainingResult
from ..core.environment_factory import EnvironmentFactory
from ..core.model_transfer import ModelTransferManager

# SB3å¯¼å…¥
try:
    from stable_baselines3 import PPO, SAC, TD3
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
    from stable_baselines3.common.vec_env import VecEnv
    from stable_baselines3.common.logger import configure
    SB3_AVAILABLE = True
except ImportError:
    print("Warning: Stable-Baselines3 not available. Install with: pip install stable-baselines3")
    SB3_AVAILABLE = False

# å¤ç”¨ç°æœ‰åŸºçº¿åŒ…è£…å™¨
from ..core.environment_factory import BaselineWrapper

logger = logging.getLogger(__name__)


class BaselineProgressCallback(BaseCallback):
    """SB3è®­ç»ƒè¿›åº¦å›è°ƒ"""
    
    def __init__(self, 
                 progress_callbacks: List = None,
                 stage: TrainingStage = TrainingStage.BASELINE,
                 algorithm: str = "unknown",
                 verbose: int = 0):
        
        super(BaselineProgressCallback, self).__init__(verbose)
        self.progress_callbacks = progress_callbacks or []
        self.stage = stage
        self.algorithm = algorithm
        self.episode_count = 0
        self.best_mean_reward = -np.inf
    
    def _on_step(self) -> bool:
        """æ¯æ­¥å›è°ƒ"""
        
        # æ¯1000æ­¥æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
        if self.n_calls % 1000 == 0:
            
            # è·å–è®­ç»ƒç»Ÿè®¡
            if hasattr(self.locals, 'infos'):
                infos = self.locals.get('infos', [])
                if infos and 'episode' in infos[0]:
                    self.episode_count = infos[0]['episode']['l']
            
            # è·å–å½“å‰å¥–åŠ±
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                recent_rewards = [ep_info['r'] for ep_info in self.model.ep_info_buffer[-10:]]
                mean_reward = np.mean(recent_rewards) if recent_rewards else 0.0
            else:
                mean_reward = 0.0
            
            # è¿›åº¦æ•°æ®
            progress_data = {
                'algorithm': self.algorithm,
                'step': self.n_calls,
                'episode': self.episode_count,
                'mean_reward': mean_reward,
                'progress': self.n_calls / getattr(self.model, 'total_timesteps', 1)
            }
            
            # è°ƒç”¨å¤–éƒ¨å›è°ƒ
            for callback in self.progress_callbacks:
                try:
                    callback(self.stage, progress_data)
                except Exception as e:
                    logger.warning(f"è¿›åº¦å›è°ƒå¤±è´¥: {e}")
        
        return True


class BaselineTrainer(BaseTrainer):
    """
    åŸºçº¿è®­ç»ƒå™¨ - ä½¿ç”¨SB3ç®—æ³•è¿›è¡Œå¯¹æ¯”è®­ç»ƒ
    
    æ”¯æŒç®—æ³•:
    - PPO: Proximal Policy Optimization
    - SAC: Soft Actor-Critic
    - TD3: Twin Delayed DDPG
    """
    
    def __init__(self,
                 config: Dict[str, Any],
                 env_factory: Optional[EnvironmentFactory] = None,
                 transfer_manager: Optional[ModelTransferManager] = None,
                 foundation_checkpoint: Optional[Dict[str, Any]] = None,
                 output_dir: str = "./models"):
        
        # è·å–ç®—æ³•åˆ—è¡¨ä½œä¸ºé˜¶æ®µå˜ä½“
        algorithms = config.get('algorithms', ['ppo', 'sac', 'td3'])
        stage_variant = '-'.join(algorithms)
        
        super().__init__(
            stage=TrainingStage.BASELINE,
            config=config,
            experiment_name="HA-UAV",
            stage_variant=stage_variant
        )
        
        # æ£€æŸ¥SB3å¯ç”¨æ€§
        if not SB3_AVAILABLE:
            raise ImportError("Stable-Baselines3 is required for baseline training")
        
        # ç¯å¢ƒå·¥å‚
        self.env_factory = env_factory or EnvironmentFactory()
        
        # è¿ç§»ç®¡ç†å™¨
        self.transfer_manager = transfer_manager
        self.foundation_checkpoint = foundation_checkpoint
        
        # è¾“å‡ºç›®å½•
        self.output_dir = output_dir
        
        # åŸºçº¿ç®—æ³•é…ç½®
        self.algorithms = config.get('algorithms', ['ppo', 'sac'])
        self.use_pretrained_init = config.get('use_pretrained_init', True)
        
        # è®­ç»ƒé…ç½®
        self.total_timesteps = config.get('total_timesteps', 200000)
        self.eval_freq = config.get('eval_freq', 10000)
        self.eval_episodes = config.get('eval_episodes', 10)
        
        # ç®—æ³•ç‰¹å®šé…ç½®
        self.algorithm_configs = {
            'ppo': {
                'learning_rate': config.get('ppo_learning_rate', 3e-4),
                'n_steps': config.get('ppo_n_steps', 2048),
                'batch_size': config.get('ppo_batch_size', 64),
                'n_epochs': config.get('ppo_n_epochs', 10),
                'gamma': config.get('gamma', 0.99),
                'gae_lambda': config.get('gae_lambda', 0.95),
                'clip_range': config.get('clip_range', 0.2),
                'ent_coef': config.get('ent_coef', 0.0),
                'vf_coef': config.get('vf_coef', 0.5)
            },
            'sac': {
                'learning_rate': config.get('sac_learning_rate', 3e-4),
                'buffer_size': config.get('sac_buffer_size', 1000000),
                'learning_starts': config.get('sac_learning_starts', 100),
                'batch_size': config.get('sac_batch_size', 256),
                'tau': config.get('sac_tau', 0.005),
                'gamma': config.get('gamma', 0.99),
                'train_freq': config.get('sac_train_freq', 1),
                'gradient_steps': config.get('sac_gradient_steps', 1)
            },
            'td3': {
                'learning_rate': config.get('td3_learning_rate', 3e-4),
                'buffer_size': config.get('td3_buffer_size', 1000000),
                'learning_starts': config.get('td3_learning_starts', 100),
                'batch_size': config.get('td3_batch_size', 100),
                'tau': config.get('td3_tau', 0.005),
                'gamma': config.get('gamma', 0.99),
                'train_freq': config.get('td3_train_freq', 1),
                'gradient_steps': config.get('td3_gradient_steps', 1),
                'policy_delay': config.get('td3_policy_delay', 2)
            }
        }
        
        # è®­ç»ƒç»“æœå­˜å‚¨
        self.baseline_results = {}
        
        # å½“å‰ç¯å¢ƒ
        self.env = None
        self.eval_env = None
        
        # ğŸ”§ ä¿®å¤ï¼šåˆå§‹åŒ–progress_callbacksï¼ˆé›†æˆå¯è§†åŒ–ç®¡ç†å™¨å›è°ƒï¼‰
        self.progress_callbacks = []
        if hasattr(self, 'visualization_manager') and self.visualization_manager:
            self.progress_callbacks.append(self.visualization_manager.on_training_progress)
    
    def setup(self) -> bool:
        """è®¾ç½®è®­ç»ƒå™¨"""
        try:
            # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
            env_config = {
                'drone_model': self.config.get('drone_model', 'CF2X'),
                'physics': self.config.get('physics', 'PYB'),
                'gui_training': self.config.get('gui_training', False),
                'max_episode_steps': self.config.get('max_episode_steps', 1000)
            }
            
            # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
            base_env = self.env_factory.create_environment(
                stage=self.stage,
                config=env_config,
                mode="train"
            )
            
            # åº”ç”¨SB3åŒ…è£…å™¨
            self.env = BaselineWrapper(base_env, agent_type="sb3")
            
            # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
            eval_base_env = self.env_factory.create_environment(
                stage=self.stage,
                config={**env_config, 'gui_training': False},
                mode="eval"
            )
            self.eval_env = BaselineWrapper(eval_base_env, agent_type="sb3")
            
            # è®¾ç½®æ—¥å¿—
            log_dir = Path(self.output_dir) / "baseline_logs"
            log_dir.mkdir(parents=True, exist_ok=True)
            
            self.logger.info("åŸºçº¿è®­ç»ƒå™¨è®¾ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"åŸºçº¿è®­ç»ƒå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def train(self) -> TrainingResult:
        """æ‰§è¡ŒåŸºçº¿è®­ç»ƒ"""
        start_time = time.time()
        
        if not self.setup():
            return TrainingResult(
                success=False,
                error_message="åŸºçº¿è®­ç»ƒå™¨è®¾ç½®å¤±è´¥"
            )
        
        try:
            self.logger.info(f"å¼€å§‹åŸºçº¿ç®—æ³•è®­ç»ƒ: {self.algorithms}")
            
            # é€ä¸ªè®­ç»ƒæ¯ç§ç®—æ³•
            for algorithm in self.algorithms:
                self.logger.info(f"å¼€å§‹è®­ç»ƒç®—æ³•: {algorithm.upper()}")
                
                result = self._train_single_algorithm(algorithm)
                self.baseline_results[algorithm] = result
                
                if result.success:
                    self.logger.info(f"ç®—æ³• {algorithm.upper()} è®­ç»ƒæˆåŠŸ")
                else:
                    self.logger.error(f"ç®—æ³• {algorithm.upper()} è®­ç»ƒå¤±è´¥: {result.error_message}")
            
            # æ±‡æ€»ç»“æœ
            training_duration = time.time() - start_time
            
            # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
            comparison_metrics = self._create_baseline_comparison()
            
            # ç¡®å®šæœ€ä½³ç®—æ³•
            best_algorithm = self._determine_best_algorithm()
            best_model = self.baseline_results[best_algorithm].trained_model if best_algorithm else None
            
            self.logger.info(f"åŸºçº¿ç®—æ³•è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_duration:.2f}ç§’")
            
            return TrainingResult(
                success=True,
                trained_model=best_model,
                metrics=comparison_metrics,
                metadata={
                    'stage': self.stage.value,
                    'training_duration': training_duration,
                    'algorithms': self.algorithms,
                    'individual_results': {k: {'success': v.success, 'metrics': v.metrics}
                                         for k, v in self.baseline_results.items()},
                    'best_algorithm': best_algorithm
                }
            )
            
        except Exception as e:
            self.logger.error(f"åŸºçº¿è®­ç»ƒè¿‡ç¨‹å¤±è´¥: {e}")
            return TrainingResult(
                success=False,
                error_message=str(e)
            )
    
    def _train_single_algorithm(self, algorithm: str) -> TrainingResult:
        """è®­ç»ƒå•ä¸ªåŸºçº¿ç®—æ³• - è½»é‡çº§åè°ƒå™¨æ¨¡å¼"""
        
        try:
            # åˆ›å»ºæ¨¡å‹
            model = self._create_sb3_model(algorithm)
            
            # æƒé‡åˆå§‹åŒ–
            if self.use_pretrained_init and self.foundation_checkpoint and self.transfer_manager:
                self._apply_pretrained_initialization(model, algorithm)
            
            # è®¡ç®—è®­ç»ƒæ­¥æ•°
            timesteps_per_algorithm = self.total_timesteps // len(self.algorithms)
            self.logger.info(f"{algorithm.upper()} è®­ç»ƒæ­¥æ•°: {timesteps_per_algorithm}")
            
            # ğŸ¯ æ ¸å¿ƒï¼šå§”æ‰˜ç»™è®­ç»ƒæ‰§è¡Œæ–¹æ³•
            final_metrics = self._execute_baseline_training(algorithm, model, timesteps_per_algorithm)
            
            # ä¿å­˜æ¨¡å‹
            model_path = Path(self.output_dir) / f"final_{algorithm}_model.zip"
            model.save(str(model_path))
            
            return TrainingResult(
                success=True,
                trained_model=model,
                metrics=final_metrics,
                metadata={
                    'algorithm': algorithm,
                    'timesteps': timesteps_per_algorithm,
                    'model_path': str(model_path)
                }
            )
            
        except Exception as e:
            self.logger.error(f"ç®—æ³• {algorithm} è®­ç»ƒå¤±è´¥: {e}")
            return TrainingResult(
                success=False,
                error_message=str(e)
            )
            
    def _execute_baseline_training(self, algorithm: str, model, timesteps_per_algorithm: int) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºçº¿è®­ç»ƒé€»è¾‘ - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿å›è°ƒæ­£å¸¸å·¥ä½œ"""
        self.logger.info(f"å¼€å§‹ {algorithm.upper()} åŸºçº¿è®­ç»ƒ: {timesteps_per_algorithm:,} æ­¥")
        
        # ğŸ¯ å…³é”®ï¼šä½¿ç”¨å¾ˆå°çš„å­¦ä¹ é—´éš”ï¼Œç¡®ä¿å›è°ƒåŠæ—¶è§¦å‘
        learn_interval = min(100, timesteps_per_algorithm // 20)  # å¾ˆå°çš„é—´éš”
        current_step = 0
        training_stats = []
        
        # ğŸ”§ ç«‹å³è°ƒç”¨ç¬¬ä¸€æ¬¡å›è°ƒï¼Œç¡®ä¿è¿›åº¦æ¡å¯åŠ¨
        if self.visualization_manager:
            initial_metrics = {
                'episode': 0,
                'total_reward': 0.0,
                'exploration_rate': 1.0,
                'policy_loss': 0.0,
                'value_loss': 0.0,
                'algorithm': algorithm
            }
            self.visualization_manager.on_step(0, initial_metrics)
            self.logger.info(f"âœ… {algorithm.upper()} åˆå§‹å›è°ƒå·²è§¦å‘ï¼Œè¿›åº¦æ¡åº”å·²å¯åŠ¨")
        
        # ä½¿ç”¨å°æ‰¹æ¬¡è®­ç»ƒï¼Œç¡®ä¿å›è°ƒåŠæ—¶è§¦å‘
        while current_step < timesteps_per_algorithm:
            try:
                # ğŸ”§ ä½¿ç”¨è¶…å°æ‰¹æ¬¡ï¼Œé¿å…é•¿æ—¶é—´å¡ä½
                steps_to_learn = min(learn_interval, timesteps_per_algorithm - current_step, 50)  # æœ€å¤š50æ­¥
                
                self.logger.info(f"å¼€å§‹ {algorithm.upper()} å­¦ä¹ : {steps_to_learn} æ­¥ (å½“å‰æ­¥æ•°: {current_step})")
                
                # é™æ—¶å­¦ä¹ ï¼Œé¿å…æ— é™å¡ä½
                learn_start = time.time()
                
                try:
                    # SB3çš„å°æ‰¹æ¬¡å­¦ä¹ 
                    model.learn(
                        total_timesteps=steps_to_learn,
                        reset_num_timesteps=False,  # å…³é”®ï¼šä¸é‡ç½®è®¡æ•°å™¨
                        progress_bar=False
                    )
                    learn_time = time.time() - learn_start
                    
                    if learn_time > 5.0:
                        self.logger.warning(f"{algorithm.upper()} å­¦ä¹ è€—æ—¶è¿‡é•¿: {learn_time:.1f}s")
                    
                except Exception as e:
                    self.logger.error(f"{algorithm.upper()} å­¦ä¹ å¤±è´¥: {e}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°è¿›åº¦
                    steps_to_learn = min(steps_to_learn, 10)  # å‡å°‘æ­¥æ•°
                
                # ğŸ¯ ç«‹å³æ›´æ–°æ­¥æ•°å’Œè§¦å‘å›è°ƒ
                current_step += steps_to_learn
                
                # ğŸ”§ æ ¸å¿ƒï¼šç«‹å³è§¦å‘å›è°ƒï¼Œç¡®ä¿è¿›åº¦æ¡æ›´æ–°
                step_metrics = {
                    'episode': current_step // 1000,  # ä¼°ç®—episodeæ•°
                    'total_reward': 0.0,  # SB3å†…éƒ¨ç»´æŠ¤
                    'exploration_rate': max(0.0, 1.0 - current_step / timesteps_per_algorithm),
                    'policy_loss': 0.0,  # SB3å†…éƒ¨ç»´æŠ¤
                    'value_loss': 0.0,   # SB3å†…éƒ¨ç»´æŠ¤
                    'algorithm': algorithm
                }
                
                # è°ƒç”¨å¯è§†åŒ–ç®¡ç†å™¨å›è°ƒ
                if self.visualization_manager:
                    self.visualization_manager.on_step(current_step, step_metrics)
                    self.logger.info(f"âœ… {algorithm.upper()} å›è°ƒå·²è§¦å‘: {current_step}/{timesteps_per_algorithm} ({current_step/timesteps_per_algorithm:.1%})")
                
                # è°ƒç”¨è®­ç»ƒå™¨å›è°ƒ
                self.on_step_callback(current_step, step_metrics)
                
                # å®šæœŸè¯„ä¼°ï¼ˆå¿«é€Ÿè·³è¿‡ï¼‰
                if current_step % self.eval_freq == 0 and current_step > 0:
                    self.logger.info(f"å¼€å§‹ {algorithm.upper()} å®šæœŸè¯„ä¼° (æ­¥æ•°: {current_step})")
                    try:
                        eval_results = self._evaluate_sb3_model(model, algorithm, num_episodes=5)  # å‡å°‘è¯„ä¼°episode
                        self.on_evaluation_callback(eval_results)
                    except Exception as e:
                        self.logger.warning(f"{algorithm.upper()} è¯„ä¼°å¤±è´¥: {e}")
                
                # è¿›åº¦æ—¥å¿—
                self.logger.info(f"{algorithm.upper()} è®­ç»ƒè¿›åº¦: {current_step:,}/{timesteps_per_algorithm:,} "
                               f"({current_step/timesteps_per_algorithm:.1%})")
                
                # è®°å½•ç»Ÿè®¡
                training_stats.append({
                    'steps': steps_to_learn,
                    'total_steps': current_step
                })
                
            except Exception as e:
                self.logger.error(f"{algorithm.upper()} è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # å³ä½¿å‡ºé”™ä¹Ÿè¦æ›´æ–°è¿›åº¦ï¼Œé¿å…å¡ä½
                current_step += learn_interval
                if self.visualization_manager:
                    error_metrics = {
                        'episode': 0,
                        'total_reward': 0.0,
                        'exploration_rate': 0.0,
                        'policy_loss': 0.0,
                        'value_loss': 0.0,
                        'algorithm': algorithm
                    }
                    self.visualization_manager.on_step(current_step, error_metrics)
                break
        
        # æœ€ç»ˆè¯„ä¼°
        try:
            final_metrics = self._evaluate_sb3_model(model, algorithm)
            self.logger.info(f"{algorithm.upper()} åŸºçº¿è®­ç»ƒå®Œæˆ: æˆåŠŸç‡={final_metrics.get('success_rate', 0):.2%}")
            return final_metrics
        except Exception as e:
            self.logger.error(f"{algorithm.upper()} æœ€ç»ˆè¯„ä¼°å¤±è´¥: {e}")
            return {'success_rate': 0.0, 'mean_reward': 0.0, 'error': str(e)}
    
    def _create_sb3_model(self, algorithm: str):
        """åˆ›å»ºSB3æ¨¡å‹"""
        
        config = self.algorithm_configs[algorithm]
        
        if algorithm == 'ppo':
            model = PPO(
                policy="MlpPolicy",
                env=self.env,
                verbose=1,
                **config
            )
        
        elif algorithm == 'sac':
            model = SAC(
                policy="MlpPolicy",
                env=self.env,
                verbose=1,
                **config
            )
        
        elif algorithm == 'td3':
            model = TD3(
                policy="MlpPolicy",
                env=self.env,
                verbose=1,
                **config
            )
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        return model
    
    def _apply_pretrained_initialization(self, model, algorithm: str) -> None:
        """åº”ç”¨é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–"""
        
        if not self.transfer_manager or not self.foundation_checkpoint:
            return
        
        try:
            # ä½¿ç”¨è¿ç§»ç®¡ç†å™¨è¿›è¡Œæƒé‡è¿ç§»
            transfer_result = self.transfer_manager.transfer_weights(
                foundation_checkpoint=self.foundation_checkpoint,
                target_model=model.policy,  # SB3æ¨¡å‹çš„ç­–ç•¥ç½‘ç»œ
                target_stage=self.stage,
                transfer_config={
                    'algorithm': algorithm,
                    'use_pretrained_init': True
                }
            )
            
            self.logger.info(f"{algorithm.upper()} é¢„è®­ç»ƒåˆå§‹åŒ–: {transfer_result['success_rate']:.2%}")
            
        except Exception as e:
            self.logger.warning(f"{algorithm.upper()} é¢„è®­ç»ƒåˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _evaluate_sb3_model(self, model, algorithm: str, num_episodes: int = None) -> Dict[str, Any]:
        """è¯„ä¼°SB3æ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ·»åŠ è¶…æ—¶å’Œå¼ºåˆ¶ç»ˆæ­¢æœºåˆ¶"""
        
        # ğŸ”§ å‡å°‘è¯„ä¼°episodeæ•°ï¼Œé¿å…é•¿æ—¶é—´è¯„ä¼°
        eval_episodes = num_episodes or min(self.eval_episodes, 5)
        eval_rewards = []
        eval_successes = 0
        
        self.logger.info(f"å¼€å§‹è¯„ä¼° {algorithm.upper()}: {eval_episodes} episodes")
        
        for episode in range(eval_episodes):
            try:
                obs, _ = self.eval_env.reset()
                episode_reward = 0
                episode_success = False
                
                # ğŸ”§ å‡å°‘æœ€å¤§æ­¥æ•°ï¼Œé¿å…episodeè¿‡é•¿
                max_steps = min(self.config.get('max_episode_steps', 1000), 200)  # æœ€å¤š200æ­¥
                
                for step in range(max_steps):
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = self.eval_env.step(action)
                    episode_reward += reward
                    
                    if terminated or truncated:
                        if info.get('success', False) or info.get('navigation_success', False):
                            eval_successes += 1
                            episode_success = True
                        break
                    
                    # ğŸ”§ æ·»åŠ æ—©æœŸæˆåŠŸåˆ¤æ–­
                    if step > 100 and episode_reward > 50:  # 100æ­¥åå¦‚æœå¥–åŠ±è¶³å¤Ÿé«˜å°±è®¤ä¸ºæˆåŠŸ
                        episode_success = True
                        eval_successes += 1
                        break
                
                eval_rewards.append(episode_reward)
                self.logger.info(f"{algorithm.upper()} è¯„ä¼° episode {episode + 1}: å¥–åŠ±={episode_reward:.2f}, æ­¥æ•°={step + 1}, æˆåŠŸ={episode_success}")
                
            except Exception as e:
                self.logger.error(f"{algorithm.upper()} è¯„ä¼° episode {episode + 1} å¤±è´¥: {e}")
                eval_rewards.append(0.0)
                continue
        
        # ç¡®ä¿æœ‰æ•°æ®
        if not eval_rewards:
            eval_rewards = [0.0]
        
        return {
            'algorithm': algorithm,
            'avg_episode_reward': float(np.mean(eval_rewards)),
            'std_episode_reward': float(np.std(eval_rewards)),
            'success_rate': eval_successes / eval_episodes if eval_episodes > 0 else 0.0,
            'total_eval_episodes': eval_episodes,
            'mean_reward': float(np.mean(eval_rewards))  # å…¼å®¹æ€§å­—æ®µ
        }
    
    def _create_baseline_comparison(self) -> Dict[str, Any]:
        """åˆ›å»ºåŸºçº¿å¯¹æ¯”æŠ¥å‘Š"""
        
        comparison = {
            'algorithm_results': {},
            'rankings': {},
            'analysis': {}
        }
        
        # æ”¶é›†å„ç®—æ³•çš„æŒ‡æ ‡
        metrics = ['avg_episode_reward', 'success_rate']
        
        for metric in metrics:
            comparison['algorithm_results'][metric] = {}
            
            for algorithm in self.algorithms:
                if algorithm in self.baseline_results and self.baseline_results[algorithm].success:
                    value = self.baseline_results[algorithm].metrics.get(metric, 0.0)
                    comparison['algorithm_results'][metric][algorithm] = value
        
        # ç”Ÿæˆæ’å
        for metric in metrics:
            if comparison['algorithm_results'][metric]:
                sorted_algorithms = sorted(
                    comparison['algorithm_results'][metric].items(),
                    key=lambda x: x[1],
                    reverse=True
                )
                comparison['rankings'][metric] = [alg for alg, _ in sorted_algorithms]
        
        # åˆ†æç»“è®º
        comparison['analysis'] = self._analyze_baseline_results()
        
        return comparison
    
    def _analyze_baseline_results(self) -> Dict[str, Any]:
        """åˆ†æåŸºçº¿ç»“æœ"""
        
        analysis = {
            'best_algorithm': self._determine_best_algorithm(),
            'performance_comparison': {},
            'insights': []
        }
        
        # æ€§èƒ½å¯¹æ¯”åˆ†æ
        rewards = []
        success_rates = []
        
        for algorithm, result in self.baseline_results.items():
            if result.success:
                rewards.append(result.metrics.get('avg_episode_reward', 0.0))
                success_rates.append(result.metrics.get('success_rate', 0.0))
        
        if rewards:
            analysis['performance_comparison'] = {
                'reward_variance': np.var(rewards),
                'success_rate_variance': np.var(success_rates),
                'performance_gap': max(rewards) - min(rewards) if len(rewards) > 1 else 0
            }
        
        # ç”Ÿæˆæ´å¯Ÿ
        if analysis['performance_comparison'].get('performance_gap', 0) > 50:
            analysis['insights'].append("ä¸åŒç®—æ³•é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚")
        
        if analysis['performance_comparison'].get('reward_variance', 0) < 100:
            analysis['insights'].append("å„ç®—æ³•æ€§èƒ½ç›¸å¯¹ç¨³å®š")
        
        return analysis
    
    def _determine_best_algorithm(self) -> Optional[str]:
        """ç¡®å®šæœ€ä½³ç®—æ³•"""
        
        best_algorithm = None
        best_score = -float('inf')
        
        for algorithm, result in self.baseline_results.items():
            if result.success:
                # ç»¼åˆè¯„åˆ†
                success_rate = result.metrics.get('success_rate', 0.0)
                avg_reward = result.metrics.get('avg_episode_reward', 0.0)
                score = success_rate * 100 + avg_reward * 0.1
                
                if score > best_score:
                    best_score = score
                    best_algorithm = algorithm
        
        return best_algorithm
    
    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        if self.env:
            self.env.close()
        
        if self.eval_env:
            self.eval_env.close()
        
        self.logger.info("åŸºçº¿è®­ç»ƒå™¨èµ„æºå·²æ¸…ç†")
    
    def _execute_training(self) -> Dict[str, Any]:
        """æ‰§è¡Œè®­ç»ƒé€»è¾‘ - å®ç°æŠ½è±¡æ–¹æ³•"""
        return self.train().metadata if hasattr(self.train(), 'metadata') else {}
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, Any]:
        """è¯„ä¼°å½“å‰æ¨¡å‹ - å®ç°æŠ½è±¡æ–¹æ³•"""
        # å¦‚æœæœ‰æœ€ä½³ç®—æ³•ï¼Œè¯„ä¼°å®ƒ
        best_algorithm = self._determine_best_algorithm()
        if best_algorithm and best_algorithm in self.baseline_results:
            model = self.baseline_results[best_algorithm].trained_model
            return self._evaluate_sb3_model(model, best_algorithm)
        return {'mean_reward': 0.0, 'success_rate': 0.0}
    
    def save_model(self, path: Path, metadata: Optional[Dict] = None) -> bool:
        """ä¿å­˜æ¨¡å‹ - å®ç°æŠ½è±¡æ–¹æ³•"""
        best_algorithm = self._determine_best_algorithm()
        if best_algorithm and best_algorithm in self.baseline_results:
            try:
                model = self.baseline_results[best_algorithm].trained_model
                model.save(str(path))
                self.logger.info(f"æœ€ä½³åŸºçº¿æ¨¡å‹ ({best_algorithm}) å·²ä¿å­˜åˆ°: {path}")
                return True
            except Exception as e:
                self.logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        return False
    
    def load_model(self, path: Path) -> bool:
        """åŠ è½½æ¨¡å‹ - å®ç°æŠ½è±¡æ–¹æ³•"""
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ­£ç¡®çš„SB3ç®—æ³•
            # ç®€åŒ–å®ç°ï¼Œå‡è®¾æ˜¯PPO
            from stable_baselines3 import PPO
            model = PPO.load(str(path))
            # å°†åŠ è½½çš„æ¨¡å‹å­˜å‚¨åˆ°ç»“æœä¸­
            from ..core.base_trainer import TrainingResult
            self.baseline_results['loaded'] = TrainingResult(
                success=True,
                trained_model=model,
                metrics={'loaded_from': str(path)}
            )
            self.logger.info(f"åŸºçº¿æ¨¡å‹å·²ä» {path} åŠ è½½")
            return True
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
