#!/usr/bin/env python3

"""
åˆ†å±‚è®­ç»ƒå™¨ - HA-UAVå®Œæ•´åˆ†å±‚å†³ç­–è®­ç»ƒ
é›†æˆæ™ºèƒ½ä¼šè¯ç®¡ç†å’Œå¯è§†åŒ–ç³»ç»Ÿ
"""

import sys
import time
import logging
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.training.core.base_trainer import BaseTrainer, TrainingStage, TrainingResult
from src.training.core.environment_factory import EnvironmentFactory
from src.training.core.training_adapter import create_training_adapter


class HierarchicalTrainer(BaseTrainer):
    """HA-UAVåˆ†å±‚ç³»ç»Ÿè®­ç»ƒå™¨ - å®Œæ•´é›†æˆ"""
    
    def __init__(self, 
                 config: Dict[str, Any], 
                 foundation_model_path: Optional[Path] = None,
                 output_dir: str = "./models"):
        super().__init__(
            stage=TrainingStage.HIERARCHICAL,
            config=config,
            experiment_name="HA-UAV",
            stage_variant=None
        )
        
        self.foundation_model_path = foundation_model_path
        self.output_dir = output_dir
        self.env_factory = EnvironmentFactory()
        self.env = None
        self.ha_components = None
        self.training_adapter = None
        
        # åˆå§‹åŒ–progress_callbacks
        self.progress_callbacks = []
        
        # è½¨è¿¹è®°å½•ç›¸å…³å˜é‡
        self.trajectory_manager = None
        self.trajectory_episode_started = False
        self.current_episode_reward = 0.0
        self.current_episode_length = 0
        self.current_exploration_rate = 1.0
        
        # æ›´æ–°é…ç½®ä»¥åŒ…å«åŸºåº§æ¨¡å‹ä¿¡æ¯
        if foundation_model_path:
            self.config['foundation_model_path'] = str(foundation_model_path)
    
    def setup(self) -> bool:
        """è®¾ç½®HA-UAVç¯å¢ƒå’Œåˆ†å±‚ç­–ç•¥"""
        try:
            # åˆå§‹åŒ–ä¼šè¯ç®¡ç†ï¼ˆåŒ…å«å¯è§†åŒ–ï¼‰
            session_info = self.initialize_session(
                enable_trajectory=self.config.get('enable_trajectory', True),
                enable_tensorboard=self.config.get('enable_tensorboard', True),
                enable_visualization=self.config.get('enable_visualization', True),
                enable_rich_display=self.config.get('enable_rich_display', True)
            )
            
            self.logger.info(f"ä¼šè¯åˆå§‹åŒ–å®Œæˆ: {session_info['session_dir']}")
            
            # åˆ›å»ºHAUAVAviaryç¯å¢ƒ
            self.env = self.env_factory.create_environment(
                TrainingStage.HIERARCHICAL,
                self.config
            )
            self.logger.info("HAUAVAviaryç¯å¢ƒåˆ›å»ºå®Œæˆ")
            
            # åˆå§‹åŒ–HAç»„ä»¶ç®¡ç†å™¨
            from src.modules import HAComponentsManager, ModelConfiguration
            
            model_config = ModelConfiguration()
            # ä»configä¸­æ›´æ–°ç»„ä»¶é…ç½®
            if 'ha_config' in self.config:
                for key, value in self.config['ha_config'].items():
                    if hasattr(model_config, key):
                        setattr(model_config, key, value)
            
            # ğŸ¯ åˆ›å»ºHAComponentsManagerï¼ˆæ ¸å¿ƒç»„ä»¶ï¼‰
            try:
                self.ha_components = HAComponentsManager(model_config)
                success = self.ha_components.initialize_components(self.env)
                if success:
                    self.logger.info("âœ… HAComponentsManageråˆå§‹åŒ–æˆåŠŸ")
                else:
                    self.logger.error("âŒ HAComponentsManageråˆå§‹åŒ–å¤±è´¥")
                    return False
            except Exception as e:
                self.logger.error(f"HAComponentsManageråˆ›å»ºå¤±è´¥: {e}")
                return False
            
            # ğŸ¯ åŸºåº§æ¨¡å‹æƒé‡è¿ç§»ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self.foundation_model_path and self.foundation_model_path.exists():
                try:
                    self._transfer_foundation_weights()
                    self.logger.info(f"âœ… åŸºåº§æ¨¡å‹æƒé‡è¿ç§»å®Œæˆ: {self.foundation_model_path}")
                except Exception as e:
                    self.logger.warning(f"æƒé‡è¿ç§»å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ: {e}")
            
            # åºŸå¼ƒTrainingAdapterï¼Œç›´æ¥ä½¿ç”¨HAComponentsManager
            self.training_adapter = None  # æ˜ç¡®æ ‡è®°ä¸ºåºŸå¼ƒ
            
            self.write_stage_message("HA-UAVåˆ†å±‚è®­ç»ƒå™¨è®¾ç½®å®Œæˆ")
            return True
                
        except Exception as e:
            self.logger.error(f"åˆ†å±‚è®­ç»ƒå™¨è®¾ç½®å¤±è´¥: {e}")
            if self.visualization_manager:
                self.visualization_manager.write_message(f"è®¾ç½®å¤±è´¥: {e}", "ERROR")
            return False
    
    def _transfer_foundation_weights(self):
        """å®ç°åŸºåº§æ¨¡å‹åˆ°åˆ†å±‚æ¨¡å‹çš„æƒé‡è¿ç§»"""
        from stable_baselines3 import PPO
        
        try:
            # åŠ è½½åŸºåº§PPOæ¨¡å‹
            foundation_model = PPO.load(str(self.foundation_model_path))
            foundation_weights = foundation_model.policy.state_dict()
            
            # è¿ç§»åˆ°åˆ†å±‚ç­–ç•¥çš„å…±äº«å±‚
            if self.ha_components and self.ha_components.policy:
                hierarchical_weights = self.ha_components.policy.state_dict()
                
                # æƒé‡æ˜ å°„è§„åˆ™ï¼ˆéœ€è¦æ ¹æ®å®é™…ç½‘ç»œç»“æ„è°ƒæ•´ï¼‰
                transferred_count = 0
                for key, value in foundation_weights.items():
                    if key in hierarchical_weights and value.shape == hierarchical_weights[key].shape:
                        hierarchical_weights[key] = value.clone()
                        transferred_count += 1
                        self.logger.debug(f"æƒé‡è¿ç§»: {key}")
                
                # åŠ è½½è¿ç§»åçš„æƒé‡
                self.ha_components.policy.load_state_dict(hierarchical_weights)
                self.logger.info(f"æˆåŠŸè¿ç§» {transferred_count} ä¸ªæƒé‡å‚æ•°")
                
        except Exception as e:
            self.logger.error(f"æƒé‡è¿ç§»å¤±è´¥: {e}")
            raise
    
    def _execute_training(self) -> Dict[str, Any]:
        """æ‰§è¡Œåˆ†å±‚è®­ç»ƒé€»è¾‘ - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿å›è°ƒæ­£å¸¸å·¥ä½œ"""
        total_timesteps = self.config.get('total_timesteps', 100000)
        evaluation_frequency = self.config.get('evaluation_frequency', 10000)
        checkpoint_frequency = self.config.get('checkpoint_frequency', 50000)
        
        # ğŸ¯ å…³é”®ï¼šä½¿ç”¨å¾ˆå°çš„å­¦ä¹ é—´éš”ï¼Œç¡®ä¿å›è°ƒåŠæ—¶è§¦å‘
        learn_interval = min(100, total_timesteps // 20)  # å¾ˆå°çš„é—´éš”
        
        training_stats = []
        start_time = time.time()
        self.logger.info(f"å¼€å§‹HA-UAVåˆ†å±‚è®­ç»ƒ: {total_timesteps:,} æ­¥ (å­¦ä¹ é—´éš”: {learn_interval} æ­¥)")
        
        # ğŸ”§ ç«‹å³è°ƒç”¨ç¬¬ä¸€æ¬¡å›è°ƒï¼Œç¡®ä¿è¿›åº¦æ¡å¯åŠ¨
        if self.visualization_manager:
            initial_metrics = {
                'episode': 0,
                'total_reward': 0.0,
                'exploration_rate': 1.0,
                'policy_loss': 0.0,
                'value_loss': 0.0
            }
            self.visualization_manager.on_step(0, initial_metrics)
            self.logger.info("âœ… åˆå§‹å›è°ƒå·²è§¦å‘ï¼Œè¿›åº¦æ¡åº”å·²å¯åŠ¨")
        
        while self.current_step < total_timesteps:
            try:
                if self.ha_components is None:
                    self.logger.error("HAComponentsManageræœªåˆå§‹åŒ–ï¼Œé€€å‡ºè®­ç»ƒ")
                    break
                
                # ğŸ”§ ä½¿ç”¨è¶…å°æ‰¹æ¬¡ï¼Œé¿å…é•¿æ—¶é—´å¡ä½
                steps_to_collect = min(learn_interval, total_timesteps - self.current_step, 50)  # æœ€å¤š50æ­¥
                
                self.logger.info(f"å¼€å§‹æ”¶é›†ç»éªŒ: {steps_to_collect} æ­¥ (å½“å‰æ­¥æ•°: {self.current_step})")
                
                # é™æ—¶æ”¶é›†ç»éªŒï¼Œé¿å…æ— é™å¡ä½
                collect_start = time.time()
                timeout = 10.0  # 10ç§’è¶…æ—¶
                
                try:
                    # ğŸ¯ å…³é”®ä¿®å¤ï¼šå°†è½¨è¿¹è®°å½•é›†æˆåˆ°collect_rolloutä¸­
                    rollout_stats = self.ha_components.collect_rollout(
                        self.env, 
                        n_steps=steps_to_collect,
                        trajectory_callback=self  # ä¼ é€’selfä½œä¸ºè½¨è¿¹è®°å½•å›è°ƒ
                    )
                    collect_time = time.time() - collect_start
                    
                    if collect_time > 5.0:
                        self.logger.warning(f"ç»éªŒæ”¶é›†è€—æ—¶è¿‡é•¿: {collect_time:.1f}s")
                    
                except Exception as e:
                    self.logger.error(f"ç»éªŒæ”¶é›†å¤±è´¥: {e}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°è¿›åº¦
                    rollout_stats = {
                        'total_steps': steps_to_collect,
                        'episodes': 0,
                        'mean_reward': 0.0
                    }
                
                # ğŸ¯ ç«‹å³æ›´æ–°æ­¥æ•°å’Œè§¦å‘å›è°ƒ
                steps_collected = rollout_stats.get('total_steps', steps_to_collect)
                self.current_step += steps_collected
                episodes_this_batch = rollout_stats.get('episodes', 0)
                
                # ğŸ”§ æ ¸å¿ƒï¼šç«‹å³è§¦å‘å›è°ƒï¼Œç¡®ä¿è¿›åº¦æ¡æ›´æ–°
                step_metrics = {
                    'episode': episodes_this_batch,
                    'total_reward': rollout_stats.get('mean_reward', 0.0),
                    'exploration_rate': max(0.0, 1.0 - self.current_step / total_timesteps),
                    'policy_loss': 0.0,
                    'value_loss': 0.0
                }
                
                # è°ƒç”¨å¯è§†åŒ–ç®¡ç†å™¨å›è°ƒ
                if self.visualization_manager:
                    self.visualization_manager.on_step(self.current_step, step_metrics)
                    self.logger.info(f"âœ… å›è°ƒå·²è§¦å‘: {self.current_step}/{total_timesteps} ({self.current_step/total_timesteps:.1%})")
                
                # è°ƒç”¨è®­ç»ƒå™¨å›è°ƒ
                self.on_step_callback(self.current_step, step_metrics)
                
                # å°è¯•ç­–ç•¥æ›´æ–°ï¼ˆå¿«é€Ÿè·³è¿‡å¦‚æœæ¡ä»¶ä¸æ»¡è¶³ï¼‰
                try:
                    if hasattr(self.ha_components, 'buffer'):
                        # æ­£ç¡®è·å–ç¼“å†²åŒºå¤§å°ï¼šä½¿ç”¨poså±æ€§
                        buffer_pos = getattr(self.ha_components.buffer, 'pos', 0)
                        buffer_full = getattr(self.ha_components.buffer, 'full', False)
                        # åŠ¨æ€è°ƒæ•´æœ€å°æ‰¹æ¬¡å¤§å°ï¼Œé€‚åº”çŸ­æœŸè®­ç»ƒ
                        min_size = min(self.config.get('batch_size', 32), total_timesteps // 2)
                        min_size = max(min_size, 10)  # è‡³å°‘10æ­¥æ‰è¿›è¡Œæ›´æ–°
                        
                        # ç¼“å†²åŒºå¤§å°ä¸ºposï¼ˆå¦‚æœæœªæ»¡ï¼‰æˆ–buffer_sizeï¼ˆå¦‚æœå·²æ»¡ï¼‰
                        effective_buffer_size = buffer_pos if not buffer_full else self.ha_components.buffer.buffer_size
                        
                        if effective_buffer_size >= min_size:
                            self.logger.info(f"æ‰§è¡Œç­–ç•¥æ›´æ–°: buffer_size={effective_buffer_size}")
                            training_stats_batch = self.ha_components.update_policy_from_buffer()
                            step_metrics.update({
                                'policy_loss': training_stats_batch.get('policy_loss', 0.0),
                                'value_loss': training_stats_batch.get('value_loss', 0.0)
                            })
                            
                            # å†æ¬¡å›è°ƒåŒ…å«è®­ç»ƒæŒ‡æ ‡
                            if self.visualization_manager:
                                self.visualization_manager.on_step(self.current_step, step_metrics)
                        else:
                            self.logger.info(f"è·³è¿‡ç­–ç•¥æ›´æ–°: buffer_size={effective_buffer_size} < {min_size}")
                except Exception as e:
                    self.logger.warning(f"ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
                
                # Episodeå›è°ƒ
                if episodes_this_batch > 0:
                    self.on_episode_callback(episodes_this_batch, rollout_stats)
                
                # å®šæœŸè¯„ä¼°ï¼ˆå¿«é€Ÿè·³è¿‡ï¼‰
                if self.current_step % evaluation_frequency == 0 and self.current_step > 0:
                    self.logger.info(f"å¼€å§‹å®šæœŸè¯„ä¼° (æ­¥æ•°: {self.current_step})")
                    try:
                        eval_results = self._perform_evaluation()
                        self.on_evaluation_callback(eval_results)
                    except Exception as e:
                        self.logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if self.current_step % checkpoint_frequency == 0 and self.current_step > 0:
                    try:
                        self.on_checkpoint_callback(self.current_step)
                    except Exception as e:
                        self.logger.warning(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                
                # è¿›åº¦æ—¥å¿—
                recent_reward = rollout_stats.get('mean_reward', 0.0)
                self.logger.info(f"è®­ç»ƒè¿›åº¦: {self.current_step:,}/{total_timesteps:,} "
                               f"({self.current_step/total_timesteps:.1%}) | "
                               f"å¥–åŠ±: {recent_reward:.3f} | "
                               f"Episodes: {episodes_this_batch}")
                
                # è®°å½•ç»Ÿè®¡
                training_stats.append(rollout_stats)
                
            except Exception as e:
                self.logger.error(f"è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # å³ä½¿å‡ºé”™ä¹Ÿè¦æ›´æ–°è¿›åº¦ï¼Œé¿å…å¡ä½
                self.current_step += learn_interval
                if self.visualization_manager:
                    error_metrics = {
                        'episode': 0,
                        'total_reward': 0.0,
                        'exploration_rate': 0.0,
                        'policy_loss': 0.0,
                        'value_loss': 0.0
                    }
                    self.visualization_manager.on_step(self.current_step, error_metrics)
                break
        
        # æ±‡æ€»è®­ç»ƒç»Ÿè®¡
        elapsed_time = time.time() - start_time
        total_reward = sum(stats.get('mean_reward', 0.0) for stats in training_stats)
        total_episodes = sum(stats.get('episodes', 0) for stats in training_stats)
        
        final_stats = {
            'final_reward': total_reward / len(training_stats) if training_stats else 0.0,
            'training_time': elapsed_time,
            'training_completion_rate': self.current_step / total_timesteps if total_timesteps > 0 else 0.0,
            'training_batches': len(training_stats),
            'total_episodes': total_episodes
        }
        
        self.logger.info(f"HA-UAVåˆ†å±‚è®­ç»ƒå®Œæˆ: å®Œæˆæ­¥æ•°={self.current_step}, æœ€ç»ˆå¥–åŠ±={final_stats['final_reward']:.3f}")
        
        return final_stats
    
    def _perform_evaluation(self) -> Dict[str, float]:
        """æ‰§è¡Œè¯„ä¼° - å§”æ‰˜ç»™HAComponentsManagerå¹¶å¢å¼ºæˆåŠŸç‡åˆ¤æ–­"""
        num_eval_episodes = self.config.get('eval_episodes', 10)
        
        if self.visualization_manager:
            self.visualization_manager.on_evaluation_start(num_eval_episodes)
        
        try:
            eval_results = self.evaluate(num_eval_episodes)
            
            # è¯„ä¼°ç»“æœåˆ†æå’Œæ—¥å¿—
            success_rate = eval_results.get('success_rate', 0.0)
            mean_reward = eval_results.get('mean_reward', 0.0)
            
            if success_rate > 0.8:
                self.logger.info(f"ğŸ‰ ä¼˜ç§€æ€§èƒ½: æˆåŠŸç‡ {success_rate:.2%}, å¹³å‡å¥–åŠ± {mean_reward:.2f}")
            elif success_rate > 0.5:
                self.logger.info(f"âœ… è‰¯å¥½æ€§èƒ½: æˆåŠŸç‡ {success_rate:.2%}, å¹³å‡å¥–åŠ± {mean_reward:.2f}")
            else:
                self.logger.warning(f"âš ï¸ éœ€è¦æ”¹è¿›: æˆåŠŸç‡ {success_rate:.2%}, å¹³å‡å¥–åŠ± {mean_reward:.2f}")
            
            self.write_stage_message(f"è¯„ä¼°å®Œæˆ: æˆåŠŸç‡ {success_rate:.1%}")
            # ä¸å†è°ƒç”¨ on_evaluation_callback é¿å…æ— é™é€’å½’
            return eval_results
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {'mean_reward': 0.0, 'success_rate': 0.0, 'error': str(e)}
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, Any]:
        """è¯„ä¼°æ–¹æ³• - å§”æ‰˜ç»™HAComponentsManagerå¹¶å¢å¼ºæˆåŠŸç‡åˆ¤æ–­"""
        self.logger.info(f"ğŸ“‹ å¼€å§‹è¯„ä¼°: {num_episodes} Episodes")
        
        if not self.ha_components or not self.env:
            self.logger.error("âŒ ç»„ä»¶æˆ–ç¯å¢ƒæœªåˆå§‹åŒ–")
            return {'mean_reward': 0.0, 'success_rate': 0.0, 'error': 'Components not initialized'}
        
        episode_rewards = []
        episode_lengths = []
        success_count = 0
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.logger.info("ğŸ”§ è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
        self.ha_components.set_training_mode(False)
        
        try:
            for episode in range(num_episodes):
                self.logger.info(f"ğŸ¯ å¼€å§‹Episode {episode + 1}/{num_episodes}")
                obs_batch, info = self.env.reset()
                self.logger.info(f"âœ… ç¯å¢ƒé‡ç½®å®Œæˆï¼Œè§‚æµ‹å½¢çŠ¶: {obs_batch.shape}")
                
                # é€‚é…è§‚æµ‹æ ¼å¼ï¼šä»(NUM_DRONES, OBS_DIM)å–ç¬¬ä¸€ä¸ªæ— äººæœº
                if obs_batch.ndim > 1 and obs_batch.shape[0] > 0:
                    obs = obs_batch[0]  # [86] å•æ™ºèƒ½ä½“è§‚æµ‹
                else:
                    obs = obs_batch
                
                episode_reward = 0
                episode_length = 0
                done = False
                max_steps = 100  # å‡å°‘æœ€å¤§æ­¥æ•°é˜²æ­¢å¡ä½
                
                self.logger.info(f"ğŸ”„ å¼€å§‹Episode {episode + 1} æ‰§è¡Œå¾ªç¯ï¼Œæœ€å¤§æ­¥æ•°: {max_steps}")
                
                while not done and episode_length < max_steps:
                    # ğŸ¯ ç›´æ¥ä½¿ç”¨HAComponentsManagerçš„é¢„æµ‹æ–¹æ³•
                    action = self.ha_components.predict(obs)
                    
                    obs_batch, reward, terminated, truncated, info = self.env.step(action.reshape(1, -1))
                    
                    # é€‚é…è§‚æµ‹æ ¼å¼
                    if obs_batch.ndim > 1 and obs_batch.shape[0] > 0:
                        obs = obs_batch[0]
                    else:
                        obs = obs_batch
                    
                    episode_reward += reward
                    episode_length += 1
                    done = terminated or truncated
                    
                    # æ¯10æ­¥è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if episode_length % 10 == 0:
                        self.logger.info(f"  ğŸ“Š Episode {episode + 1} æ­¥æ•°: {episode_length}, å¥–åŠ±: {episode_reward:.3f}")
                    
                    # ğŸ”§ å¢å¼ºçš„æˆåŠŸæ¡ä»¶åˆ¤æ–­
                    if self._is_success(terminated, truncated, info, episode_length):
                        success_count += 1
                        break
                
                self.logger.info(f"âœ… Episode {episode + 1} å®Œæˆ: å¥–åŠ±={episode_reward:.3f}, æ­¥æ•°={episode_length}, å®ŒæˆåŸå› ={'ç»ˆæ­¢' if terminated else 'æˆªæ–­' if truncated else 'è¶…æ—¶'}")
                
                episode_rewards.append(episode_reward)
                episode_lengths.append(episode_length)
            
            # æ¢å¤è®­ç»ƒæ¨¡å¼
            self.logger.info("ğŸ”§ æ¢å¤è®­ç»ƒæ¨¡å¼")
            self.ha_components.set_training_mode(True)
            
            self.logger.info(f"ğŸ“ˆ è¯„ä¼°ç»Ÿè®¡: å¹³å‡å¥–åŠ±={np.mean(episode_rewards):.3f}, æˆåŠŸç‡={success_count}/{num_episodes}")
            
            return {
                'mean_reward': float(np.mean(episode_rewards)),
                'std_reward': float(np.std(episode_rewards)),
                'success_rate': success_count / num_episodes,
                'mean_episode_length': float(np.mean(episode_lengths)),
                'episodes_evaluated': num_episodes
            }
            
        except Exception as e:
            self.ha_components.set_training_mode(True)
            self.logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {'mean_reward': 0.0, 'success_rate': 0.0, 'error': str(e)}
    
    def _is_success(self, terminated: bool, truncated: bool, info: dict, episode_length: int) -> bool:
        """å¢å¼ºçš„æˆåŠŸæ¡ä»¶åˆ¤æ–­"""
        if not terminated:
            return False
        
        # 1. æ˜¾å¼æˆåŠŸæ ‡å¿—
        if info.get('navigation_success', False):
            return True
        
        # 2. æ¢ç´¢å®Œæˆ
        if info.get('exploration_completed', False):
            return True
            
        # 3. åŸºäºæ¢ç´¢ç‡çš„æˆåŠŸåˆ¤æ–­
        exploration_rate = info.get('exploration_rate', 0.0)
        if exploration_rate > 0.8:  # 80%ä»¥ä¸Šæ¢ç´¢ç‡
            return True
        
        # 4. åŸºäºå¥–åŠ±é˜ˆå€¼çš„æˆåŠŸåˆ¤æ–­
        total_reward = info.get('total_reward', 0.0)
        if total_reward > 100:  # è‡ªå®šä¹‰å¥–åŠ±é˜ˆå€¼
            return True
        
        # 5. åŸºäºepisodeé•¿åº¦çš„æˆåŠŸåˆ¤æ–­ï¼ˆé¿å…æ—©æœŸç»ˆæ­¢ï¼‰
        if episode_length > 500:  # é•¿æœŸå­˜æ´»
            return True
            
        return False
        
        try:
            for episode in range(num_episodes):
                # HAUAVAviary.resetè¿”å› (NUM_DRONES, OBS_DIM) æ ¼å¼
                obs_batch, info = self.env.reset()
                # ä½¿ç”¨é€‚é…å™¨å¤„ç†è§‚æµ‹æ ¼å¼
                obs = self.training_adapter.adapt_observation_format(obs_batch)
                
                episode_reward = 0
                episode_length = 0
                done = False
                
                while not done:
                    # ä½¿ç”¨é€‚é…å™¨è¿›è¡Œå®‰å…¨é¢„æµ‹
                    action = self.training_adapter.safe_predict(obs, deterministic=True)
                    
                    # HAUAVAviary.stepè¿”å›å¤šæ™ºèƒ½ä½“æ ¼å¼
                    env_output = self.env.step(action)
                    obs, reward, terminated, truncated, info = self.training_adapter.adapt_step_output(env_output)
                    
                    episode_reward += reward
                    episode_length += 1
                    done = terminated or truncated
                    
                    # åˆ¤æ–­æˆåŠŸæ¡ä»¶
                    if terminated and info.get('navigation_success', False):
                        success_count += 1
                
                episode_rewards.append(episode_reward)
                episode_lengths.append(episode_length)
            
            return {
                'mean_reward': float(np.mean(episode_rewards)),
                'std_reward': float(np.std(episode_rewards)),
                'success_rate': success_count / num_episodes,
                'mean_episode_length': float(np.mean(episode_lengths)),
                'episodes_evaluated': num_episodes
            }
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                'mean_reward': 0.0,
                'std_reward': 0.0,
                'success_rate': 0.0,
                'mean_episode_length': 0.0,
                'episodes_evaluated': 0,
                'error': str(e)
            }
    
    def _perform_final_evaluation(self) -> Dict[str, Any]:
        """é‡å†™æœ€ç»ˆè¯„ä¼°ï¼Œæ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯"""
        self.logger.info("ğŸš å¼€å§‹æœ€ç»ˆè¯„ä¼°...")
        
        try:
            final_eval_episodes = self.config.get('final_eval_episodes', 3)  # å‡å°‘åˆ°3ä¸ªEpisode
            self.logger.info(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°é…ç½®: {final_eval_episodes} Episodes")
            
            if self.visualization_manager:
                self.visualization_manager.write_stage_message("ğŸ—ï¸ å¼€å§‹æœ€ç»ˆè¯„ä¼°...")
            
            self.logger.info("ğŸ“‹ è°ƒç”¨evaluateæ–¹æ³•...")
            final_eval_results = self.evaluate(final_eval_episodes)
            self.logger.info(f"ğŸ“ˆ è¯„ä¼°ç»“æœ: {final_eval_results}")
            
            # æ·»åŠ final_å‰ç¼€åŒºåˆ†æœ€ç»ˆè¯„ä¼°
            final_results = {}
            for key, value in final_eval_results.items():
                final_results[f'final_{key}'] = value
            
            self.logger.info(f"ğŸ—ï¸ æœ€ç»ˆè¯„ä¼°å®Œæˆ - å¥–åŠ±: {final_eval_results.get('mean_reward', 0.0):.3f}")
            
            if self.visualization_manager:
                self.visualization_manager.write_stage_message(
                    f"ğŸ—ï¸ æœ€ç»ˆè¯„ä¼°å®Œæˆ - å¥–åŠ±: {final_eval_results.get('mean_reward', 0.0):.3f}"
                )
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"âŒ æœ€ç»ˆè¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'final_evaluation_error': str(e)}
    
    def save_model(self, path: Path, metadata: Optional[Dict] = None) -> bool:
        """ä¿å­˜HA-UAVæ¨¡å‹"""
        if not self.ha_components:
            self.logger.error("HAç»„ä»¶æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜æ¨¡å‹")
            return False
        
        try:
            self.ha_components.save_model(str(path))
            self.logger.info(f"HA-UAVæ¨¡å‹å·²ä¿å­˜: {path}")
            return True
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def load_model(self, path: Path) -> bool:
        """åŠ è½½HA-UAVæ¨¡å‹"""
        if not self.ha_components or not self.env:
            self.logger.error("ç»„ä»¶æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
            return False
        
        try:
            self.ha_components.load_model(str(path), self.env)
            self.logger.info(f"HA-UAVæ¨¡å‹å·²åŠ è½½: {path}")
            return True
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False


    def on_step_callback(self, step: int, metrics: Dict[str, Any]):
        """æ­¥éª¤å›è°ƒ - æ›´æ–°å¯è§†åŒ–å’ŒæŒ‡æ ‡"""
        super().on_step_callback(step, metrics)
        
        # æ›´æ–°å¯è§†åŒ–ç®¡ç†å™¨
        if self.visualization_manager:
            # æ„å»ºæ ‡å‡†åŒ–çš„æ­¥éª¤æŒ‡æ ‡
            step_metrics = {
                'step': step,
                'mean_reward': metrics.get('mean_reward', 0.0),
                'episode_length': metrics.get('episode_length', 0),
                'buffer_size': metrics.get('buffer_size', 0),
                'exploration_rate': metrics.get('exploration_rate', 0.0),
                'loss': metrics.get('loss', 0.0),
                'learning_rate': metrics.get('learning_rate', 0.0)
            }
            
            # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            try:
                self.visualization_manager.on_step(step, step_metrics)
            except AttributeError:
                # å¦‚æœæ–¹æ³•ä¸å­˜åœ¨ï¼Œå°è¯•æ›´æ–°æŒ‡æ ‡
                self.visualization_manager.update_metrics(step_metrics)
    
    def on_episode_callback(self, episode: int, metrics: Dict[str, Any]):
        """Episodeå›è°ƒ"""
        self.current_episode = episode
        
        episode_reward = metrics.get('mean_reward', 0.0)
        if episode_reward > self.best_reward:
            self.best_reward = episode_reward
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if self.session_manager:
                best_model_path = self.session_manager.get_model_save_path("best_model")
                self.save_model(best_model_path)
                
                if self.visualization_manager:
                    self.visualization_manager.on_model_save(
                        "æœ€ä½³", 
                        self.best_reward, 
                        str(best_model_path)
                    )
        
        if self.visualization_manager:
            episode_info = {
                'episode': episode,
                'mean_reward': episode_reward,
                'episode_length': metrics.get('episode_length', 0),
                'is_best': episode_reward > self.best_reward
            }
            # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            try:
                self.visualization_manager.on_episode_end(episode, episode_info)
            except AttributeError:
                self.visualization_manager.update_metrics(episode_info)
    
    def on_evaluation_callback(self, eval_results: Dict[str, float]):
        """è¯„ä¼°å›è°ƒ"""
        self.evaluation_history.append(eval_results)
        
        if self.visualization_manager:
            try:
                self.visualization_manager.on_evaluation_end(eval_results)
            except AttributeError:
                self.visualization_manager.update_metrics(eval_results)
    
    def on_checkpoint_callback(self, step: int):
        """æ£€æŸ¥ç‚¹å›è°ƒ"""
        if self.session_manager:
            checkpoint_path = self.session_manager.get_model_save_path(f"checkpoint_{step}")
            self.save_model(checkpoint_path)
            
            if self.visualization_manager:
                self.visualization_manager.write_stage_message(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: step {step}")
    
    def write_stage_message(self, message: str):
        """å†™å…¥é˜¶æ®µæ¶ˆæ¯"""
        if self.visualization_manager:
            self.visualization_manager.write_stage_message(message)
        else:
            self.logger.info(message)
    
    def write_message(self, message: str, msg_type: str = "INFO"):
        """å†™å…¥æ¶ˆæ¯"""
        if self.visualization_manager:
            self.visualization_manager.write_message(message, msg_type)
        else:
            if msg_type == "ERROR":
                self.logger.error(message)
            elif msg_type == "WARNING":
                self.logger.warning(message)
            else:
                self.logger.info(message)
    
    def _start_trajectory_episode(self, episode_num: int):
        """å¼€å§‹æ–°episodeçš„è½¨è¿¹è®°å½• - ä¸åŸºç¡€è®­ç»ƒå™¨ä¸€è‡´"""
        if self.trajectory_manager:
            try:
                self.trajectory_manager.start_new_episode(episode_num)
                self.trajectory_episode_started = True
                self.current_episode_reward = 0.0
                self.current_episode_length = 0
            except Exception as e:
                self.logger.warning(f"å¯åŠ¨è½¨è¿¹è®°å½•å¤±è´¥: {e}")
                self.trajectory_episode_started = False

    def _log_trajectory_step(self, obs, action, reward, next_obs, done, info):
        """è®°å½•å•æ­¥è½¨è¿¹æ•°æ® - å¤åˆ¶åŸºç¡€è®­ç»ƒå™¨çš„å®ç°"""
        if not self.trajectory_manager or not self.trajectory_episode_started:
            return
        
        try:
            # ğŸ”§ æ­£ç¡®è§£æè½¨è¿¹æ•°æ®ï¼šä»ç¯å¢ƒçš„get_trajectory_step_dataæ–¹æ³•è·å–å‡†ç¡®æ•°æ®
            trajectory_data = None
            if hasattr(self.env, 'get_trajectory_step_data'):
                try:
                    trajectory_data = self.env.get_trajectory_step_data(drone_idx=0)
                except Exception as e:
                    # å¦‚æœå‚æ•°åä¸åŒ¹é…ï¼Œå°è¯•æ— å‚æ•°è°ƒç”¨
                    try:
                        trajectory_data = self.env.get_trajectory_step_data()
                    except:
                        trajectory_data = None
            
            if trajectory_data is not None:
                # ä½¿ç”¨ç¯å¢ƒæä¾›çš„å‡†ç¡®æ•°æ®
                current_position = trajectory_data['current_position'].tolist() if isinstance(trajectory_data['current_position'], np.ndarray) else list(trajectory_data['current_position'])
                current_velocity = trajectory_data['current_velocity'].tolist() if isinstance(trajectory_data['current_velocity'], np.ndarray) else list(trajectory_data['current_velocity'])
                target_action = trajectory_data.get('target_velocity', [0.0, 0.0, 0.0, 0.0])
                rpm_action = trajectory_data.get('rpm_action', [0.0, 0.0, 0.0, 0.0])
                
                # è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°å®é™…æ•°æ®
                if self.config.get('debug', False) and self.current_episode_length < 5:
                    self.logger.info(f"=== åˆ†å±‚è½¨è¿¹è®°å½•è°ƒè¯• (æ­¥éª¤ {self.current_episode_length}) ===")
                    self.logger.info(f"å®é™…ä½ç½®: {current_position}")
                    self.logger.info(f"å®é™…é€Ÿåº¦: {current_velocity}")
                    self.logger.info(f"ç›®æ ‡åŠ¨ä½œ: {target_action}")
                    self.logger.info(f"RPMè¾“å‡º: {rpm_action}")
                    if isinstance(info, dict) and 'direct_state' in info:
                        direct_state = info['direct_state']
                        if isinstance(direct_state, dict) and 'current_velocity' in direct_state:
                            self.logger.info(f"ç›´æ¥çŠ¶æ€é€Ÿåº¦: {direct_state['current_velocity']}")
            else:
                # åå¤‡æ–¹æ¡ˆï¼šä»è§‚æµ‹æ•°æ®è§£æï¼ˆåŸºäºHAUAVAviaryçš„86ç»´è§‚æµ‹æ ¼å¼ï¼‰
                if isinstance(obs, np.ndarray) and len(obs) >= 12:
                    # 86ç»´è§‚æµ‹æ ¼å¼çš„ä½ç½®å’Œé€Ÿåº¦ä¿¡æ¯
                    current_position = obs[:3].tolist()      # ä½ç½® [0:3]
                    current_velocity = obs[10:13].tolist()   # é€Ÿåº¦ [10:13] 
                else:
                    current_position = [0.0, 0.0, 0.0]
                    current_velocity = [0.0, 0.0, 0.0]
                
                # ä»infoä¸­è·å–æ§åˆ¶å™¨æ•°æ®
                target_action = [0.0, 0.0, 0.0, 0.0]
                rpm_action = [0.0, 0.0, 0.0, 0.0]
                
                if isinstance(info, dict):
                    # æ£€æŸ¥direct_stateä¸­çš„é€Ÿåº¦ä¿¡æ¯
                    if 'direct_state' in info and isinstance(info['direct_state'], dict):
                        direct_state = info['direct_state']
                        if 'current_velocity' in direct_state:
                            vel_tensor = direct_state['current_velocity']
                            if hasattr(vel_tensor, 'tolist'):
                                current_velocity = vel_tensor.tolist()
                            elif hasattr(vel_tensor, '__len__') and len(vel_tensor) >= 3:
                                current_velocity = list(vel_tensor[:3])
                    
                    # è·å–åˆ†å±‚æ§åˆ¶ç›¸å…³æ•°æ®
                    if 'hierarchical_control' in info and isinstance(info['hierarchical_control'], dict):
                        target_data = info['hierarchical_control'].get('high_level_target', [0.0, 0.0, 0.0, 0.0])
                        if hasattr(target_data, '__len__') and len(target_data) >= 4:
                            target_action = list(target_data[:4])
                    elif 'incremental_control' in info and isinstance(info['incremental_control'], dict):
                        target_data = info['incremental_control'].get('drone_0_target', [0.0, 0.0, 0.0, 0.0])
                        if hasattr(target_data, '__len__') and len(target_data) >= 4:
                            target_action = list(target_data[:4])
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯floatç±»å‹
            current_position = [float(x) for x in current_position[:3]]
            current_velocity = [float(x) for x in current_velocity[:3]]
            target_velocity = target_action[:3] if target_action and len(target_action) >= 3 else [0.0, 0.0, 0.0]
            
            # æ„å»ºè½¨è¿¹æ•°æ® - é€‚é…HAUAVAviary
            step_data = {
                'step': float(self.current_episode_length),
                'current_position': current_position,
                'current_velocity': current_velocity,
                'target_velocity': target_velocity,
                'model_action': action.tolist() if isinstance(action, np.ndarray) else list(action),
                'rpm_action': rpm_action.tolist() if isinstance(rpm_action, np.ndarray) else list(rpm_action) if rpm_action else [0.0, 0.0, 0.0, 0.0],
                'exploration_rate': float(getattr(self, 'current_exploration_rate', 1.0))
            }
            
            self.trajectory_manager.log_step(step_data)
            self.current_episode_length += 1
            self.current_episode_reward += reward
            
        except Exception as e:
            self.logger.warning(f"åˆ†å±‚è½¨è¿¹è®°å½•å¤±è´¥: {e}")
            # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°æ›´å¤šä¿¡æ¯
            if self.config.get('debug', False):
                self.logger.warning(f"obs shape: {obs.shape if isinstance(obs, np.ndarray) else type(obs)}")
                self.logger.warning(f"info keys: {list(info.keys()) if isinstance(info, dict) else type(info)}")
                import traceback
                self.logger.warning(f"åˆ†å±‚è½¨è¿¹è®°å½•è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

    def _finalize_trajectory_episode(self, episode_reward: float, episode_length: int, info: Dict[str, Any]):
        """å®Œæˆepisodeçš„è½¨è¿¹è®°å½• - ä¸åŸºç¡€è®­ç»ƒå™¨ä¸€è‡´"""
        if not self.trajectory_manager or not self.trajectory_episode_started:
            return
        
        try:
            # ç¡®å®šç»ˆæ­¢åŸå›  - é€‚é…åˆ†å±‚è®­ç»ƒ
            termination_reason = "unknown"
            if info.get('collision', False):
                termination_reason = "collision"
            elif info.get('timeout', False):
                termination_reason = "timeout"
            elif info.get('navigation_success', False):
                termination_reason = "navigation_success"
            elif info.get('exploration_completed', False):
                termination_reason = "exploration_completed"
            elif info.get('out_of_bounds', False):
                termination_reason = "out_of_bounds"
            else:
                termination_reason = "completed"
            
            self.trajectory_manager.finalize_episode(
                termination_reason=termination_reason,
                final_exploration_rate=float(getattr(self, 'current_exploration_rate', 1.0)),
                total_reward=episode_reward
            )
            
            self.trajectory_episode_started = False
            
        except Exception as e:
            self.logger.warning(f"å®Œæˆåˆ†å±‚è½¨è¿¹è®°å½•å¤±è´¥: {e}")
            self.trajectory_episode_started = False

def create_hierarchical_trainer(config: Dict[str, Any], 
                               foundation_model_path: Optional[Path] = None) -> HierarchicalTrainer:
    """åˆ›å»ºåˆ†å±‚è®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°"""
    return HierarchicalTrainer(config, foundation_model_path)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    test_config = {
        'total_timesteps': 10000,
        'evaluation_frequency': 2000,
        'eval_episodes': 5,
        'enable_trajectory': True,
        'enable_tensorboard': True,
        'enable_visualization': True,
        'enable_rich_display': True
    }
    
    trainer = create_hierarchical_trainer(test_config)
    
    print("å¼€å§‹æµ‹è¯•åˆ†å±‚è®­ç»ƒå™¨...")
    if trainer.setup():
        print("âœ… è®­ç»ƒå™¨è®¾ç½®æˆåŠŸ")
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç®€çŸ­çš„è®­ç»ƒæµ‹è¯•
    else:
        print("âŒ è®­ç»ƒå™¨è®¾ç½®å¤±è´¥")
