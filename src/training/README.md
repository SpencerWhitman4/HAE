# HA-UAV è®­ç»ƒç³»ç»Ÿæ¶æ„æ–‡æ¡£

## ğŸ“‹ ç³»ç»Ÿæ¦‚è¿°

HA-UAVè®­ç»ƒç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºå››å±‚æ¶æ„è®¾è®¡çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œå®ç°äº†ä»åŸºåº§æ¨¡å‹åˆ°ä¸“ä¸šä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµæ°´çº¿ã€‚ç³»ç»Ÿé‡‡ç”¨"åŸºåº§æ¨¡å‹é©±åŠ¨+ä¸‰åˆ†æ”¯å¹¶è¡Œ"çš„è®¾è®¡æ¨¡å¼ï¼Œä¸ºå®¤å†…æ— äººæœºå¯¼èˆªä»»åŠ¡æä¾›äº†å®Œæ•´çš„è®­ç»ƒè§£å†³æ–¹æ¡ˆã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### âœ… **å››å±‚æ¶æ„è®¾è®¡**
- **Coreå±‚**: åŸºç¡€è®¾æ–½å’ŒæŠ½è±¡æ¥å£ (BaseTrainer, EnvironmentFactory, TrainingPipeline)
- **Foundationå±‚**: åŸºåº§æ¨¡å‹è®­ç»ƒ (BaseFlightTrainer + BaseFlightAviary)
- **Brancheså±‚**: ä¸‰åˆ†æ”¯ä¸“ä¸šè®­ç»ƒ (HierarchicalTrainer/AblationTrainer/BaselineTrainer)
- **Orchestrationå±‚**: ç»Ÿä¸€è®­ç»ƒç¼–æ’ (TrainingOrchestrator)

### âœ… **ç»Ÿä¸€è®­ç»ƒé€»è¾‘**
- æ‰€æœ‰è®­ç»ƒå™¨å…±äº«ç›¸åŒçš„BaseTraineræŠ½è±¡æ¥å£
- æ ‡å‡†åŒ–çš„TrainingResultæ•°æ®æµ
- ä¸€è‡´çš„ä¼šè¯ç®¡ç†å’Œå¯è§†åŒ–ç³»ç»Ÿ

### âœ… **æ™ºèƒ½æ¨¡å‹è¿ç§»**
- åŸºåº§æ¨¡å‹è‡ªåŠ¨è¿ç§»åˆ°ä¸“ä¸šåˆ†æ”¯
- ModelTransferManagerå¤„ç†æƒé‡æ˜ å°„
- é¿å…ç¾éš¾æ€§é—å¿˜çš„æ¸è¿›å¼å­¦ä¹ 

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HA-UAV è®­ç»ƒç³»ç»Ÿ (å››å±‚æ¶æ„)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Orchestration Layer                           â”‚
â”‚        TrainingOrchestrator - ç»Ÿä¸€è®­ç»ƒç¼–æ’å’Œç»“æœåˆ†æ              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Branches Layer                              â”‚
â”‚ HierarchicalTrainer â”‚ AblationTrainer  â”‚ BaselineTrainer        â”‚
â”‚ (HAComponentsManager)â”‚(AblationManager) â”‚(SB3 PPO/SAC/TD3)      â”‚
â”‚   å®Œæ•´HA-UAVç³»ç»Ÿ     â”‚  B1/B2/B3æ¶ˆè   â”‚    åŸºçº¿ç®—æ³•å¯¹æ¯”        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Foundation Layer                             â”‚
â”‚  BaseFlightTrainer - åŸºåº§æ¨¡å‹è®­ç»ƒ (BaseFlightAviary)             â”‚
â”‚      æ‚¬åœè®­ç»ƒ â†’ é£è¡Œè®­ç»ƒ â†’ åŸºç¡€æŠ€èƒ½å»ºç«‹                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      Core Layer                                 â”‚
â”‚BaseTrainerâ”‚EnvironmentFactoryâ”‚ModelTransferManagerâ”‚Pipeline     â”‚
â”‚SessionManagerâ”‚VisualizationManagerâ”‚TrainingAdapterâ”‚Config      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ å®Œæ•´è®­ç»ƒæµç¨‹

### é˜¶æ®µ1: Foundation Training (åŸºåº§æ¨¡å‹è®­ç»ƒ)
```python
BaseFlightTrainer â†’ BaseFlightAviary â†’ PPOè®­ç»ƒ â†’ åŸºåº§æ¨¡å‹(.zip)
    â†“
è¯¾ç¨‹å­¦ä¹ : æ‚¬åœæŠ€èƒ½ â†’ å¯¼èˆªæŠ€èƒ½ â†’ åŸºç¡€é£è¡Œèƒ½åŠ›
    â†“
è¾“å‡º: foundation_model.zip (åŒ…å«åŸºç¡€æ§åˆ¶ç­–ç•¥)
```

### é˜¶æ®µ2: Model Transfer (æ¨¡å‹è¿ç§»)
```python
ModelTransferManager.transfer_weights()
    â†“
åŸºåº§æ¨¡å‹æƒé‡ â†’ æƒé‡æ˜ å°„ â†’ åˆ†æ”¯åˆå§‹åŒ–
    â†“
ä½å±‚æ§åˆ¶ç½‘ç»œ: ç»§æ‰¿åŸºåº§æƒé‡ (é¿å…é‡æ–°å­¦ä¹ åŸºç¡€æŠ€èƒ½)
é«˜å±‚å†³ç­–ç½‘ç»œ: éšæœºåˆå§‹åŒ– (å­¦ä¹ ä»»åŠ¡ç‰¹å®šç­–ç•¥)
```

### é˜¶æ®µ3: Branch Training (åˆ†æ”¯ä¸“ä¸šè®­ç»ƒ)
```python
# åˆ†å±‚è®­ç»ƒåˆ†æ”¯
HierarchicalTrainer â†’ HAUAVAviary â†’ HAComponentsManager
    â†“
å®Œæ•´HA-UAVç³»ç»Ÿ: é«˜å±‚ç­–ç•¥ + ä½å±‚æ‰§è¡Œ + çŠ¶æ€ç®¡ç†
    â†“
è¾“å‡º: hierarchical_model.zip

# æ¶ˆèå®éªŒåˆ†æ”¯  
AblationTrainer â†’ HAUAVAviary â†’ AblationComponentsManager
    â†“
B1: ç›´æ¥æ§åˆ¶ / B2: æ‰å¹³åŒ– / B3: å•æ­¥åˆ†å±‚
    â†“
è¾“å‡º: ablation_B1/B2/B3_model.zip

# åŸºçº¿å¯¹æ¯”åˆ†æ”¯
BaselineTrainer â†’ BaselineWrapper â†’ SB3ç®—æ³•
    â†“
PPO/SAC/TD3æ ‡å‡†ç®—æ³•è®­ç»ƒ
    â†“
è¾“å‡º: baseline_ppo/sac/td3_model.zip
```

### é˜¶æ®µ4: Result Analysis (ç»“æœåˆ†æ)
```python
TrainingOrchestrator.generate_comparison_report()
    â†“
æ€§èƒ½å¯¹æ¯”: æœ€ç»ˆå¥–åŠ±ã€æˆåŠŸç‡ã€è®­ç»ƒæ•ˆç‡
    â†“
æ¶ˆèåˆ†æ: éªŒè¯åˆ†å±‚æ¶æ„çš„æœ‰æ•ˆæ€§
    â†“
åŸºçº¿å¯¹æ¯”: ä¸æ ‡å‡†RLç®—æ³•çš„æ€§èƒ½å·®å¼‚
```

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 1. Core Layer - åŸºç¡€è®¾æ–½å±‚

#### BaseTrainer - æŠ½è±¡è®­ç»ƒå™¨åŸºç±»
```python
class BaseTrainer(ABC):
    """æ‰€æœ‰è®­ç»ƒå™¨çš„ç»Ÿä¸€æ¥å£"""
    
    def __init__(self, stage: TrainingStage, config: Dict[str, Any]):
        self.stage = stage
        self.config = config
        self.session_manager = None      # æ™ºèƒ½ä¼šè¯ç®¡ç†
        self.visualization_manager = None # å®æ—¶å¯è§†åŒ–
        self.progress_callbacks = []     # è¿›åº¦å›è°ƒ
    
    @abstractmethod
    def setup(self) -> bool:
        """è®¾ç½®è®­ç»ƒç¯å¢ƒå’Œæ¨¡å‹"""
        pass
    
    def train(self) -> TrainingResult:
        """æ ‡å‡†åŒ–è®­ç»ƒæµç¨‹"""
        # 1. åˆå§‹åŒ–ä¼šè¯å’Œå¯è§†åŒ–
        self.initialize_session()
        
        # 2. è®¾ç½®è®­ç»ƒå™¨
        if not self.setup():
            return TrainingResult(success=False, ...)
        
        # 3. æ‰§è¡Œå…·ä½“è®­ç»ƒ
        training_metrics = self._execute_training()
        
        # 4. ä¿å­˜æ¨¡å‹å’Œç»“æœ
        model_path = self._save_final_model()
        
        return TrainingResult(
            stage=self.stage,
            success=True,
            model_path=model_path,
            metrics=training_metrics
        )
    
    @abstractmethod
    def _execute_training(self) -> Dict[str, Any]:
        """å­ç±»å®ç°å…·ä½“è®­ç»ƒé€»è¾‘"""
        pass
```

#### EnvironmentFactory - ç»Ÿä¸€ç¯å¢ƒç®¡ç†
```python
class EnvironmentFactory:
    """æ ¹æ®è®­ç»ƒé˜¶æ®µåˆ›å»ºå¯¹åº”ç¯å¢ƒ"""
    
    def create_environment(self, stage: TrainingStage, config: Dict) -> gym.Env:
        if stage == TrainingStage.FOUNDATION:
            return self._create_baseflight_env(config)
        elif stage == TrainingStage.HIERARCHICAL:
            return self._create_hauav_env(config)
        elif stage == TrainingStage.ABLATION:
            return self._create_ablation_env(config)
        elif stage == TrainingStage.BASELINE:
            return self._create_baseline_env(config)
    
    def _create_baseflight_env(self, config):
        """åˆ›å»ºBaseFlightAviary - åŸºç¡€é£è¡Œè®­ç»ƒ"""
        flight_config = BaseFlightConfig(
            hover_training_steps=25000,
            flight_training_steps=75000,
            enable_curriculum=True
        )
        return BaseFlightAviary(config=flight_config)
    
    def _create_hauav_env(self, config):
        """åˆ›å»ºHAUAVAviary - åˆ†å±‚å¯¼èˆªè®­ç»ƒ"""
        hauav_config = HAUAVConfig(
            map_name="room_complex",
            max_episode_steps=1000,
            enable_hierarchical=True
        )
        return HAUAVAviary(config=hauav_config)
    
    def _create_baseline_env(self, config):
        """åˆ›å»ºåŸºçº¿ç¯å¢ƒ - SB3å…¼å®¹åŒ…è£…"""
        base_env = self._create_hauav_env(config)
        return BaselineWrapper(base_env, agent_type="sb3")
```

#### TrainingPipeline - æµæ°´çº¿ç¼–æ’
```python
class TrainingPipeline:
    """å››é˜¶æ®µè®­ç»ƒæµæ°´çº¿ç®¡ç†"""
    
    def run_sequential_training(self) -> Dict[str, TrainingResult]:
        """æ‰§è¡Œé¡ºåºè®­ç»ƒæµç¨‹"""
        results = {}
        
        # é˜¶æ®µ1: Foundationè®­ç»ƒ
        foundation_result = self._run_foundation_stage()
        results['foundation'] = foundation_result
        
        if foundation_result.success:
            # é˜¶æ®µ2: æ¨¡å‹è¿ç§»
            self._transfer_foundation_model(foundation_result.model_path)
            
            # é˜¶æ®µ3: åˆ†æ”¯è®­ç»ƒ (å¹¶è¡Œæˆ–ä¸²è¡Œ)
            branch_results = self._run_branch_stages()
            results.update(branch_results)
        
        return results
    
    def _run_foundation_stage(self) -> TrainingResult:
        """è¿è¡ŒåŸºåº§æ¨¡å‹è®­ç»ƒ"""
        from ..foundation import BaseFlightTrainer
        
        trainer = BaseFlightTrainer(
            config=self.config.foundation_config,
            env_factory=self.env_factory
        )
        
        return trainer.train()
    
    def _run_branch_stages(self) -> Dict[str, TrainingResult]:
        """è¿è¡Œåˆ†æ”¯è®­ç»ƒ"""
        branch_results = {}
        
        # åˆ†å±‚è®­ç»ƒ
        hierarchical_trainer = HierarchicalTrainer(
            config=self.config.hierarchical_config,
            foundation_model_path=self.foundation_model_path
        )
        branch_results['hierarchical'] = hierarchical_trainer.train()
        
        # æ¶ˆèå®éªŒ
        for ablation_type in ['B1', 'B2', 'B3']:
            ablation_trainer = AblationTrainer(
                config=self.config.ablation_config,
                ablation_type=ablation_type,
                foundation_model_path=self.foundation_model_path
            )
            branch_results[f'ablation_{ablation_type}'] = ablation_trainer.train()
        
        # åŸºçº¿å¯¹æ¯”
        for algorithm in ['ppo', 'sac', 'td3']:
            baseline_trainer = BaselineTrainer(
                config=self.config.baseline_config,
                algorithm=algorithm,
                foundation_model_path=self.foundation_model_path
            )
            branch_results[f'baseline_{algorithm}'] = baseline_trainer.train()
        
        return branch_results
```

### 2. Foundation Layer - åŸºåº§æ¨¡å‹å±‚

#### BaseFlightTrainer - åŸºåº§æ¨¡å‹è®­ç»ƒå™¨
```python
class BaseFlightTrainer(BaseTrainer):
    """åŸºäºBaseFlightAviaryçš„åŸºåº§æ¨¡å‹è®­ç»ƒ"""
    
    def __init__(self, config: Dict, env_factory: EnvironmentFactory):
        super().__init__(TrainingStage.FOUNDATION, config)
        self.env_factory = env_factory
        self.model = None
        
    def setup(self) -> bool:
        """è®¾ç½®BaseFlightAviaryå’ŒPPOæ¨¡å‹"""
        # åˆ›å»ºBaseFlightAviaryç¯å¢ƒ
        self.env = self.env_factory.create_environment(
            TrainingStage.FOUNDATION, 
            self.config
        )
        
        # åˆ›å»ºPPOæ¨¡å‹ - å­¦ä¹ åŸºç¡€é£è¡Œæ§åˆ¶
        from stable_baselines3 import PPO
        self.model = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=self.config.get('learning_rate', 3e-4),
            batch_size=self.config.get('batch_size', 256),
            gamma=self.config.get('gamma', 0.99),
            verbose=1
        )
        
        return True
    
    def _execute_training(self) -> Dict[str, Any]:
        """æ‰§è¡Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒ"""
        total_steps = 0
        training_stats = []
        
        # è¯¾ç¨‹å­¦ä¹ : æ‚¬åœ â†’ é£è¡Œ â†’ æ··åˆ
        curriculum_stages = [
            ('hover', self.config.get('hover_training_steps', 25000)),
            ('flight', self.config.get('flight_training_steps', 75000))
        ]
        
        for stage_name, timesteps in curriculum_stages:
            self.logger.info(f"å¼€å§‹è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ: {stage_name}")
            
            # è®¾ç½®ç¯å¢ƒä»»åŠ¡ç±»å‹
            self.env.set_task_type(stage_name)
            
            # è®­ç»ƒè¯¥é˜¶æ®µ
            self.model.learn(total_timesteps=timesteps)
            total_steps += timesteps
            
            # è¯„ä¼°å½“å‰é˜¶æ®µæ€§èƒ½
            eval_stats = self.evaluate(num_episodes=20)
            training_stats.append({
                'stage': stage_name,
                'steps': timesteps,
                'eval_stats': eval_stats
            })
            
            self.logger.info(f"é˜¶æ®µ {stage_name} å®Œæˆ: {eval_stats}")
        
        return {
            'total_steps': total_steps,
            'curriculum_stats': training_stats,
            'final_eval': self.evaluate(num_episodes=100)
        }
```

### 3. Branches Layer - åˆ†æ”¯è®­ç»ƒå±‚

#### HierarchicalTrainer - åˆ†å±‚ç³»ç»Ÿè®­ç»ƒå™¨
```python
class HierarchicalTrainer(BaseTrainer):
    """HA-UAVå®Œæ•´åˆ†å±‚ç³»ç»Ÿè®­ç»ƒ"""
    
    def __init__(self, config: Dict, foundation_model_path: Optional[Path]):
        super().__init__(TrainingStage.HIERARCHICAL, config)
        self.foundation_model_path = foundation_model_path
        self.ha_components = None
        
    def setup(self) -> bool:
        """è®¾ç½®HAUAVAviaryå’ŒHAç»„ä»¶"""
        # åˆå§‹åŒ–ä¼šè¯ç®¡ç†
        self.initialize_session(
            enable_trajectory=True,
            enable_tensorboard=True,
            enable_visualization=True
        )
        
        # åˆ›å»ºHAUAVAviaryç¯å¢ƒ
        self.env = self.env_factory.create_environment(
            TrainingStage.HIERARCHICAL,
            self.config
        )
        
        # åˆå§‹åŒ–HAç»„ä»¶ç®¡ç†å™¨
        from src.modules import HAComponentsManager, ModelConfiguration
        
        model_config = ModelConfiguration()
        self.ha_components = HAComponentsManager(model_config)
        
        # ç»„ä»¶åˆå§‹åŒ–
        success = self.ha_components.initialize_components(self.env)
        if not success:
            return False
        
        # åŸºåº§æ¨¡å‹æƒé‡è¿ç§»
        if self.foundation_model_path:
            self._transfer_foundation_weights()
        
        return True
    
    def _execute_training(self) -> Dict[str, Any]:
        """ä½¿ç”¨HAComponentsManagerçš„ç»Ÿä¸€è®­ç»ƒé€»è¾‘"""
        training_stats = []
        total_steps = 0
        best_reward = float('-inf')
        
        while total_steps < self.config['total_timesteps']:
            # ä½¿ç”¨HAç»„ä»¶çš„train_stepæ–¹æ³•
            step_stats = self.ha_components.train_step(self.env)
            training_stats.append(step_stats)
            total_steps = step_stats['total_steps']
            
            # æ›´æ–°æœ€ä½³å¥–åŠ±
            current_reward = step_stats.get('mean_reward', 0)
            if current_reward > best_reward:
                best_reward = current_reward
            
            # å®šæœŸè¯„ä¼°å’Œå›è°ƒ
            if total_steps % 10000 == 0:
                eval_stats = self.evaluate(num_episodes=10)
                self.on_evaluation_callback(eval_stats)
                
                self.logger.info(f"æ­¥éª¤ {total_steps}: è®­ç»ƒå¥–åŠ±={current_reward:.3f}, è¯„ä¼°å¥–åŠ±={eval_stats['mean_reward']:.3f}")
        
        return {
            'total_steps': total_steps,
            'best_reward': best_reward,
            'training_stats': training_stats,
            'final_eval': self.evaluate(num_episodes=100)
        }
    
    def _transfer_foundation_weights(self):
        """ä»åŸºåº§æ¨¡å‹è¿ç§»æƒé‡åˆ°åˆ†å±‚æ¶æ„"""
        try:
            # åŠ è½½åŸºåº§PPOæ¨¡å‹
            from stable_baselines3 import PPO
            foundation_model = PPO.load(str(self.foundation_model_path))
            foundation_weights = foundation_model.policy.state_dict()
            
            # è¿ç§»åˆ°åˆ†å±‚ç­–ç•¥çš„ä½å±‚ç½‘ç»œ
            if self.ha_components and self.ha_components.policy:
                hierarchical_weights = self.ha_components.policy.state_dict()
                
                # æƒé‡æ˜ å°„: åŸºåº§ç½‘ç»œ â†’ ä½å±‚æ§åˆ¶ç½‘ç»œ
                transfer_mapping = {
                    'mlp_extractor.policy_net.0.weight': 'low_level_actor.0.weight',
                    'mlp_extractor.policy_net.0.bias': 'low_level_actor.0.bias',
                    'mlp_extractor.value_net.0.weight': 'low_level_critic.0.weight',
                    'mlp_extractor.value_net.0.bias': 'low_level_critic.0.bias'
                }
                
                transferred_count = 0
                for foundation_key, hierarchical_key in transfer_mapping.items():
                    if (foundation_key in foundation_weights and 
                        hierarchical_key in hierarchical_weights):
                        
                        foundation_param = foundation_weights[foundation_key]
                        hierarchical_param = hierarchical_weights[hierarchical_key]
                        
                        if foundation_param.shape == hierarchical_param.shape:
                            hierarchical_weights[hierarchical_key] = foundation_param.clone()
                            transferred_count += 1
                
                # åŠ è½½è¿ç§»åçš„æƒé‡
                self.ha_components.policy.load_state_dict(hierarchical_weights)
                self.logger.info(f"âœ… æˆåŠŸè¿ç§» {transferred_count} ä¸ªæƒé‡å‚æ•°")
                
        except Exception as e:
            self.logger.error(f"æƒé‡è¿ç§»å¤±è´¥: {e}")
            raise
```

#### AblationTrainer - æ¶ˆèå®éªŒè®­ç»ƒå™¨
```python
class AblationTrainer(BaseTrainer):
    """Bç»„æ¶ˆèå®éªŒè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict, ablation_type: str, foundation_model_path: Optional[Path]):
        super().__init__(TrainingStage.ABLATION, config)
        self.ablation_type = ablation_type  # 'B1', 'B2', 'B3'
        self.foundation_model_path = foundation_model_path
        self.ablation_components = None
        
    def setup(self) -> bool:
        """è®¾ç½®æ¶ˆèå®éªŒç¯å¢ƒå’Œç»„ä»¶"""
        # åˆ›å»ºHAUAVAviaryç¯å¢ƒ (ä¸åˆ†å±‚è®­ç»ƒç›¸åŒç¯å¢ƒ)
        self.env = self.env_factory.create_environment(
            TrainingStage.ABLATION,
            self.config
        )
        
        # åˆå§‹åŒ–æ¶ˆèç»„ä»¶ç®¡ç†å™¨
        from src.modules import AblationComponentsManager, get_ablation_config
        
        ablation_config = get_ablation_config(self.ablation_type)
        self.ablation_components = AblationComponentsManager(ablation_config)
        
        # ç»„ä»¶åˆå§‹åŒ–
        success = self.ablation_components.initialize_components(self.env)
        if not success:
            return False
        
        # åŸºåº§æ¨¡å‹æƒé‡è¿ç§»
        if self.foundation_model_path:
            self._transfer_foundation_weights()
        
        return True
    
    def _execute_training(self) -> Dict[str, Any]:
        """ä½¿ç”¨AblationComponentsManagerçš„ç»Ÿä¸€è®­ç»ƒé€»è¾‘"""
        training_stats = []
        total_steps = 0
        
        while total_steps < self.config['total_timesteps']:
            # ä½¿ç”¨æ¶ˆèç»„ä»¶çš„train_stepæ–¹æ³• (æ¥å£ä¸HAç³»ç»Ÿä¸€è‡´)
            step_stats = self.ablation_components.train_step(self.env)
            training_stats.append(step_stats)
            total_steps = step_stats['total_steps']
            
            # å®šæœŸè¯„ä¼°
            if total_steps % 10000 == 0:
                eval_stats = self.evaluate(num_episodes=10)
                self.logger.info(f"æ¶ˆè{self.ablation_type} æ­¥éª¤{total_steps}: {eval_stats}")
        
        return {
            'ablation_type': self.ablation_type,
            'total_steps': total_steps,
            'training_stats': training_stats,
            'final_eval': self.evaluate(num_episodes=100)
        }
```

#### BaselineTrainer - åŸºçº¿å¯¹æ¯”è®­ç»ƒå™¨
```python
class BaselineTrainer(BaseTrainer):
    """SB3åŸºçº¿ç®—æ³•è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict, algorithm: str, foundation_model_path: Optional[Path]):
        super().__init__(TrainingStage.BASELINE, config)
        self.algorithm = algorithm  # 'ppo', 'sac', 'td3'
        self.foundation_model_path = foundation_model_path
        self.model = None
        
    def setup(self) -> bool:
        """è®¾ç½®åŸºçº¿ç¯å¢ƒå’ŒSB3æ¨¡å‹"""
        # åˆ›å»ºåŒ…è£…åçš„ç¯å¢ƒ (SB3å…¼å®¹)
        base_env = self.env_factory.create_environment(
            TrainingStage.BASELINE,
            self.config
        )
        
        # ç¡®ä¿ç¯å¢ƒè¢«æ­£ç¡®åŒ…è£…ä¸ºBaselineWrapper
        if not isinstance(base_env, BaselineWrapper):
            self.env = BaselineWrapper(base_env)
        else:
            self.env = base_env
        
        # åˆ›å»ºSB3æ¨¡å‹
        self.model = self._create_sb3_model()
        
        # åŸºåº§æ¨¡å‹è¿ç§» (å¦‚æœå¯èƒ½)
        if self.foundation_model_path:
            self._transfer_foundation_weights()
        
        return True
    
    def _create_sb3_model(self):
        """æ ¹æ®ç®—æ³•ç±»å‹åˆ›å»ºSB3æ¨¡å‹"""
        if self.algorithm == 'ppo':
            from stable_baselines3 import PPO
            return PPO(
                'MlpPolicy', 
                self.env,
                learning_rate=self.config.get('learning_rate', 3e-4),
                batch_size=self.config.get('batch_size', 64),
                verbose=1
            )
        elif self.algorithm == 'sac':
            from stable_baselines3 import SAC
            return SAC(
                'MlpPolicy',
                self.env,
                learning_rate=self.config.get('learning_rate', 3e-4),
                buffer_size=self.config.get('buffer_size', 100000),
                verbose=1
            )
        elif self.algorithm == 'td3':
            from stable_baselines3 import TD3
            return TD3(
                'MlpPolicy',
                self.env,
                learning_rate=self.config.get('learning_rate', 1e-3),
                buffer_size=self.config.get('buffer_size', 100000),
                verbose=1
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åŸºçº¿ç®—æ³•: {self.algorithm}")
    
    def _execute_training(self) -> Dict[str, Any]:
        """ä½¿ç”¨SB3çš„æ ‡å‡†è®­ç»ƒæµç¨‹"""
        # æ·»åŠ è¿›åº¦å›è°ƒ
        callback = BaselineProgressCallback(
            progress_callbacks=self.progress_callbacks,
            algorithm=self.algorithm
        )
        
        # SB3æ ‡å‡†è®­ç»ƒ
        self.model.learn(
            total_timesteps=self.config['total_timesteps'],
            callback=callback
        )
        
        return {
            'algorithm': self.algorithm,
            'total_steps': self.config['total_timesteps'],
            'final_eval': self.evaluate(num_episodes=100)
        }
```

### 4. Orchestration Layer - ç¼–æ’å±‚

#### TrainingOrchestrator - è®­ç»ƒç¼–æ’å™¨
```python
class TrainingOrchestrator:
    """ç»Ÿä¸€è®­ç»ƒç¼–æ’å™¨ - é«˜çº§æ¥å£"""
    
    def __init__(self, config: OrchestrationConfig):
        self.config = config
        self.pipeline = TrainingPipeline(
            config=self._create_pipeline_config(),
            environment_factory=EnvironmentFactory(),
            model_transfer_manager=ModelTransferManager()
        )
        
    def run_complete_training(self) -> OrchestrationResult:
        """è¿è¡Œå®Œæ•´çš„å››é˜¶æ®µè®­ç»ƒæµç¨‹"""
        start_time = time.time()
        
        try:
            # æ‰§è¡Œæµæ°´çº¿è®­ç»ƒ
            pipeline_results = self.pipeline.run_sequential_training()
            
            # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
            comparison_report = self._generate_comparison_report(pipeline_results)
            
            # ç”Ÿæˆæ¶ˆèåˆ†ææŠ¥å‘Š
            ablation_report = self._generate_ablation_analysis(pipeline_results)
            
            total_duration = time.time() - start_time
            
            return OrchestrationResult(
                success=True,
                total_duration=total_duration,
                pipeline_results=pipeline_results,
                comparison_report=comparison_report,
                ablation_analysis=ablation_report
            )
            
        except Exception as e:
            return OrchestrationResult(
                success=False,
                total_duration=time.time() - start_time,
                error_message=str(e)
            )
    
    def _generate_comparison_report(self, results: Dict[str, TrainingResult]) -> str:
        """ç”Ÿæˆè®­ç»ƒç»“æœå¯¹æ¯”æŠ¥å‘Š"""
        report = "# HA-UAVè®­ç»ƒç»“æœå¯¹æ¯”æŠ¥å‘Š\\n\\n"
        
        # æ€§èƒ½æ’å
        sorted_results = sorted(
            [(k, v) for k, v in results.items() if v.success],
            key=lambda x: x[1].final_reward,
            reverse=True
        )
        
        report += "## æ€§èƒ½æ’å\\n"
        for rank, (stage, result) in enumerate(sorted_results, 1):
            report += f"{rank}. {stage}: {result.final_reward:.3f}\\n"
        
        # è¯¦ç»†ç»Ÿè®¡
        report += "\\n## è¯¦ç»†ç»Ÿè®¡\\n"
        for stage, result in results.items():
            if result.success:
                report += f"### {stage}\\n"
                report += f"- æœ€ç»ˆå¥–åŠ±: {result.final_reward:.3f}\\n"
                report += f"- æœ€ä½³å¥–åŠ±: {result.best_reward:.3f}\\n"
                report += f"- è®­ç»ƒæ­¥æ•°: {result.total_steps}\\n"
                report += f"- è®­ç»ƒæ—¶é•¿: {result.training_time:.1f}s\\n\\n"
        
        return report
    
    def _generate_ablation_analysis(self, results: Dict[str, TrainingResult]) -> str:
        """ç”Ÿæˆæ¶ˆèå®éªŒåˆ†ææŠ¥å‘Š"""
        ablation_results = {k: v for k, v in results.items() if 'ablation' in k}
        hierarchical_result = results.get('hierarchical')
        
        if not hierarchical_result or not ablation_results:
            return "æ¶ˆèå®éªŒæ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•ç”Ÿæˆåˆ†ææŠ¥å‘Šã€‚"
        
        analysis = "# æ¶ˆèå®éªŒåˆ†ææŠ¥å‘Š\\n\\n"
        
        # åˆ†å±‚æ¶æ„æœ‰æ•ˆæ€§éªŒè¯
        analysis += "## åˆ†å±‚æ¶æ„æœ‰æ•ˆæ€§éªŒè¯\\n"
        hierarchical_reward = hierarchical_result.final_reward
        
        for ablation_name, ablation_result in ablation_results.items():
            if ablation_result.success:
                improvement = hierarchical_reward - ablation_result.final_reward
                improvement_pct = (improvement / abs(ablation_result.final_reward)) * 100
                
                analysis += f"- {ablation_name}: "
                analysis += f"åˆ†å±‚æ¶æ„æå‡ {improvement:.3f} ({improvement_pct:+.1f}%)\\n"
        
        return analysis
```

## ğŸ”„ æ•°æ®æµæ¶æ„

### è®­ç»ƒæ•°æ®æµ
```
ç¯å¢ƒè§‚æµ‹(86ç»´) â†’ StateManager â†’ åˆ†å±‚è§‚æµ‹å¤„ç† â†’ ç­–ç•¥ç½‘ç»œ â†’ åŠ¨ä½œ(4ç»´)
      â†“                                                         â†“
   å†å²çŠ¶æ€ â† HierarchicalRolloutBuffer â† ç»éªŒæ”¶é›† â† ç¯å¢ƒåé¦ˆ
      â†“                                                         â†“  
   GAEè®¡ç®— â†’ ä¼˜åŠ¿ä¼°è®¡ â†’ PPOæŸå¤±è®¡ç®— â†’ æ¢¯åº¦æ›´æ–° â†’ ç­–ç•¥ä¼˜åŒ–
```

### æ¨¡å‹è¿ç§»æ•°æ®æµ
```
BaseFlightAviaryè®­ç»ƒ â†’ PPOæ¨¡å‹æƒé‡(.zip) â†’ æƒé‡æå–å™¨
                                           â†“
ModelTransferManager â† æƒé‡æ˜ å°„ â† æ¶æ„é€‚é…å™¨ â† æƒé‡è½¬æ¢å™¨
                                           â†“
åˆ†å±‚ç­–ç•¥ç½‘ç»œ â† ä½å±‚åˆå§‹åŒ– â† é«˜å±‚éšæœºåˆå§‹åŒ– â† è¿ç§»æƒé‡
```

### ä¼šè¯ç®¡ç†æ•°æ®æµ
```
è®­ç»ƒå™¨åˆå§‹åŒ– â†’ SessionManager â†’ ç›®å½•åˆ›å»º â†’ é…ç½®ä¿å­˜
      â†“                           â†“
è®­ç»ƒè¿›åº¦ â†’ VisualizationManager â†’ å®æ—¶æ˜¾ç¤º â†’ TensorBoard
      â†“                           â†“
è®­ç»ƒç»“æœ â†’ TrajectoryManager â†’ è½¨è¿¹è®°å½• â†’ æ€§èƒ½åˆ†æ
```

## ğŸš€ ä½¿ç”¨æŒ‡å—

### 1. å¿«é€Ÿå¼€å§‹ - å®Œæ•´è®­ç»ƒæµç¨‹
```python
# ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œå®Œæ•´è®­ç»ƒ
from src.training import TrainingOrchestrator, create_default_config

config = create_default_config("my_hauav_experiment")
orchestrator = TrainingOrchestrator(config)
result = orchestrator.run_complete_training()

print(f"è®­ç»ƒå®Œæˆ: {result.success}")
print(f"æ€§èƒ½æŠ¥å‘Š: {result.comparison_report}")
```

### 2. å‘½ä»¤è¡Œå¿«é€Ÿè®­ç»ƒ
```bash
# å•é˜¶æ®µè®­ç»ƒ
python start_training.py --stage hierarchical --timesteps 10000

# æ¶ˆèå®éªŒ
python start_training.py --stage ablation --ablation-type B1 --timesteps 5000

# åŸºçº¿å¯¹æ¯”  
python start_training.py --stage baseline --algorithm ppo --timesteps 5000

# å®Œæ•´æµæ°´çº¿
python start_training.py --pipeline --timesteps 20000
```

### 3. è‡ªå®šä¹‰é…ç½®è®­ç»ƒ
```python
from src.training import HierarchicalTrainer, EnvironmentFactory

# è‡ªå®šä¹‰åˆ†å±‚è®­ç»ƒ
config = {
    'total_timesteps': 50000,
    'high_level_update_frequency': 5,
    'future_horizon': 10,
    'learning_rate': 1e-4,
    'batch_size': 128,
    'enable_visualization': True
}

trainer = HierarchicalTrainer(
    config=config,
    foundation_model_path=Path("./models/foundation_model.zip")
)

result = trainer.train()
print(f"åˆ†å±‚è®­ç»ƒå®Œæˆ: {result.final_reward:.3f}")
```

### 4. æµæ°´çº¿è‡ªå®šä¹‰
```python
from src.training import TrainingPipeline, PipelineConfig

# è‡ªå®šä¹‰æµæ°´çº¿é…ç½®
pipeline_config = PipelineConfig(
    experiment_name="custom_experiment",
    foundation_config={
        'total_timesteps': 30000,
        'enable_curriculum': True
    },
    branches_config={
        'hierarchical': {'total_timesteps': 100000},
        'ablation': {
            'total_timesteps': 50000,
            'ablation_types': ['B1', 'B2']  # åªæµ‹è¯•B1å’ŒB2
        },
        'baseline': {
            'total_timesteps': 75000,
            'algorithms': ['ppo']  # åªæµ‹è¯•PPO
        }
    }
)

pipeline = TrainingPipeline(pipeline_config)
results = pipeline.run_sequential_training()

for stage, result in results.items():
    print(f"{stage}: {result.final_reward:.3f}")
```

## ğŸ” å…³é”®å®ç°ç»†èŠ‚

### ç»Ÿä¸€train_stepæ¥å£
æ‰€æœ‰ç»„ä»¶ç®¡ç†å™¨(HAComponentsManager, AblationComponentsManager)éƒ½å®ç°ç›¸åŒçš„train_stepæ¥å£:
```python
def train_step(self, env) -> Dict[str, float]:
    """ç»Ÿä¸€çš„è®­ç»ƒæ­¥éª¤æ¥å£"""
    return {
        'total_steps': int,
        'episodes': int, 
        'mean_reward': float,
        'policy_loss': float,
        'value_loss': float,
        'total_loss': float
    }
```

### æ™ºèƒ½ä¼šè¯ç®¡ç†
```python
# è‡ªåŠ¨åˆ›å»ºé˜¶æ®µç‰¹å®šç›®å½•ç»“æ„
SessionManagerè‡ªåŠ¨åˆ›å»º:
logs/
â”œâ”€â”€ train_hierarchical_20250821_120000/
â”‚   â”œâ”€â”€ Config/           # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ Model/            # æ¨¡å‹ä¿å­˜
â”‚   â”œâ”€â”€ Result/           # è®­ç»ƒç»“æœ
â”‚   â”œâ”€â”€ Tensorboard/      # TensorBoardæ—¥å¿—
â”‚   â””â”€â”€ Visualization/    # å¯è§†åŒ–æ•°æ®
```

### æ¨¡å‹æƒé‡è¿ç§»ç­–ç•¥
```python
# åŸºåº§æ¨¡å‹ â†’ åˆ†å±‚æ¶æ„æƒé‡æ˜ å°„
transfer_mapping = {
    'mlp_extractor.policy_net.0.weight': 'low_level_actor.0.weight',
    'mlp_extractor.policy_net.0.bias': 'low_level_actor.0.bias',
    # é«˜å±‚ç½‘ç»œæƒé‡ä¿æŒéšæœºåˆå§‹åŒ–ï¼Œå­¦ä¹ ä»»åŠ¡ç‰¹å®šç­–ç•¥
}
```

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### å®æ—¶è®­ç»ƒç›‘æ§
```python
# VisualizationManageræä¾›å®æ—¶è¿›åº¦æ˜¾ç¤º
ğŸš HA-UAVåˆ†å±‚è®­ç»ƒ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% [01:23<00:00]
   æ­¥éª¤: 50000/50000, å¥–åŠ±: 15.67, æŸå¤±: 0.023
```

### è‡ªåŠ¨æ€§èƒ½å¯¹æ¯”
```python
# TrainingOrchestratorè‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
æ€§èƒ½æ’å:
1. hierarchical: 18.24
2. ablation_B3: 16.89
3. baseline_ppo: 15.47
4. ablation_B1: 14.23
5. ablation_B2: 13.56

æ¶ˆèåˆ†æ:
- B1æ¶ˆè: åˆ†å±‚æ¶æ„æå‡ 4.01 (+28.2%)
- B2æ¶ˆè: åˆ†å±‚æ¶æ„æå‡ 4.68 (+34.5%)  
- B3æ¶ˆè: åˆ†å±‚æ¶æ„æå‡ 1.35 (+8.0%)
```

## ğŸ“ˆ ç³»ç»Ÿä¼˜åŠ¿æ€»ç»“

### ğŸ¯ **æ¶æ„ä¼˜åŠ¿**
- **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„å››å±‚æ¶æ„ï¼ŒèŒè´£åˆ†ç¦»
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰è®­ç»ƒå™¨å…±äº«BaseTraineræŠ½è±¡
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ–°ç®—æ³•å’Œç¯å¢ƒçš„è½»æ¾é›†æˆ
- **å¤ç”¨æ€§**: åŸºåº§æ¨¡å‹æƒé‡åœ¨æ‰€æœ‰åˆ†æ”¯é—´å…±äº«

### ğŸ”§ **æŠ€æœ¯ä¼˜åŠ¿**  
- **æ™ºèƒ½è¿ç§»**: è‡ªåŠ¨åŒ–çš„æ¨¡å‹æƒé‡è¿ç§»æœºåˆ¶
- **ç»Ÿä¸€è®­ç»ƒ**: æ‰€æœ‰åˆ†æ”¯ä½¿ç”¨ä¸€è‡´çš„train_stepé€»è¾‘
- **æ ‡å‡†åŒ–è¯„ä¼°**: ç»Ÿä¸€çš„TrainingResultæ•°æ®æ ¼å¼
- **æµæ°´çº¿åŒ–**: ä»åŸºåº§åˆ°åˆ†æ”¯çš„è‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹

### ğŸ“Š **å®ç”¨ä¼˜åŠ¿**
- **ä¸€é”®è®­ç»ƒ**: æ”¯æŒå®Œæ•´æµç¨‹çš„å‘½ä»¤è¡Œå¯åŠ¨
- **çµæ´»é…ç½®**: ä¸°å¯Œçš„é…ç½®é€‰é¡¹é€‚åº”å„ç§å®éªŒéœ€æ±‚  
- **å®æ—¶ç›‘æ§**: å®Œå–„çš„è¿›åº¦æ˜¾ç¤ºå’Œæ€§èƒ½å¯è§†åŒ–
- **è‡ªåŠ¨åˆ†æ**: æ™ºèƒ½ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å’Œæ¶ˆèåˆ†ææŠ¥å‘Š

**ğŸš€ HA-UAVè®­ç»ƒç³»ç»Ÿå·²å‡†å¤‡å¥½æ”¯æŒå®Œæ•´çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç ”ç©¶å’Œåº”ç”¨ï¼**

