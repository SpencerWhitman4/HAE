#!/usr/bin/env python3

"""
åŸºåº§æ¨¡å‹è®­ç»ƒå™¨ - åŸºäºBaseFlightAviaryè®­ç»ƒæ‚¬åœ+é£è¡ŒåŸºç¡€æ¨¡å‹
"""

import logging
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from ..core.base_trainer import BaseTrainer, TrainingStage, TrainingResult
from ..core.environment_factory import EnvironmentFactory

# å¤ç”¨ç°æœ‰ç»„ä»¶
from src.envs.BaseFlightAviary import BaseFlightAviary, BaseFlightConfig
from src.utils.Logger import Logger
from src.utils.utils import sync

logger = logging.getLogger(__name__)


class BaseFlightModel(nn.Module):
    """
    åŸºåº§é£è¡Œæ¨¡å‹ - å­¦ä¹ æ‚¬åœå’ŒåŸºç¡€é£è¡Œæ§åˆ¶
    
    è¾“å…¥: 86ç»´æ¿€å…‰é›·è¾¾è§‚æµ‹
    è¾“å‡º: 4ç»´æ§åˆ¶æŒ‡ä»¤ [thrust, roll, pitch, yaw_rate]
    """
    
    def __init__(self, 
                 obs_dim: int = 86,
                 action_dim: int = 4,
                 hidden_dims: List[int] = [256, 256, 128],
                 activation: str = "relu"):
        
        super(BaseFlightModel, self).__init__()
        
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        
        # ç‰¹å¾æå–å™¨
        layers = []
        in_dim = obs_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.ReLU() if activation == "relu" else nn.Tanh(),
                nn.Dropout(0.1)
            ])
            in_dim = hidden_dim
        
        self.encoder = nn.Sequential(*layers)
        
        # Actorç½‘ç»œ - æ§åˆ¶ç­–ç•¥
        self.actor = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1, 1]
        )
        
        # Criticç½‘ç»œ - ä»·å€¼ä¼°è®¡  
        self.critic = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        # å‚æ•°åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """å‚æ•°åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=0.5)
            nn.init.constant_(module.bias, 0.0)
    
    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            obs: è§‚æµ‹å¼ é‡ [batch_size, obs_dim]
            
        Returns:
            (action, value): åŠ¨ä½œå’Œä»·å€¼ä¼°è®¡
        """
        features = self.encoder(obs)
        action = self.actor(features)
        value = self.critic(features)
        
        return action, value
    
    def get_action(self, obs: torch.Tensor, deterministic: bool = False) -> torch.Tensor:
        """è·å–åŠ¨ä½œ"""
        with torch.no_grad():
            action, _ = self.forward(obs)
            
            if not deterministic:
                # æ·»åŠ æ¢ç´¢å™ªå£°
                noise = torch.normal(0, 0.1, size=action.shape)
                action = action + noise
                action = torch.clamp(action, -1.0, 1.0)
            
            return action
    
    def get_value(self, obs: torch.Tensor) -> torch.Tensor:
        """è·å–ä»·å€¼ä¼°è®¡"""
        with torch.no_grad():
            features = self.encoder(obs)
            value = self.critic(features)
            return value


class BaseFlightTrainer(BaseTrainer):
    """
    åŸºåº§æ¨¡å‹è®­ç»ƒå™¨
    
    è®­ç»ƒBaseFlightAviaryç¯å¢ƒä¸­çš„æ‚¬åœ+åŸºç¡€é£è¡Œæ§åˆ¶ç­–ç•¥
    ä½œä¸ºåç»­åˆ†å±‚è®­ç»ƒçš„ç»Ÿä¸€åŸºåº§
    """
    
    def __init__(self, 
                 config: Dict[str, Any],
                 foundation_model_path: Optional[Path] = None,
                 output_dir: str = "./models"):
        
        super().__init__(
            stage=TrainingStage.FOUNDATION,
            config=config,
            experiment_name="BaseFlightModel",
            stage_variant=None
        )
        
        self.foundation_model_path = foundation_model_path
        self.output_dir = output_dir
        self.env_factory = EnvironmentFactory()
        
        # è®­ç»ƒé…ç½®
        self.total_timesteps = config.get('total_timesteps', 100000)
        self.batch_size = config.get('batch_size', 256)
        self.learning_rate = config.get('learning_rate', 3e-4)
        self.gamma = config.get('gamma', 0.99)
        self.gae_lambda = config.get('gae_lambda', 0.95)
        self.clip_range = config.get('clip_range', 0.2)
        self.max_grad_norm = config.get('max_grad_norm', 0.5)
        
        # è¯¾ç¨‹å­¦ä¹ é…ç½®
        self.hover_training_steps = config.get('hover_training_steps', 25000)
        self.flight_training_steps = config.get('flight_training_steps', 75000)
        self.enable_curriculum = config.get('enable_curriculum', True)
        
        # æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶
        self.model = None
        self.optimizer = None
        self.env = None
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'hover_success_rate': [],
            'flight_success_rate': [],
            'value_losses': [],
            'policy_losses': []
        }
        
        # Episodeè®¡æ•°å’Œè½¨è¿¹è®°å½•
        self.episode_count = 0
        self.current_episode_reward = 0.0
        self.current_episode_length = 0
        self.trajectory_episode_started = False
        
        # å½“å‰è®­ç»ƒé˜¶æ®µ
        self.current_phase = "hover"  # "hover" or "flight"
        self.phase_start_step = 0
        
        # åˆå§‹åŒ–progress_callbacks
        self.progress_callbacks = []
    
    def setup(self) -> bool:
        """è®¾ç½®è®­ç»ƒå™¨"""
        try:
            # åˆå§‹åŒ–ä¼šè¯ç®¡ç†ï¼ˆåŒ…å«å¯è§†åŒ–ï¼‰
            session_info = self.initialize_session(
                enable_trajectory=self.config.get('enable_trajectory', True),
                enable_tensorboard=self.config.get('enable_tensorboard', True),
                enable_visualization=self.config.get('enable_visualization', True),
                enable_rich_display=self.config.get('enable_rich_display', True)
            )
            
            self.logger.info(f"ä¼šè¯åˆå§‹åŒ–å®Œæˆ: {session_info['session_dir']}")
            
            # åˆå§‹åŒ–TensorBoard Writer
            self.tensorboard_writer = None
            if self.session_manager and self.session_manager.feature_flags.get('tensorboard', False):
                try:
                    from torch.utils.tensorboard import SummaryWriter
                    tensorboard_path = self.session_manager.data_managers['tensorboard']['train']
                    self.tensorboard_writer = SummaryWriter(log_dir=str(tensorboard_path))
                    self.logger.info(f"âœ… TensorBoard Writer åˆå§‹åŒ–å®Œæˆ: {tensorboard_path}")
                except ImportError:
                    self.logger.warning("TensorBoardä¸å¯ç”¨ï¼Œè·³è¿‡TensorBoardè®°å½•")
                except Exception as e:
                    self.logger.warning(f"TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆå§‹åŒ–è½¨è¿¹ç®¡ç†å™¨
            self.trajectory_manager = None
            if self.session_manager and self.session_manager.feature_flags.get('trajectory', False):
                try:
                    trajectory_managers = self.session_manager.data_managers.get('trajectory', {})
                    self.trajectory_manager = trajectory_managers.get('train')
                    if self.trajectory_manager:
                        self.logger.info("âœ… è½¨è¿¹ç®¡ç†å™¨ åˆå§‹åŒ–å®Œæˆ")
                except Exception as e:
                    self.logger.warning(f"è½¨è¿¹ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # åˆ›å»ºç¯å¢ƒ
            env_config = {
                'drone_model': self.config.get('drone_model', 'CF2X'),
                'physics': self.config.get('physics', 'PYB'),
                'gui_training': self.config.get('gui_training', False),
                'max_episode_steps': self.config.get('max_episode_steps', 1000),
                'hover_training_steps': self.hover_training_steps,
                'flight_training_steps': self.flight_training_steps,
                'enable_curriculum': self.enable_curriculum
            }
            
            self.env = self.env_factory.create_environment(
                stage=self.stage,
                config=env_config,
                mode="train"
            )
            
            # åˆ›å»ºæ¨¡å‹
            self.model = BaseFlightModel(
                obs_dim=86,
                action_dim=4,
                hidden_dims=self.config.get('hidden_dims', [256, 256, 128])
            )
            
            # åˆ›å»ºä¼˜åŒ–å™¨
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.learning_rate,
                eps=1e-5
            )
            
            # ğŸ”„ å°è¯•åŠ è½½ä¹‹å‰çš„æ¨¡å‹è¿›è¡Œç»­è®­
            self._try_resume_training()
            
            self.write_stage_message("åŸºåº§æ¨¡å‹è®­ç»ƒå™¨è®¾ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå™¨è®¾ç½®å¤±è´¥: {e}")
            if self.visualization_manager:
                self.visualization_manager.write_message(f"è®¾ç½®å¤±è´¥: {e}", "ERROR")
            return False
    
    def _execute_training(self) -> Dict[str, Any]:
        """æ‰§è¡ŒåŸºåº§æ¨¡å‹è®­ç»ƒé€»è¾‘ - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿å›è°ƒæ­£å¸¸å·¥ä½œ"""
        evaluation_frequency = self.config.get('evaluation_frequency', 10000)
        checkpoint_frequency = self.config.get('checkpoint_frequency', 50000)
        save_frequency = self.config.get('save_frequency', 10000)  # æ·»åŠ å®šæœŸä¿å­˜é¢‘ç‡
        
        # è°ƒè¯•æ¨¡å¼ä¸‹æ›´é¢‘ç¹çš„ä¿å­˜å’Œè¯„ä¼°
        if self.config.get('debug', False):
            checkpoint_frequency = min(checkpoint_frequency, 500)  # è°ƒè¯•æ¨¡å¼ä¸‹æ¯500æ­¥ä¿å­˜æ£€æŸ¥ç‚¹
            evaluation_frequency = min(evaluation_frequency, 500)  # è°ƒè¯•æ¨¡å¼ä¸‹æ¯500æ­¥è¯„ä¼°
            save_frequency = min(save_frequency, 200)  # è°ƒè¯•æ¨¡å¼ä¸‹æ¯200æ­¥ä¿å­˜æ¨¡å‹
        
        # ğŸ¯ ä¼˜åŒ–å­¦ä¹ å‚æ•°ï¼šä½¿ç”¨é…ç½®ä¸­çš„buffer_size
        # ä»é…ç½®ä¸­è·å–buffer_sizeï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨n_stepsæˆ–é»˜è®¤å€¼
        buffer_size = self.config.get('buffer_size', self.config.get('n_steps', 2048))
        effective_batch_size = min(self.batch_size, 64)  # é™åˆ¶æ‰¹æ¬¡å¤§å°ä¸º64ï¼Œæé«˜è®­ç»ƒé¢‘ç‡
        learn_interval = min(buffer_size, max(effective_batch_size, buffer_size))  # ä½¿ç”¨é…ç½®çš„buffer_sizeä½œä¸ºå­¦ä¹ é—´éš”
        
        training_stats = []
        start_time = time.time()
        self.logger.info(f"å¼€å§‹åŸºåº§æ¨¡å‹è®­ç»ƒ: {self.total_timesteps:,} æ­¥ (å­¦ä¹ é—´éš”: {learn_interval} æ­¥)")
        self.logger.info(f"ğŸ“Š æ£€æŸ¥ç‚¹é¢‘ç‡: {checkpoint_frequency}, è¯„ä¼°é¢‘ç‡: {evaluation_frequency}, ä¿å­˜é¢‘ç‡: {save_frequency}")
        
        # åˆå§‹åŒ–è®­ç»ƒå¾ªç¯
        obs, _ = self.env.reset()
        episode_reward = 0
        episode_length = 0
        episode_count = 0
        rollout_buffer = []
        
        # ğŸ¬ å¯åŠ¨ç¬¬ä¸€ä¸ªepisodeçš„è½¨è¿¹è®°å½•
        self._start_trajectory_episode(episode_count)
        
        # ğŸ”§ æ£€æŸ¥å¯è§†åŒ–ç®¡ç†å™¨çŠ¶æ€
        if self.visualization_manager:
            self.logger.info(f"âœ… å¯è§†åŒ–ç®¡ç†å™¨å·²å¯ç”¨: {type(self.visualization_manager).__name__}")
        else:
            self.logger.warning("âŒ å¯è§†åŒ–ç®¡ç†å™¨æœªå¯ç”¨ï¼Œè¿›åº¦æ¡å°†ä¸æ˜¾ç¤º")
        
        # ğŸ”§ ç«‹å³è°ƒç”¨ç¬¬ä¸€æ¬¡å›è°ƒï¼Œç¡®ä¿è¿›åº¦æ¡å¯åŠ¨
        if self.visualization_manager:
            initial_metrics = {
                'episode': 0,
                'total_reward': 0.0,
                'exploration_rate': 1.0,
                'policy_loss': 0.0,
                'value_loss': 0.0,
                'phase': self.current_phase
            }
            self.visualization_manager.on_step(0, initial_metrics)
            self.logger.info("âœ… åŸºåº§æ¨¡å‹åˆå§‹å›è°ƒå·²è§¦å‘ï¼Œè¿›åº¦æ¡åº”å·²å¯åŠ¨")
        
        while self.current_step < self.total_timesteps:
            try:
                # ğŸ”§ åˆç†çš„æ‰¹æ¬¡æ”¶é›†ï¼šç¡®ä¿æœ‰è¶³å¤Ÿç»éªŒè¿›è¡Œè®­ç»ƒ
                steps_to_collect = min(learn_interval, self.total_timesteps - self.current_step)
                
                self.logger.info(f"å¼€å§‹æ”¶é›†ç»éªŒ: {steps_to_collect} æ­¥ (å½“å‰æ­¥æ•°: {self.current_step})")
                
                # é™æ—¶æ”¶é›†ç»éªŒï¼Œé¿å…æ— é™å¡ä½
                collect_start = time.time()
                steps_collected = 0
                max_collect_time = 60.0  # æœ€å¤š60ç§’æ”¶é›†æ—¶é—´
                
                try:
                    # è¯¾ç¨‹å­¦ä¹ é˜¶æ®µåˆ‡æ¢
                    if self.enable_curriculum:
                        self._update_curriculum_phase(self.current_step)
                    
                    # æ”¶é›†ç»éªŒ - æ·»åŠ è¶…æ—¶ä¿æŠ¤
                    for step_idx in range(steps_to_collect):
                        # è¶…æ—¶æ£€æŸ¥
                        if time.time() - collect_start > max_collect_time:
                            self.logger.warning(f"ç»éªŒæ”¶é›†è¶…æ—¶ ({max_collect_time}s)ï¼Œå·²æ”¶é›† {steps_collected}/{steps_to_collect} æ­¥")
                            break
                        # æ”¶é›†ç»éªŒ
                        with torch.no_grad():
                            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                            action, value = self.model(obs_tensor)
                            action_np = action.squeeze(0).cpu().numpy()
                        
                        # ç¯å¢ƒäº¤äº’
                        next_obs, reward, terminated, truncated, info = self.env.step(action_np)
                        done = terminated or truncated
                        
                        # ğŸ¬ è®°å½•è½¨è¿¹æ•°æ®
                        self._log_trajectory_step(obs, action_np, reward, next_obs, done, info)
                        
                        # å­˜å‚¨ç»éªŒ
                        rollout_buffer.append({
                            'obs': obs.copy(),
                            'action': action_np.copy(),
                            'reward': reward,
                            'value': value.item(),
                            'done': done
                        })
                        
                        # æ›´æ–°çŠ¶æ€
                        obs = next_obs
                        episode_reward += reward
                        episode_length += 1
                        steps_collected += 1
                        
                        # æ¯éš”ä¸€å®šæ­¥æ•°æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…é¢‘ç¹æ›´æ–°
                        if step_idx % 100 == 0:
                            temp_metrics = {
                                'episode': episode_count,
                                'total_reward': episode_reward,
                                'exploration_rate': max(0.0, 1.0 - (self.current_step + steps_collected) / self.total_timesteps),
                                'phase': self.current_phase
                            }
                            if self.visualization_manager:
                                self.visualization_manager.on_step(self.current_step + steps_collected, temp_metrics)
                        
                        # å¤„ç†å›åˆç»“æŸ
                        if done:
                            # ğŸ¬ ç»“æŸè½¨è¿¹è®°å½•
                            self._finalize_trajectory_episode(episode_reward, episode_length, info)
                            self._handle_episode_end(episode_reward, episode_length, info)
                            
                            # é‡ç½®ç¯å¢ƒå¹¶å¼€å§‹æ–°episode
                            obs, _ = self.env.reset()
                            episode_count += 1
                            episode_reward = 0
                            episode_length = 0
                            
                            # ğŸ¬ å¯åŠ¨æ–°episodeçš„è½¨è¿¹è®°å½•
                            self._start_trajectory_episode(episode_count)
                        
                        # è¶…æ—¶ä¿æŠ¤ - å¦‚æœå•æ­¥æ—¶é—´è¿‡é•¿ï¼Œè®°å½•å¹¶ç»§ç»­
                        if time.time() - collect_start > 10.0 and step_idx < 10:
                            self.logger.warning(f"ç»éªŒæ”¶é›†æ­¥éª¤ {step_idx} è€—æ—¶è¿‡é•¿ï¼Œå¯èƒ½å­˜åœ¨ç¯å¢ƒé—®é¢˜")
                    
                    collect_time = time.time() - collect_start
                    
                    if collect_time > 3.0:  # é™ä½è­¦å‘Šé˜ˆå€¼
                        self.logger.warning(f"ç»éªŒæ”¶é›†è€—æ—¶è¿‡é•¿: {collect_time:.1f}s")
                    
                    # åœ¨æ”¶é›†å®Œç»éªŒåè¿›è¡Œè®­ç»ƒ
                    if len(rollout_buffer) >= effective_batch_size:
                        self._train_step(rollout_buffer)
                        rollout_buffer = []
                    
                except Exception as e:
                    self.logger.error(f"ç»éªŒæ”¶é›†å¤±è´¥: {e}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°è¿›åº¦ï¼Œç¡®ä¿è®­ç»ƒç»§ç»­
                    if steps_collected == 0:
                        steps_collected = steps_to_collect  # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°æ­¥æ•°ï¼Œä½¿ç”¨é¢„æœŸå€¼
                
                # ğŸ¯ ç«‹å³æ›´æ–°æ­¥æ•°å’Œè§¦å‘å›è°ƒ
                self.current_step += steps_collected
                
                # ğŸ”§ æ ¸å¿ƒï¼šç«‹å³è§¦å‘å›è°ƒï¼Œç¡®ä¿è¿›åº¦æ¡æ›´æ–°
                step_metrics = {
                    'episode': episode_count,
                    'total_reward': episode_reward,
                    'exploration_rate': max(0.0, 1.0 - self.current_step / self.total_timesteps),
                    'policy_loss': np.mean(self.training_stats['policy_losses'][-10:]) if self.training_stats['policy_losses'] else 0.0,
                    'value_loss': np.mean(self.training_stats['value_losses'][-10:]) if self.training_stats['value_losses'] else 0.0,
                    'phase': self.current_phase
                }
                
                # ğŸ“Š TensorBoardè®°å½•
                self._log_to_tensorboard(step_metrics)
                
                # è°ƒç”¨å¯è§†åŒ–ç®¡ç†å™¨å›è°ƒ
                if self.visualization_manager:
                    self.visualization_manager.on_step(self.current_step, step_metrics)
                    self.logger.info(f"âœ… å›è°ƒå·²è§¦å‘: {self.current_step}/{self.total_timesteps} ({self.current_step/self.total_timesteps:.1%})")
                
                # è°ƒç”¨è®­ç»ƒå™¨å›è°ƒ
                self.on_step_callback(self.current_step, step_metrics)
                
                # Episodeå›è°ƒ
                if episode_count > 0 and episode_count != getattr(self, '_last_episode_count', 0):
                    self.on_episode_callback(episode_count, step_metrics)
                    self._last_episode_count = episode_count
                
                # å®šæœŸè¯„ä¼°ï¼ˆæ”¹ä¸ºèŒƒå›´è§¦å‘ï¼Œæ›´å¯é ï¼‰
                steps_since_last_eval = self.current_step - getattr(self, '_last_eval_step', 0)
                if steps_since_last_eval >= evaluation_frequency and self.current_step > 0:
                    self.logger.info(f"ğŸ¯ å¼€å§‹å®šæœŸè¯„ä¼° (æ­¥æ•°: {self.current_step}, è·ä¸Šæ¬¡è¯„ä¼°: {steps_since_last_eval} æ­¥)")
                    try:
                        eval_results = self._perform_evaluation()
                        self.on_evaluation_callback(eval_results)
                        self._last_eval_step = self.current_step  # è®°å½•æœ€åè¯„ä¼°æ­¥æ•°
                        self.logger.info(f"âœ… è¯„ä¼°å®Œæˆ - å¹³å‡å¥–åŠ±: {eval_results.get('mean_reward', 0):.2f}")
                    except Exception as e:
                        self.logger.warning(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
                elif self.current_step > 0 and steps_since_last_eval >= evaluation_frequency * 0.8:
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ¥è¿‘è¯„ä¼°ç‚¹æ—¶æç¤º
                    self.logger.debug(f"ğŸ”” æ¥è¿‘è¯„ä¼°ç‚¹: è¿˜éœ€ {evaluation_frequency - steps_since_last_eval} æ­¥è§¦å‘è¯„ä¼°")
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä¹Ÿä½¿ç”¨èŒƒå›´è§¦å‘ï¼‰
                steps_since_last_checkpoint = self.current_step - getattr(self, '_last_checkpoint_step', 0)
                if steps_since_last_checkpoint >= checkpoint_frequency and self.current_step > 0:
                    self.logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹ (æ­¥æ•°: {self.current_step}, è·ä¸Šæ¬¡ä¿å­˜: {steps_since_last_checkpoint} æ­¥)")
                    try:
                        self.on_checkpoint_callback(self.current_step)
                        self._last_checkpoint_step = self.current_step  # è®°å½•æœ€åæ£€æŸ¥ç‚¹æ­¥æ•°
                        self.logger.info(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜")
                    except Exception as e:
                        self.logger.warning(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                
                # å®šæœŸä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨èŒƒå›´è§¦å‘ï¼‰
                steps_since_last_save = self.current_step - getattr(self, '_last_save_step', 0)
                if steps_since_last_save >= save_frequency and self.current_step > 0:
                    self.logger.info(f"ğŸ’¾ å®šæœŸä¿å­˜æ¨¡å‹ (æ­¥æ•°: {self.current_step}, è·ä¸Šæ¬¡ä¿å­˜: {steps_since_last_save} æ­¥)")
                    try:
                        if self.session_manager:
                            save_path = self.session_manager.get_model_save_path(f"model_step_{self.current_step}.zip")
                            if self.save_model(save_path):
                                self._last_save_step = self.current_step  # è®°å½•æœ€åä¿å­˜æ­¥æ•°
                                if self.visualization_manager:
                                    self.visualization_manager.on_model_save("å®šæœŸ", path=str(save_path))
                                self.logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜: {save_path}")
                            else:
                                self.logger.warning(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥")
                    except Exception as e:
                        self.logger.warning(f"âŒ å®šæœŸä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
                
                # è¿›åº¦æ—¥å¿—
                recent_rewards = self.training_stats['episode_rewards'][-10:] if self.training_stats['episode_rewards'] else [0.0]
                avg_reward = np.mean(recent_rewards)
                self.logger.info(f"åŸºåº§æ¨¡å‹è®­ç»ƒè¿›åº¦: {self.current_step:,}/{self.total_timesteps:,} "
                               f"({self.current_step/self.total_timesteps:.1%}) | "
                               f"é˜¶æ®µ: {self.current_phase} | "
                               f"å¹³å‡å¥–åŠ±: {avg_reward:.3f} | "
                               f"Episodes: {episode_count}")
                
                # è®°å½•ç»Ÿè®¡
                batch_stats = {
                    'steps': steps_collected,
                    'total_steps': self.current_step,
                    'phase': self.current_phase,
                    'avg_reward': avg_reward
                }
                training_stats.append(batch_stats)
                
            except Exception as e:
                self.logger.error(f"è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # å³ä½¿å‡ºé”™ä¹Ÿè¦æ›´æ–°è¿›åº¦ï¼Œé¿å…å¡ä½
                self.current_step += steps_collected  # ä½¿ç”¨å®é™…æ”¶é›†çš„æ­¥æ•°
                if self.visualization_manager:
                    error_metrics = {
                        'episode': episode_count,
                        'total_reward': 0.0,
                        'exploration_rate': 0.0,
                        'policy_loss': 0.0,
                        'value_loss': 0.0,
                        'phase': self.current_phase
                    }
                    self.visualization_manager.on_step(self.current_step, error_metrics)
                break
        
        # æœ€ç»ˆè®­ç»ƒæ­¥éª¤
        if rollout_buffer:
            self._train_step(rollout_buffer)
        
        # æ±‡æ€»è®­ç»ƒç»Ÿè®¡
        elapsed_time = time.time() - start_time
        
        # æœ€ç»ˆè¯„ä¼°
        final_metrics = self._final_evaluation()
        final_metrics.update({
            'training_time': elapsed_time,
            'training_completion_rate': self.current_step / self.total_timesteps if self.total_timesteps > 0 else 0.0,
            'training_batches': len(training_stats),
            'total_episodes': episode_count
        })
        
        self.logger.info(f"åŸºåº§æ¨¡å‹è®­ç»ƒå®Œæˆ: å®Œæˆæ­¥æ•°={self.current_step}, æœ€ç»ˆå¥–åŠ±={final_metrics.get('avg_episode_reward', 0.0):.3f}")
        
        return final_metrics
    
    def _update_curriculum_phase(self, step: int) -> None:
        """æ›´æ–°è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ"""
        
        if step < self.hover_training_steps and self.current_phase != "hover":
            self.current_phase = "hover"
            self.phase_start_step = step
            self.logger.info(f"åˆ‡æ¢åˆ°æ‚¬åœè®­ç»ƒé˜¶æ®µ (æ­¥éª¤ {step})")
            
        elif step >= self.hover_training_steps and self.current_phase != "flight":
            self.current_phase = "flight" 
            self.phase_start_step = step
            self.logger.info(f"åˆ‡æ¢åˆ°é£è¡Œè®­ç»ƒé˜¶æ®µ (æ­¥éª¤ {step})")
    
    def _train_step(self, rollout_buffer: List[Dict]) -> None:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        
        # è½¬æ¢ä¸ºå¼ é‡
        batch_obs = torch.FloatTensor([exp['obs'] for exp in rollout_buffer])
        batch_actions = torch.FloatTensor([exp['action'] for exp in rollout_buffer])
        batch_rewards = torch.FloatTensor([exp['reward'] for exp in rollout_buffer])
        batch_values = torch.FloatTensor([exp['value'] for exp in rollout_buffer])
        batch_dones = torch.FloatTensor([exp['done'] for exp in rollout_buffer])
        
        # è®¡ç®—GAEä¼˜åŠ¿
        advantages = self._compute_gae(batch_rewards, batch_values, batch_dones)
        returns = advantages + batch_values
        
        # å½’ä¸€åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å‰å‘ä¼ æ’­
        pred_actions, pred_values = self.model(batch_obs)
        
        # è®¡ç®—æŸå¤±
        value_loss = nn.MSELoss()(pred_values.squeeze(), returns)
        
        # PPOç­–ç•¥æŸå¤± (ç®€åŒ–ç‰ˆ)
        action_diff = (pred_actions - batch_actions).pow(2).mean()
        policy_loss = action_diff - 0.01 * advantages.mean()  # ç®€åŒ–çš„ç­–ç•¥æŸå¤±
        
        total_loss = value_loss + policy_loss
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
        self.optimizer.step()
        
        # è®°å½•ç»Ÿè®¡
        self.training_stats['value_losses'].append(value_loss.item())
        self.training_stats['policy_losses'].append(policy_loss.item())
    
    def _compute_gae(self, 
                    rewards: torch.Tensor, 
                    values: torch.Tensor, 
                    dones: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—GAEä¼˜åŠ¿"""
        
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1] * (1 - dones[i])
            
            delta = rewards[i] + self.gamma * next_value - values[i]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
            advantages[i] = gae
        
        return advantages
    
    def _handle_episode_end(self, 
                           episode_reward: float, 
                           episode_length: int,
                           info: Dict[str, Any]) -> None:
        """å¤„ç†å›åˆç»“æŸ"""
        
        self.training_stats['episode_rewards'].append(episode_reward)
        self.training_stats['episode_lengths'].append(episode_length)
        
        # æ ¹æ®å½“å‰é˜¶æ®µè®°å½•æˆåŠŸç‡
        if self.current_phase == "hover":
            success = info.get('hover_success', False)
            self.training_stats['hover_success_rate'].append(1.0 if success else 0.0)
        else:
            success = info.get('flight_success', False)
            self.training_stats['flight_success_rate'].append(1.0 if success else 0.0)
    
    def _report_progress(self, step: int, episode_count: int) -> None:
        """æŠ¥å‘Šè®­ç»ƒè¿›åº¦"""
        
        if len(self.training_stats['episode_rewards']) > 0:
            recent_rewards = self.training_stats['episode_rewards'][-10:]
            avg_reward = np.mean(recent_rewards)
            
            if self.current_phase == "hover" and self.training_stats['hover_success_rate']:
                recent_success = self.training_stats['hover_success_rate'][-10:]
                success_rate = np.mean(recent_success)
            elif self.current_phase == "flight" and self.training_stats['flight_success_rate']:
                recent_success = self.training_stats['flight_success_rate'][-10:]
                success_rate = np.mean(recent_success)
            else:
                success_rate = 0.0
            
            progress_data = {
                'step': step,
                'episode': episode_count,
                'phase': self.current_phase,
                'avg_reward': avg_reward,
                'success_rate': success_rate,
                'progress': step / self.total_timesteps
            }
            
            self.logger.info(
                f"æ­¥éª¤ {step}/{self.total_timesteps} | "
                f"é˜¶æ®µ: {self.current_phase} | "
                f"å¹³å‡å¥–åŠ±: {avg_reward:.2f} | "
                f"æˆåŠŸç‡: {success_rate:.2%}"
            )
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            for callback in self.progress_callbacks:
                callback(self.stage, progress_data)
    
    def _perform_evaluation(self) -> Dict[str, float]:
        """æ‰§è¡Œè¯„ä¼°"""
        num_eval_episodes = self.config.get('eval_episodes', 10)
        
        if self.visualization_manager:
            self.visualization_manager.on_evaluation_start(num_eval_episodes)
        
        try:
            eval_results = self.evaluate(num_eval_episodes)
            
            # è¯„ä¼°ç»“æœåˆ†æå’Œæ—¥å¿—
            success_rate = eval_results.get('success_rate', 0.0)
            mean_reward = eval_results.get('mean_reward', 0.0)
            
            if success_rate > 0.8:
                self.logger.info(f"ğŸ‰ ä¼˜ç§€æ€§èƒ½: æˆåŠŸç‡ {success_rate:.2%}, å¹³å‡å¥–åŠ± {mean_reward:.2f}")
            elif success_rate > 0.5:
                self.logger.info(f"âœ… è‰¯å¥½æ€§èƒ½: æˆåŠŸç‡ {success_rate:.2%}, å¹³å‡å¥–åŠ± {mean_reward:.2f}")
            else:
                self.logger.warning(f"âš ï¸ éœ€è¦æ”¹è¿›: æˆåŠŸç‡ {success_rate:.2%}, å¹³å‡å¥–åŠ± {mean_reward:.2f}")
            
            self.write_stage_message(f"è¯„ä¼°å®Œæˆ: æˆåŠŸç‡ {success_rate:.1%}")
            return eval_results
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {'mean_reward': 0.0, 'success_rate': 0.0, 'error': str(e)}
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, Any]:
        """è¯„ä¼°æ–¹æ³• - ä¿®å¤ç‰ˆæœ¬ï¼Œæ·»åŠ è¶…æ—¶å’Œå¼ºåˆ¶ç»ˆæ­¢æœºåˆ¶"""
        if not self.model or not self.env:
            return {'mean_reward': 0.0, 'success_rate': 0.0, 'error': 'Model or environment not initialized'}
        
        episode_rewards = []
        episode_lengths = []
        success_count = 0
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        try:
            # ğŸ”§ å‡å°‘è¯„ä¼°episodeæ•°ï¼Œé¿å…é•¿æ—¶é—´è¯„ä¼°
            eval_episodes = min(num_episodes, 5)
            self.logger.info(f"å¼€å§‹è¯„ä¼°: {eval_episodes} episodes")
            
            for episode in range(eval_episodes):
                try:
                    obs, info = self.env.reset()
                    episode_reward = 0
                    episode_length = 0
                    done = False
                    
                    # ğŸ”§ å‡å°‘æœ€å¤§æ­¥æ•°ï¼Œé¿å…episodeè¿‡é•¿
                    max_steps = 150  # å›ºå®š150æ­¥æœ€å¤§é™åˆ¶
                    
                    while not done and episode_length < max_steps:
                        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œ
                        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                        action = self.model.get_action(obs_tensor, deterministic=True)
                        action_np = action.squeeze(0).cpu().numpy()
                        
                        obs, reward, terminated, truncated, info = self.env.step(action_np)
                        
                        episode_reward += reward
                        episode_length += 1
                        done = terminated or truncated
                        
                        # ğŸ”§ å¢å¼ºçš„æˆåŠŸæ¡ä»¶åˆ¤æ–­
                        if self._is_success(terminated, truncated, info, episode_length):
                            success_count += 1
                            break
                    
                    episode_rewards.append(episode_reward)
                    episode_lengths.append(episode_length)
                    self.logger.info(f"è¯„ä¼° episode {episode + 1}: å¥–åŠ±={episode_reward:.2f}, æ­¥æ•°={episode_length}")
                    
                except Exception as e:
                    self.logger.error(f"è¯„ä¼° episode {episode + 1} å¤±è´¥: {e}")
                    episode_rewards.append(0.0)
                    episode_lengths.append(0)
                    continue
            
            # æ¢å¤è®­ç»ƒæ¨¡å¼
            self.model.train()
            
            # ç¡®ä¿æœ‰æ•°æ®
            if not episode_rewards:
                episode_rewards = [0.0]
                episode_lengths = [0]
            
            return {
                'mean_reward': float(np.mean(episode_rewards)),
                'std_reward': float(np.std(episode_rewards)),
                'success_rate': success_count / eval_episodes if eval_episodes > 0 else 0.0,
                'mean_episode_length': float(np.mean(episode_lengths)),
                'episodes_evaluated': eval_episodes
            }
            
        except Exception as e:
            self.model.train()
            self.logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {'mean_reward': 0.0, 'success_rate': 0.0, 'error': str(e)}
    
    def _is_success(self, terminated: bool, truncated: bool, info: dict, episode_length: int) -> bool:
        """åˆ¤æ–­æˆåŠŸæ¡ä»¶"""
        if not terminated:
            return False
        
        # 1. æ˜¾å¼æˆåŠŸæ ‡å¿—
        if info.get('hover_success', False) or info.get('flight_success', False):
            return True
        
        # 2. åŸºäºé˜¶æ®µçš„æˆåŠŸåˆ¤æ–­
        if self.current_phase == "hover":
            # æ‚¬åœæˆåŠŸï¼šç¨³å®šæ—¶é—´ > é˜ˆå€¼
            if info.get('stable_time', 0) > 100:
                return True
        elif self.current_phase == "flight":
            # é£è¡ŒæˆåŠŸï¼šè·ç¦»ç›®æ ‡ < é˜ˆå€¼
            if info.get('distance_to_target', float('inf')) < 0.5:
                return True
        
        # 3. åŸºäºå¥–åŠ±é˜ˆå€¼çš„æˆåŠŸåˆ¤æ–­
        total_reward = info.get('total_reward', 0.0)
        if total_reward > 50:  # è‡ªå®šä¹‰å¥–åŠ±é˜ˆå€¼
            return True
        
        # 4. åŸºäºepisodeé•¿åº¦çš„æˆåŠŸåˆ¤æ–­ï¼ˆé¿å…æ—©æœŸç»ˆæ­¢ï¼‰
        if episode_length > 800:  # é•¿æœŸå­˜æ´»
            return True
            
        return False
    
    def save_model(self, path: Path, metadata: Optional[Dict] = None) -> bool:
        """ä¿å­˜åŸºåº§æ¨¡å‹"""
        if not self.model:
            self.logger.error("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
            return False
        
        try:
            # ä¿å­˜PyTorchæ¨¡å‹ï¼ŒåŒ…å«è®­ç»ƒè¿›åº¦
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
                'current_step': getattr(self, 'current_step', 0),  # ä¿å­˜å½“å‰è®­ç»ƒæ­¥æ•°
                'current_phase': getattr(self, 'current_phase', 'hover'),  # ä¿å­˜å½“å‰è®­ç»ƒé˜¶æ®µ
                'config': self.config,
                'training_stats': self.training_stats,
                'metadata': metadata or {}
            }, str(path))
            
            self.logger.info(f"âœ… åŸºåº§æ¨¡å‹å·²ä¿å­˜: {path} (æ­¥æ•°: {getattr(self, 'current_step', 0)})")
            return True
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def load_model(self, path: Path) -> bool:
        """åŠ è½½åŸºåº§æ¨¡å‹"""
        if not self.model:
            self.logger.error("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½")
            return False
        
        try:
            # ä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜
            checkpoint = torch.load(str(path), weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            if self.optimizer and 'optimizer_state_dict' in checkpoint:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # æ¢å¤è®­ç»ƒè¿›åº¦
            if 'current_step' in checkpoint:
                self.current_step = checkpoint['current_step']
                self.logger.info(f"ç»­è®­èµ·å§‹æ­¥æ•°: {self.current_step}")
            
            # æ¢å¤è®­ç»ƒé˜¶æ®µ
            if 'current_phase' in checkpoint:
                self.current_phase = checkpoint['current_phase']
                self.logger.info(f"ç»­è®­èµ·å§‹é˜¶æ®µ: {self.current_phase}")
            
            self.logger.info(f"âœ… åŸºåº§æ¨¡å‹å·²åŠ è½½: {path}")
            return True
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _try_resume_training(self):
        """å°è¯•åŠ è½½ä¹‹å‰çš„æ¨¡å‹è¿›è¡Œç»­è®­"""
        if not self.config.get('resume_training', True):
            self.logger.info("ç»­è®­åŠŸèƒ½å·²ç¦ç”¨ï¼Œä»é›¶å¼€å§‹è®­ç»ƒ")
            return
        
        # æŸ¥æ‰¾å¯åŠ è½½çš„æ¨¡å‹
        model_paths_to_try = []
        
        # 1. æœç´¢æ‰€æœ‰å†å²foundationè®­ç»ƒä¼šè¯
        logs_dir = Path("logs")
        if logs_dir.exists():
            # æŸ¥æ‰¾æ‰€æœ‰foundationè®­ç»ƒç›®å½•
            foundation_dirs = list(logs_dir.glob("train_foundation_*"))
            foundation_dirs.sort(key=lambda p: p.stat().st_mtime, reverse=True)  # æŒ‰ä¿®æ”¹æ—¶é—´å€’åº
            
            self.logger.info(f"ğŸ” å‘ç° {len(foundation_dirs)} ä¸ªfoundationè®­ç»ƒä¼šè¯")
            
            # éå†æŸ¥æ‰¾å¯ç”¨æ¨¡å‹
            for session_dir in foundation_dirs:
                model_dir = session_dir / "Model"
                if not model_dir.exists():
                    continue
                
                # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹
                best_model_path = model_dir / "best_model_foundation.zip"
                if best_model_path.exists():
                    model_paths_to_try.append(("æœ€ä½³æ¨¡å‹", best_model_path))
                    self.logger.debug(f"æ‰¾åˆ°æœ€ä½³æ¨¡å‹: {best_model_path}")
                
                # æŸ¥æ‰¾æ£€æŸ¥ç‚¹
                try:
                    checkpoints = list(model_dir.glob("checkpoint_*.zip"))
                    if checkpoints:
                        # æŒ‰æ–‡ä»¶åä¸­çš„æ­¥æ•°æ’åºï¼Œå–æœ€æ–°çš„
                        latest_checkpoint = max(checkpoints, key=lambda p: int(p.stem.split('_')[1]))
                        model_paths_to_try.append(("æ£€æŸ¥ç‚¹", latest_checkpoint))
                        self.logger.debug(f"æ‰¾åˆ°æ£€æŸ¥ç‚¹: {latest_checkpoint}")
                except Exception as e:
                    self.logger.debug(f"è§£ææ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                
                # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿå¤šçš„å€™é€‰æ¨¡å‹ï¼Œå°±ä¸ç»§ç»­æœç´¢äº†
                if len(model_paths_to_try) >= 3:
                    break
        
        # ä¼˜å…ˆçº§æ’åºï¼šæœ€æ–°çš„æœ€ä½³æ¨¡å‹ > æœ€æ–°çš„æ£€æŸ¥ç‚¹
        if model_paths_to_try:
            self.logger.info(f"ğŸ“¦ æ‰¾åˆ° {len(model_paths_to_try)} ä¸ªå¯åŠ è½½çš„æ¨¡å‹")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            for model_type, model_path in model_paths_to_try:
                try:
                    if self.load_model(model_path):
                        self.logger.info(f"ğŸ”„ ç»­è®­å·²å¯åŠ¨ï¼ŒåŠ è½½{model_type}: {model_path}")
                        self.write_stage_message(f"ç»­è®­æ¨¡å¼ - åŠ è½½{model_type}")
                        return
                except Exception as e:
                    self.logger.warning(f"åŠ è½½{model_type}å¤±è´¥: {e}")
        
        self.logger.info("ğŸ†• æœªæ‰¾åˆ°å¯åŠ è½½çš„æ¨¡å‹ï¼Œä»é›¶å¼€å§‹è®­ç»ƒ")
        self.write_stage_message("æ–°è®­ç»ƒ - ä»é›¶å¼€å§‹")
    
    def on_step_callback(self, step: int, metrics: Dict[str, Any]):
        """æ­¥éª¤å›è°ƒ"""
        super().on_step_callback(step, metrics)
        
        # æ›´æ–°å¯è§†åŒ–ç®¡ç†å™¨
        if self.visualization_manager:
            step_metrics = {
                'step': step,
                'mean_reward': metrics.get('total_reward', 0.0),
                'episode_length': metrics.get('episode_length', 0),
                'exploration_rate': metrics.get('exploration_rate', 0.0),
                'loss': metrics.get('value_loss', 0.0),
                'phase': metrics.get('phase', self.current_phase)
            }
            
            try:
                self.visualization_manager.on_step(step, step_metrics)
            except AttributeError:
                self.visualization_manager.update_metrics(step_metrics)
    
    def on_episode_callback(self, episode: int, metrics: Dict[str, Any]):
        """Episodeå›è°ƒ"""
        self.current_episode = episode
        
        episode_reward = metrics.get('total_reward', 0.0)
        if episode_reward > self.best_reward:
            self.best_reward = episode_reward
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if self.session_manager:
                best_model_path = self.session_manager.get_model_save_path("best_model")
                self.save_model(best_model_path)
                
                if self.visualization_manager:
                    self.visualization_manager.on_model_save(
                        "æœ€ä½³", 
                        self.best_reward, 
                        str(best_model_path)
                    )
        
        if self.visualization_manager:
            episode_info = {
                'episode': episode,
                'mean_reward': episode_reward,
                'episode_length': metrics.get('episode_length', 0),
                'is_best': episode_reward > self.best_reward,
                'phase': self.current_phase
            }
            try:
                self.visualization_manager.on_episode_end(episode, episode_info)
            except AttributeError:
                self.visualization_manager.update_metrics(episode_info)
    
    def on_evaluation_callback(self, eval_results: Dict[str, float]):
        """è¯„ä¼°å›è°ƒ"""
        self.evaluation_history.append(eval_results)
        
        # ğŸ“Š è®°å½•è¯„ä¼°ç»“æœåˆ°TensorBoard
        self._log_evaluation_to_tensorboard(eval_results)
        
        if self.visualization_manager:
            try:
                self.visualization_manager.on_evaluation_end(eval_results)
            except AttributeError:
                self.visualization_manager.update_metrics(eval_results)
    
    def on_checkpoint_callback(self, step: int):
        """æ£€æŸ¥ç‚¹å›è°ƒ"""
        if self.session_manager:
            checkpoint_path = self.session_manager.get_model_save_path(f"checkpoint_{step}.zip")
            if self.save_model(checkpoint_path):
                if self.visualization_manager:
                    self.visualization_manager.write_stage_message(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: step {step}")
            else:
                self.logger.error(f"æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {checkpoint_path}")
    
    def write_stage_message(self, message: str):
        """å†™å…¥é˜¶æ®µæ¶ˆæ¯"""
        if self.visualization_manager:
            self.visualization_manager.write_stage_message(message)
        else:
            self.logger.info(message)
    
    def _final_evaluation(self) -> Dict[str, Any]:
        """æœ€ç»ˆè¯„ä¼° - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«æ‚¬åœè´¨é‡å’Œé£è¡Œè´¨é‡è¯¦ç»†åˆ†æ"""
        eval_episodes = 5  # å‡å°‘è¯„ä¼°episodeæ•°ï¼Œé¿å…é•¿æ—¶é—´å¡ä½
        eval_rewards = []
        hover_successes = 0
        flight_successes = 0
        
        # è¯¦ç»†è´¨é‡æŒ‡æ ‡
        hover_quality_scores = []
        flight_quality_scores = []
        position_stability_scores = []
        velocity_smoothness_scores = []
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        try:
            self.logger.info(f"å¼€å§‹æœ€ç»ˆè¯„ä¼°: {eval_episodes} episodes")
            self.logger.info("=" * 60)
            self.logger.info("ğŸ¯ åŸºåº§æ¨¡å‹è¯„ä¼° - æ‚¬åœä¸é£è¡Œè´¨é‡åˆ†æ")
            self.logger.info("=" * 60)
            
            for episode in range(eval_episodes):
                self.logger.info(f"\nğŸ“‹ Episode {episode + 1}/{eval_episodes} å¼€å§‹è¯„ä¼°")
                
                try:
                    obs, _ = self.env.reset()
                    episode_reward = 0
                    episode_success = False
                    
                    # è´¨é‡è¯„ä¼°æŒ‡æ ‡
                    positions = []
                    velocities = []
                    hover_quality = 0.0
                    flight_quality = 0.0
                    
                    # ğŸ”§ å‡å°‘æœ€å¤§æ­¥æ•°ï¼Œé¿å…episodeè¿‡é•¿
                    max_steps = min(self.config.get('max_episode_steps', 1000), 200)  # æœ€å¤š200æ­¥
                    
                    for step in range(max_steps):
                        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                        action = self.model.get_action(obs_tensor, deterministic=True)
                        action_np = action.squeeze(0).cpu().numpy()
                        
                        obs, reward, terminated, truncated, info = self.env.step(action_np)
                        episode_reward += reward
                        
                        # æ”¶é›†ä½ç½®å’Œé€Ÿåº¦æ•°æ®ç”¨äºè´¨é‡åˆ†æ
                        if hasattr(self.env, 'pos') and hasattr(self.env, 'vel'):
                            positions.append(self.env.pos[0].copy())  # å–ç¬¬ä¸€ä¸ªæ— äººæœºçš„ä½ç½®
                            velocities.append(self.env.vel[0].copy())  # å–ç¬¬ä¸€ä¸ªæ— äººæœºçš„é€Ÿåº¦
                        
                        # ğŸ”§ å¼ºåŒ–ç»ˆæ­¢æ¡ä»¶
                        if terminated or truncated:
                            hover_quality = info.get('hover_quality', 0.0)
                            flight_quality = info.get('flight_quality', 0.0)
                            
                            if info.get('hover_success', False):
                                hover_successes += 1
                                episode_success = True
                                self.logger.info(f"  âœ… æ‚¬åœæˆåŠŸ! æ‚¬åœè´¨é‡: {hover_quality:.3f}")
                            if info.get('flight_success', False):
                                flight_successes += 1
                                episode_success = True
                                self.logger.info(f"  âœ… é£è¡ŒæˆåŠŸ! é£è¡Œè´¨é‡: {flight_quality:.3f}")
                            break
                        
                        # ğŸ”§ æ·»åŠ æ—©æœŸæˆåŠŸåˆ¤æ–­
                        if step > 100 and episode_reward > 50:  # 100æ­¥åå¦‚æœå¥–åŠ±è¶³å¤Ÿé«˜å°±è®¤ä¸ºæˆåŠŸ
                            episode_success = True
                            # ä¼°ç®—è´¨é‡åˆ†æ•°
                            hover_quality = min(episode_reward / 100.0, 1.0)
                            flight_quality = min(episode_reward / 200.0, 1.0)
                            self.logger.info(f"  â­ æ—©æœŸå®Œæˆ! ä¼°ç®—æ‚¬åœè´¨é‡: {hover_quality:.3f}, é£è¡Œè´¨é‡: {flight_quality:.3f}")
                            break
                    
                    # è®¡ç®—è¯¦ç»†è´¨é‡æŒ‡æ ‡
                    if len(positions) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                        position_stability = self._calculate_position_stability(positions)
                        velocity_smoothness = self._calculate_velocity_smoothness(velocities)
                        position_stability_scores.append(position_stability)
                        velocity_smoothness_scores.append(velocity_smoothness)
                        
                        self.logger.info(f"  ğŸ“Š ä½ç½®ç¨³å®šæ€§: {position_stability:.3f}")
                        self.logger.info(f"  ğŸ“Š é€Ÿåº¦å¹³æ»‘æ€§: {velocity_smoothness:.3f}")
                    else:
                        position_stability_scores.append(0.0)
                        velocity_smoothness_scores.append(0.0)
                    
                    hover_quality_scores.append(hover_quality)
                    flight_quality_scores.append(flight_quality)
                    eval_rewards.append(episode_reward)
                    
                    self.logger.info(f"  ğŸ“ˆ Episode {episode + 1} å®Œæˆ:")
                    self.logger.info(f"    å¥–åŠ±: {episode_reward:.2f}")
                    self.logger.info(f"    æ­¥æ•°: {step + 1}")
                    self.logger.info(f"    æˆåŠŸ: {'æ˜¯' if episode_success else 'å¦'}")
                    self.logger.info(f"    æ‚¬åœè´¨é‡: {hover_quality:.3f}")
                    self.logger.info(f"    é£è¡Œè´¨é‡: {flight_quality:.3f}")
                    
                except Exception as e:
                    self.logger.error(f"è¯„ä¼° episode {episode + 1} å¤±è´¥: {e}")
                    eval_rewards.append(0.0)  # æ·»åŠ é»˜è®¤å¥–åŠ±é¿å…ç©ºåˆ—è¡¨
                    hover_quality_scores.append(0.0)
                    flight_quality_scores.append(0.0)
                    position_stability_scores.append(0.0)
                    velocity_smoothness_scores.append(0.0)
                    continue
                    
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆè¯„ä¼°å¤±è´¥: {e}")
            # ç¡®ä¿æœ‰é»˜è®¤å€¼
            if not eval_rewards:
                eval_rewards = [0.0]
                hover_quality_scores = [0.0]
                flight_quality_scores = [0.0]
                position_stability_scores = [0.0]
                velocity_smoothness_scores = [0.0]
        finally:
            # æ¢å¤è®­ç»ƒæ¨¡å¼
            self.model.train()
            
        # æ‰“å°è¯¦ç»†è¯„ä¼°æ€»ç»“
        self._print_evaluation_summary(
            eval_rewards, hover_quality_scores, flight_quality_scores,
            position_stability_scores, velocity_smoothness_scores,
            hover_successes, flight_successes, eval_episodes
        )
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        final_metrics = {
            'avg_episode_reward': float(np.mean(eval_rewards)) if eval_rewards else 0.0,
            'std_episode_reward': float(np.std(eval_rewards)) if eval_rewards else 0.0,
            'hover_success_rate': hover_successes / eval_episodes if eval_episodes > 0 else 0.0,
            'flight_success_rate': flight_successes / eval_episodes if eval_episodes > 0 else 0.0,
            'avg_hover_quality': float(np.mean(hover_quality_scores)) if hover_quality_scores else 0.0,
            'avg_flight_quality': float(np.mean(flight_quality_scores)) if flight_quality_scores else 0.0,
            'avg_position_stability': float(np.mean(position_stability_scores)) if position_stability_scores else 0.0,
            'avg_velocity_smoothness': float(np.mean(velocity_smoothness_scores)) if velocity_smoothness_scores else 0.0,
            'total_episodes_trained': len(self.training_stats['episode_rewards']),
            'final_avg_reward': float(np.mean(self.training_stats['episode_rewards'][-10:])) if self.training_stats['episode_rewards'] else 0.0,
            'eval_episodes_completed': len(eval_rewards)
        }
        
        return final_metrics
    
    def _calculate_position_stability(self, positions: List[np.ndarray]) -> float:
        """è®¡ç®—ä½ç½®ç¨³å®šæ€§åˆ†æ•° (0-1ï¼Œè¶Šé«˜è¶Šç¨³å®š)"""
        if len(positions) < 2:
            return 0.0
        
        positions_array = np.array(positions)
        
        # è®¡ç®—ä½ç½®å˜åŒ–çš„æ ‡å‡†å·®
        position_std = np.std(positions_array, axis=0)
        avg_std = np.mean(position_std)
        
        # è½¬æ¢ä¸º0-1åˆ†æ•°ï¼Œè¾ƒå°çš„æ ‡å‡†å·®è¡¨ç¤ºæ›´å¥½çš„ç¨³å®šæ€§
        stability_score = max(0.0, min(1.0, 1.0 - avg_std / 2.0))
        
        return stability_score
    
    def _calculate_velocity_smoothness(self, velocities: List[np.ndarray]) -> float:
        """è®¡ç®—é€Ÿåº¦å¹³æ»‘æ€§åˆ†æ•° (0-1ï¼Œè¶Šé«˜è¶Šå¹³æ»‘)"""
        if len(velocities) < 3:
            return 0.0
        
        velocities_array = np.array(velocities)
        
        # è®¡ç®—é€Ÿåº¦å˜åŒ–çš„åŠ é€Ÿåº¦
        accelerations = np.diff(velocities_array, axis=0)
        acceleration_magnitude = np.linalg.norm(accelerations, axis=1)
        
        # è®¡ç®—å¹³å‡åŠ é€Ÿåº¦å¹…åº¦
        avg_acceleration = np.mean(acceleration_magnitude)
        
        # è½¬æ¢ä¸º0-1åˆ†æ•°ï¼Œè¾ƒå°çš„åŠ é€Ÿåº¦å˜åŒ–è¡¨ç¤ºæ›´å¹³æ»‘çš„è¿åŠ¨
        smoothness_score = max(0.0, min(1.0, 1.0 - avg_acceleration / 10.0))
        
        return smoothness_score
    
    def _print_evaluation_summary(self, eval_rewards: List[float], 
                                hover_quality_scores: List[float],
                                flight_quality_scores: List[float],
                                position_stability_scores: List[float],
                                velocity_smoothness_scores: List[float],
                                hover_successes: int, flight_successes: int, 
                                total_episodes: int):
        """æ‰“å°è¯¦ç»†çš„è¯„ä¼°æ€»ç»“"""
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ¯ åŸºåº§æ¨¡å‹è¯„ä¼°æ€»ç»“æŠ¥å‘Š")
        self.logger.info("=" * 80)
        
        # åŸºæœ¬ç»Ÿè®¡
        avg_reward = np.mean(eval_rewards) if eval_rewards else 0.0
        std_reward = np.std(eval_rewards) if eval_rewards else 0.0
        
        self.logger.info(f"\nğŸ“Š åŸºæœ¬æ€§èƒ½æŒ‡æ ‡:")
        self.logger.info(f"  æ€» Episodes: {total_episodes}")
        self.logger.info(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f} Â± {std_reward:.2f}")
        self.logger.info(f"  å¥–åŠ±èŒƒå›´: [{min(eval_rewards):.2f}, {max(eval_rewards):.2f}]")
        
        # æˆåŠŸç‡ç»Ÿè®¡
        hover_success_rate = hover_successes / total_episodes * 100
        flight_success_rate = flight_successes / total_episodes * 100
        
        self.logger.info(f"\nğŸ¯ ä»»åŠ¡æˆåŠŸç‡:")
        self.logger.info(f"  æ‚¬åœæˆåŠŸç‡: {hover_success_rate:.1f}% ({hover_successes}/{total_episodes})")
        self.logger.info(f"  é£è¡ŒæˆåŠŸç‡: {flight_success_rate:.1f}% ({flight_successes}/{total_episodes})")
        
        # è´¨é‡åˆ†æ
        avg_hover_quality = np.mean(hover_quality_scores) if hover_quality_scores else 0.0
        avg_flight_quality = np.mean(flight_quality_scores) if flight_quality_scores else 0.0
        avg_position_stability = np.mean(position_stability_scores) if position_stability_scores else 0.0
        avg_velocity_smoothness = np.mean(velocity_smoothness_scores) if velocity_smoothness_scores else 0.0
        
        self.logger.info(f"\nâ­ é£è¡Œè´¨é‡åˆ†æ:")
        self.logger.info(f"  æ‚¬åœè´¨é‡: {avg_hover_quality:.3f}/1.000")
        self.logger.info(f"  é£è¡Œè´¨é‡: {avg_flight_quality:.3f}/1.000")
        self.logger.info(f"  ä½ç½®ç¨³å®šæ€§: {avg_position_stability:.3f}/1.000")
        self.logger.info(f"  é€Ÿåº¦å¹³æ»‘æ€§: {avg_velocity_smoothness:.3f}/1.000")
        
        # è´¨é‡ç­‰çº§è¯„ä¼°
        overall_quality = (avg_hover_quality + avg_flight_quality + 
                          avg_position_stability + avg_velocity_smoothness) / 4.0
        
        if overall_quality >= 0.8:
            quality_level = "ä¼˜ç§€ ğŸŒŸ"
        elif overall_quality >= 0.6:
            quality_level = "è‰¯å¥½ âœ…"
        elif overall_quality >= 0.4:
            quality_level = "ä¸€èˆ¬ âš ï¸"
        else:
            quality_level = "éœ€è¦æ”¹è¿› âŒ"
            
        self.logger.info(f"\nğŸ† ç»¼åˆè´¨é‡è¯„çº§:")
        self.logger.info(f"  æ€»ä½“è´¨é‡åˆ†æ•°: {overall_quality:.3f}/1.000")
        self.logger.info(f"  è´¨é‡ç­‰çº§: {quality_level}")
        
        # è¯¦ç»†åˆ†æå’Œå»ºè®®
        self.logger.info(f"\nğŸ“‹ è¯¦ç»†åˆ†æ:")
        
        if avg_hover_quality < 0.5:
            self.logger.info("  âš ï¸  æ‚¬åœè´¨é‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ‚¬åœè®­ç»ƒæ—¶é—´")
        elif avg_hover_quality >= 0.8:
            self.logger.info("  âœ… æ‚¬åœèƒ½åŠ›ä¼˜ç§€")
            
        if avg_flight_quality < 0.5:
            self.logger.info("  âš ï¸  é£è¡Œè´¨é‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ é£è¡Œè·¯å¾„è®­ç»ƒ")
        elif avg_flight_quality >= 0.8:
            self.logger.info("  âœ… é£è¡Œèƒ½åŠ›ä¼˜ç§€")
            
        if avg_position_stability < 0.6:
            self.logger.info("  âš ï¸  ä½ç½®ç¨³å®šæ€§éœ€è¦æå‡ï¼Œå»ºè®®è°ƒæ•´PIDå‚æ•°")
        elif avg_position_stability >= 0.8:
            self.logger.info("  âœ… ä½ç½®æ§åˆ¶ç¨³å®š")
            
        if avg_velocity_smoothness < 0.6:
            self.logger.info("  âš ï¸  é€Ÿåº¦å˜åŒ–è¿‡äºå‰§çƒˆï¼Œå»ºè®®ä¼˜åŒ–åŠ¨ä½œç©ºé—´")
        elif avg_velocity_smoothness >= 0.8:
            self.logger.info("  âœ… è¿åŠ¨å¹³æ»‘è‡ªç„¶")
        
        self.logger.info("=" * 80)
        self.logger.info("âœ… åŸºåº§æ¨¡å‹è¯„ä¼°å®Œæˆ")
        self.logger.info("=" * 80 + "\n")
    
    def _log_to_tensorboard(self, metrics: Dict[str, Any]):
        """è®°å½•æŒ‡æ ‡åˆ°TensorBoard"""
        if not self.tensorboard_writer:
            return
        
        try:
            step = self.current_step
            
            # è®­ç»ƒæŒ‡æ ‡
            if 'total_reward' in metrics:
                self.tensorboard_writer.add_scalar('Train/EpisodeReward', metrics['total_reward'], step)
            
            if 'exploration_rate' in metrics:
                self.tensorboard_writer.add_scalar('Train/ExplorationRate', metrics['exploration_rate'], step)
            
            if 'policy_loss' in metrics and metrics['policy_loss'] > 0:
                self.tensorboard_writer.add_scalar('Train/PolicyLoss', metrics['policy_loss'], step)
            
            if 'value_loss' in metrics and metrics['value_loss'] > 0:
                self.tensorboard_writer.add_scalar('Train/ValueLoss', metrics['value_loss'], step)
            
            # è®­ç»ƒé˜¶æ®µ
            phase_mapping = {'hover': 0, 'flight': 1}
            if 'phase' in metrics and metrics['phase'] in phase_mapping:
                self.tensorboard_writer.add_scalar('Train/TrainingPhase', phase_mapping[metrics['phase']], step)
            
            # ç»Ÿè®¡ä¿¡æ¯
            if self.training_stats['episode_rewards']:
                avg_reward = np.mean(self.training_stats['episode_rewards'][-10:])
                self.tensorboard_writer.add_scalar('Train/AvgReward_10ep', avg_reward, step)
            
            if self.training_stats['episode_lengths']:
                avg_length = np.mean(self.training_stats['episode_lengths'][-10:])
                self.tensorboard_writer.add_scalar('Train/AvgEpisodeLength', avg_length, step)
            
            # æˆåŠŸç‡
            if self.training_stats['hover_success_rate']:
                self.tensorboard_writer.add_scalar('Train/HoverSuccessRate', 
                                                 self.training_stats['hover_success_rate'][-1], step)
            
            if self.training_stats['flight_success_rate']:
                self.tensorboard_writer.add_scalar('Train/FlightSuccessRate', 
                                                 self.training_stats['flight_success_rate'][-1], step)
            
            # å®šæœŸflush
            if step % 100 == 0:
                self.tensorboard_writer.flush()
                
        except Exception as e:
            # ä¸å½±å“è®­ç»ƒè¿›ç¨‹ï¼Œåªè®°å½•è­¦å‘Š
            self.logger.warning(f"TensorBoardè®°å½•å¤±è´¥: {e}")
    
    def _log_evaluation_to_tensorboard(self, eval_results: Dict[str, Any]):
        """è®°å½•è¯„ä¼°ç»“æœåˆ°TensorBoard"""
        if not self.tensorboard_writer:
            return
        
        try:
            step = self.current_step
            
            # è¯„ä¼°æŒ‡æ ‡
            if 'avg_reward' in eval_results:
                self.tensorboard_writer.add_scalar('Eval/AvgReward', eval_results['avg_reward'], step)
            
            if 'success_rate' in eval_results:
                self.tensorboard_writer.add_scalar('Eval/SuccessRate', eval_results['success_rate'], step)
            
            if 'hover_quality' in eval_results:
                self.tensorboard_writer.add_scalar('Eval/HoverQuality', eval_results['hover_quality'], step)
            
            if 'flight_quality' in eval_results:
                self.tensorboard_writer.add_scalar('Eval/FlightQuality', eval_results['flight_quality'], step)
            
            if 'position_stability' in eval_results:
                self.tensorboard_writer.add_scalar('Eval/PositionStability', eval_results['position_stability'], step)
            
            if 'velocity_smoothness' in eval_results:
                self.tensorboard_writer.add_scalar('Eval/VelocitySmoothness', eval_results['velocity_smoothness'], step)
            
            self.tensorboard_writer.flush()
            
        except Exception as e:
            self.logger.warning(f"è¯„ä¼°TensorBoardè®°å½•å¤±è´¥: {e}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.tensorboard_writer:
            try:
                self.tensorboard_writer.close()
                self.logger.info("âœ… TensorBoard Writer å·²å…³é—­")
            except Exception as e:
                self.logger.warning(f"å…³é—­TensorBoard Writerå¤±è´¥: {e}")
        
        super().cleanup() if hasattr(super(), 'cleanup') else None
    
    def _start_trajectory_episode(self, episode_num: int):
        """å¼€å§‹æ–°episodeçš„è½¨è¿¹è®°å½•"""
        if self.trajectory_manager:
            try:
                self.trajectory_manager.start_new_episode(episode_num)
                self.trajectory_episode_started = True
                self.current_episode_reward = 0.0
                self.current_episode_length = 0
            except Exception as e:
                self.logger.warning(f"å¯åŠ¨è½¨è¿¹è®°å½•å¤±è´¥: {e}")
                self.trajectory_episode_started = False
    
    def _log_trajectory_step(self, obs, action, reward, next_obs, done, info):
        """è®°å½•å•æ­¥è½¨è¿¹æ•°æ®"""
        if not self.trajectory_manager or not self.trajectory_episode_started:
            return
        
        try:
            # ğŸ”§ æ­£ç¡®è§£æè½¨è¿¹æ•°æ®ï¼šä»ç¯å¢ƒçš„get_trajectory_step_dataæ–¹æ³•è·å–å‡†ç¡®æ•°æ®
            trajectory_data = None
            if hasattr(self.env, 'get_trajectory_step_data'):
                try:
                    trajectory_data = self.env.get_trajectory_step_data(drone_idx=0)
                except Exception as e:
                    # å¦‚æœå‚æ•°åä¸åŒ¹é…ï¼Œå°è¯•æ— å‚æ•°è°ƒç”¨
                    try:
                        trajectory_data = self.env.get_trajectory_step_data()
                    except:
                        trajectory_data = None
            
            if trajectory_data is not None:
                # ä½¿ç”¨ç¯å¢ƒæä¾›çš„å‡†ç¡®æ•°æ®
                current_position = trajectory_data['current_position'].tolist() if isinstance(trajectory_data['current_position'], np.ndarray) else list(trajectory_data['current_position'])
                current_velocity = trajectory_data['current_velocity'].tolist() if isinstance(trajectory_data['current_velocity'], np.ndarray) else list(trajectory_data['current_velocity'])
                target_action = trajectory_data.get('target_velocity', [0.0, 0.0, 0.0, 0.0])
                rpm_action = trajectory_data.get('rpm_action', [0.0, 0.0, 0.0, 0.0])
                
                # è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°å®é™…æ•°æ®
                if self.config.get('debug', False) and self.current_episode_length < 5:
                    self.logger.info(f"=== è½¨è¿¹è®°å½•è°ƒè¯• (æ­¥éª¤ {self.current_episode_length}) ===")
                    self.logger.info(f"å®é™…ä½ç½®: {current_position}")
                    self.logger.info(f"å®é™…é€Ÿåº¦: {current_velocity}")
                    self.logger.info(f"ç›®æ ‡åŠ¨ä½œ: {target_action}")
                    self.logger.info(f"RPMè¾“å‡º: {rpm_action}")
                    if isinstance(info, dict) and 'direct_state' in info:
                        direct_state = info['direct_state']
                        if isinstance(direct_state, dict) and 'current_velocity' in direct_state:
                            self.logger.info(f"ç›´æ¥çŠ¶æ€é€Ÿåº¦: {direct_state['current_velocity']}")
            else:
                # åå¤‡æ–¹æ¡ˆï¼šä»è§‚æµ‹æ•°æ®è§£æï¼ˆåŸºäºgym-pybullet-dronesçš„KINè§‚æµ‹æ ¼å¼ï¼‰
                if isinstance(obs, np.ndarray) and len(obs) >= 12:
                    # KINè§‚æµ‹æ ¼å¼ï¼š[x, y, z, qx, qy, qz, qw, roll, pitch, yaw, vx, vy, vz, wx, wy, wz, ...]
                    current_position = obs[:3].tolist()      # ä½ç½® [0:3]
                    current_velocity = obs[10:13].tolist()   # é€Ÿåº¦ [10:13]
                else:
                    current_position = [0.0, 0.0, 0.0]
                    current_velocity = [0.0, 0.0, 0.0]
                
                # ä»infoä¸­è·å–æ§åˆ¶å™¨æ•°æ®
                target_action = [0.0, 0.0, 0.0, 0.0]
                rpm_action = [0.0, 0.0, 0.0, 0.0]
                
                if isinstance(info, dict):
                    # æ£€æŸ¥direct_stateä¸­çš„é€Ÿåº¦ä¿¡æ¯
                    if 'direct_state' in info and isinstance(info['direct_state'], dict):
                        direct_state = info['direct_state']
                        if 'current_velocity' in direct_state:
                            vel_tensor = direct_state['current_velocity']
                            if hasattr(vel_tensor, 'tolist'):
                                current_velocity = vel_tensor.tolist()
                            elif hasattr(vel_tensor, '__len__') and len(vel_tensor) >= 3:
                                current_velocity = list(vel_tensor[:3])
                    
                    # è·å–æ§åˆ¶ç›¸å…³æ•°æ®
                    if 'incremental_control' in info and isinstance(info['incremental_control'], dict):
                        target_data = info['incremental_control'].get('drone_0_target', [0.0, 0.0, 0.0, 0.0])
                        if hasattr(target_data, '__len__') and len(target_data) >= 4:
                            target_action = list(target_data[:4])
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯floatç±»å‹
            current_position = [float(x) for x in current_position[:3]]
            current_velocity = [float(x) for x in current_velocity[:3]]
            target_velocity = target_action[:3] if target_action and len(target_action) >= 3 else [0.0, 0.0, 0.0]
            
            # æ„å»ºè½¨è¿¹æ•°æ®
            step_data = {
                'step': float(self.current_episode_length),
                'current_position': current_position,
                'current_velocity': current_velocity,
                'target_velocity': target_velocity,
                'model_action': action.tolist() if isinstance(action, np.ndarray) else list(action),
                'rpm_action': rpm_action.tolist() if isinstance(rpm_action, np.ndarray) else list(rpm_action) if rpm_action else [0.0, 0.0, 0.0, 0.0],
                'reward': float(reward),
                'exploration_rate': float(max(0.0, 1.0 - self.current_step / self.total_timesteps)),
                'done': bool(done)  # ç¡®ä¿æ˜¯å¸ƒå°”å€¼
            }
            
            self.trajectory_manager.log_step(step_data)
            self.current_episode_length += 1
            self.current_episode_reward += reward
            
        except Exception as e:
            self.logger.warning(f"è½¨è¿¹è®°å½•å¤±è´¥: {e}")
            # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°æ›´å¤šä¿¡æ¯
            if self.config.get('debug', False):
                self.logger.warning(f"obs shape: {obs.shape if isinstance(obs, np.ndarray) else type(obs)}")
                self.logger.warning(f"info keys: {list(info.keys()) if isinstance(info, dict) else type(info)}")
                import traceback
                self.logger.warning(f"è½¨è¿¹è®°å½•è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    def _finalize_trajectory_episode(self, episode_reward: float, episode_length: int, info: Dict[str, Any]):
        """å®Œæˆepisodeçš„è½¨è¿¹è®°å½•"""
        if not self.trajectory_manager or not self.trajectory_episode_started:
            return
        
        try:
            # ç¡®å®šç»ˆæ­¢åŸå› 
            termination_reason = "unknown"
            if info.get('collision', False):
                termination_reason = "collision"
            elif info.get('timeout', False):
                termination_reason = "timeout"
            elif info.get('hover_success', False):
                termination_reason = "hover_success"
            elif info.get('flight_success', False):
                termination_reason = "flight_success"
            elif info.get('out_of_bounds', False):
                termination_reason = "out_of_bounds"
            else:
                termination_reason = "normal"
            
            # æœ€ç»ˆexploration rate
            final_exploration_rate = max(0.0, 1.0 - self.current_step / self.total_timesteps)
            
            self.trajectory_manager.finalize_episode(
                termination_reason=termination_reason,
                final_exploration_rate=final_exploration_rate,
                total_reward=episode_reward
            )
            
            self.trajectory_episode_started = False
            
        except Exception as e:
            self.logger.warning(f"å®Œæˆè½¨è¿¹è®°å½•å¤±è´¥: {e}")
            self.trajectory_episode_started = False

    
def create_baseflight_trainer(config: Dict[str, Any], 
                             foundation_model_path: Optional[Path] = None) -> BaseFlightTrainer:
    """åˆ›å»ºåŸºåº§æ¨¡å‹è®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°"""
    return BaseFlightTrainer(config, foundation_model_path)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    test_config = {
        'total_timesteps': 10000,
        'hover_training_steps': 5000,
        'flight_training_steps': 5000,
        'evaluation_frequency': 2000,
        'eval_episodes': 5,
        'enable_trajectory': True,
        'enable_tensorboard': True,
        'enable_visualization': True,
        'enable_rich_display': True,
        'enable_curriculum': True
    }
    
    trainer = create_baseflight_trainer(test_config)
    
    print("å¼€å§‹æµ‹è¯•åŸºåº§æ¨¡å‹è®­ç»ƒå™¨...")
    if trainer.setup():
        print("âœ… è®­ç»ƒå™¨è®¾ç½®æˆåŠŸ")
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç®€çŸ­çš„è®­ç»ƒæµ‹è¯•
    else:
        print("âŒ è®­ç»ƒå™¨è®¾ç½®å¤±è´¥")
